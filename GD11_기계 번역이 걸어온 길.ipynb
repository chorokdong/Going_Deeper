{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GD11_기계 번역이 걸어온 길.ipynb","provenance":[],"authorship_tag":"ABX9TyMDjq4JaRKUMEAO7Be8vI3u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","# 번역의 흐름\n","---"],"metadata":{"id":"zCo9owy7Lx2L"}},{"cell_type":"markdown","source":["---\n","### 규칙 기반 기계 번역\n","---"],"metadata":{"id":"Ae8W9ToTMxKA"}},{"cell_type":"markdown","source":["잠시 한영 번역가가 되었다고 생각해 봅시다.  \n","나는 이라는 어절을 어떻게 번역할 수 있을까요? 아마 백이면 백 I am 또는 I'm 이라고 번역을 할 겁니다.  \n","그렇다면 너는은 어떻죠? You am이라고 번역하실 분은 설마 없겠죠?  \n","You are이나 You're로 번역하는 것이 맞습니다! \n","  \n","나 + 는 과 너 + 는 모두 는을 포함하는데, 영문에서는 는 이 am 과 are로 나뉘는 것을 알 수 있습니다.  \n","그러니 는 = am 이라고 정의할 수도, 는 = are 이라고 정의할 수도 없죠.  \n","먼저 등장하는 단어가 나 인지 너 인지에 따라 분기를 나눠줘야 하는 거예요.  \n","  \n","이처럼 번역할 때 경우의 수를 직접 정의해 주는 방식이 규칙 기반 기계 번역(RBMT, Rule-Based Machine Translation) 입니다.  \n","이 수많은 규칙들은 모두 언어학을 기반으로 하기 때문에, 개발 과정에 언어학자가 동반되어야만 했어요.\n","  \n","그리고 첫 성과가 난 것은 1954년, 미국의 조지타운대와 IBM이 직접 정의한 언어 규칙을 통해 60개의 러시아어 문장을 영어로 번역하는 데에 성공합니다! \n","\n","예상하셨겠지만 규칙 기반 기계 번역은 한계가 명확합니다.  \n","규칙에 없는 문장이 들어올 경우 번역이 불가능하고 유연성이 떨어지며, 무엇보다 모든 규칙을 정의하는 과정이 너무나도 복잡하고 오랜 시간이 필요 합니다.  \n","더 편하게, 유연하게 번역을 해낼 수는 없을까요?"],"metadata":{"id":"F9q7rpjiL8Nt"}},{"cell_type":"markdown","source":["---\n","### 통계적 기계 번역\n","---"],"metadata":{"id":"2_u8xoX7M4m1"}},{"cell_type":"markdown","source":["단점이 명확한 규칙 기반 기계 번역을 개선하고자 하는 시도는 당연히 존재했으며, 아무래도 그것은 직접 구현해 본 쪽이 유리했나 봅니다.  \n","1988년에 IBM이 Model 1을 통해 새로운 번역 방식을 선보였죠.  \n","수많은 데이터로부터 통계적 확률을 구해 번역을 진행하는 통계적 기계 번역(SMT, Statistical Machine Translation) 이 바로 그것입니다.  \n","  \n","\"문장이 존재할 확률을 측정한다.\" 뭔가 멋진 개념을 담은 언어 모델이죠?  \n","통계적 언어 모델을 활용한 SMT는 과연 어떻게 동작하는 걸까요?  \n","번역은 자연스러운 문장을 생성하는 것뿐만 아니라 번역문의 문법 구조도 고려해야 하기 때문에 단순히 P(Target | Source) 만을 고려해서는 안 된답니다.  \n","원문과 번역문, 각 단어 간의 매핑 관계를 추가로 고려해야 하는데, 그 관계를 정렬(Alignment) 이라고 부릅니다.  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-02.max-800x600.png'>\n","<br><br>\n","\n","정렬에는 퍼틸리티(Fertility) 와 왜곡(Distortion), 두 가지 요소가 포함됩니다.\n","\n","퍼틸리티는 직역하면 '출생률' 정도 되는 뜬금없는 의미를 갖고 있는데요, 원문의 각 단어가 번역 후에 몇 개의 단어로 나타나는지를 의미하는 값입니다.  \n","위 예를 따르면 Everyone의 퍼틸리티는 2이고, Clapped 의 퍼틸리티는 3입니다.  \n","그리고 to, the 의 퍼틸리티는 0이죠! 번역에 직접적으로 등장하지 않는다는 의미입니다.  \n","\n","    퍼틸리티에 대한 확률은 p(n|w) 로 정의되며 \n","    - n은 퍼틸리티 값, \n","    - w는 원문의 단어입니다.  \n","music 은 높은 확률로 음악 으로만 번역될 테니 p(1|music) = 0.9 정도로 표현할 수 있겠죠?\n","\n","왜곡은 원문의 단어가 번역문에서 존재하는 위치를 나타냅니다.  n\n","위의 예에서 Clapped는 손뼉(6) 을(7) 쳤다(8) 로 번역되므로 Clapped의 왜곡은 (6, 7, 8)로 나타납니다. \n","\n","    왜곡에 대한 확률은 p(t|s, l) 로 정의되며 \n","    - t은 번역문에서 각 단어의 위치, \n","    - s는 원문에서 각 단어의 위치, \n","    - l은 번역문의 길이입니다. \n","    \n","Everyone(1) 으로 시작하는 문장은 모두(1) / 가(2) 라고 번역될 확률이 높으니 p(1|1, 8) x p(2|1, 8) 역시 제법 높을 것을 추측할 수 있습니다.\n","\n","통계적 언어 모델에서 파생된 확률에 위 모든 확률을 곱하여 학습하는 것이 바로 통계적 기계 번역입니다.\n","\n","문제를 하나 풀어봅시다.\n","\n","예문\n","\n","    E: Everyone(1, 2) clapped(6, 7, 8) in(4) time(5) to(·) the(·) music(3)\n","    -> K: 모두(1) 가(2) 음악(3) 에(4) 맞춰(5) 손뼉(6) 을(7) 쳤다(8)\n","\n","수식\n","\n","\tp(E|K) =\n","\t{p(2|Everyone) x p(1|1, 8) x p(2|1, 8) x p(모두|Everyone) x p(가|Everyone)} x\n","\t{p(3|clapped) x p(6|2, 8) x p(7|2, 8) x p(8|2, 8) x p(손뼉|clapped) x p(을|clapped) x p(쳤다|clapped)} x\n","\t{p(1|in) x p(4|3, 8) x p(에|in)} x\n","\t{p(1|time) x p(5|4, 8) x p(맞춰|time)} x\n","\t{p(0|to) x} x\n","\t{p(0|the) x} x\n","\t{p(1|music) x p(3|7, 8) x p(음악|music)}\n","\n","우리가 배운 예시는 단 하나의 단어를 기반으로 정렬 값을 구했는데, 이건 약간의 찜찜함이 남죠. in time to 를 묶어서 한 번에 ~에 맞춰 로 번역하면 좋을 텐데요!\n","  \n","이에 발맞춰 두 단어 이상으로 정렬을 구하는 구문 기반 번역(PBMT, Phrase Based Machine Translation) 이 등장하여 꽤나 최근인 2006년까지도 사용되었습니다.  \n","또 규칙 기반 번역과 결합하여 하이브리드로 사용된 사례도 있죠!\n","  \n","하지만 모든 기업을 단 하나로 수렴하게 만드는 끝판왕이 등장하게 됩니다."],"metadata":{"id":"mUe9bVFrNtnj"}},{"cell_type":"markdown","source":["---\n","# 신경망 기계 번역\n","---"],"metadata":{"id":"ctfR-MerP6GQ"}},{"cell_type":"markdown","source":["예상하셨겠지만 그것이 바로 신경망 기계 번역(Neural Machine Translation) 입니다.  \n","seq2seq나 transformer는 신경망 기계 번역에 활용되는 대표적인 모델들이죠! 지금부터 배울 것은 그다음의 이야기예요!"],"metadata":{"id":"EdtFMe69P9EV"}},{"cell_type":"markdown","source":["---\n","# 지적 생성을 위한 넓고 얕은 탐색 (1) Greedy Decoding\n","---"],"metadata":{"id":"A-dc7nYUP_Yy"}},{"cell_type":"markdown","source":["지금까지 몇몇 예제와 프로젝트를 하며 문장을 생성했던 경험이 있으시죠? 잠시 이전에 함께 다루었던 예제 코드를 살펴봅시다.\n","\n","    def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    while True:\n","        predict = model(test_tensor)\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n","\n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated\n","\n","이 함수는 훈련시킨 모델이 문장을 생성할 수 있게 해주는 함수입니다. 우리가 주목해야 하는 부분은 바로 단어를 결정하는 부분입니다!\n","\n","    predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n","바로 이 부분인데요,  \n","모델이 예측한 predict 값을 Softmax를 통해 확률값으로 변환한 후, 가장 높은 확률을 갖는 단어가 다음 단어로 결정되는 순간이죠!  \n","아주 직관적인 개념이기에 별다른 언급이 없이 \"음~ 그렇구나~\" 하고 넘어갔지만, 이는 엄연히 탐욕 알고리즘(Greedy Algorithm) 이 사용된 거랍니다.  \n","  \n","탐욕적인 방법으로 문장을 Decoding 하니 기계 번역에서는 이를 Greedy Decoding이라고 칭합니다.\n","  \n","탐욕적인 방법은 효율적이지만 최적의 해를 구해준다는 보장이 없습니다.  \n","즉 우리는 지금 최고의 번역을 생성하고 있는 것이 아닌 거죠! 최고의 확률을 갖는 단어가 최고의 번역이 아니라는 것은 조금 와닿지 않으실 수도 있습니다.  \n","  \n","아래 예제로 자세하게 이해해 봅시다.\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-03.max-800x600.png'>\n","<br><br>\n","앞서 통계적 기계 번역 파트에서 배운 것처럼 우리는 문장이 존재할 확률을 구할 수 있죠?  \n","실제 세계에선 굳이 계산해 보지 않더라도 커피를 한 잔 가져도 될까요? 보다 커피를 한 잔 마셔도 될까요? 가 높은 확률로 존재할 것을 예상할 수 있습니다. \n","   \n","하지만 훈련 데이터가 실제 세계의 모든 데이터를 포함할 수는 없기 때문에,  \n","have 가 마시다 로 사용되는 경우가 훈련 데이터에 적거나 없었다면 탐욕적인 방법은 have 를 가장 높은 확률을 갖는 가지다 로 번역할 수밖에 없었을 겁니다.  \n","실제 세계에서는 높은 확률로 존재하는 문장이지만 훈련 데이터 세계에서는 커피를 가지는 게 더 타당하다는 거죠!  \n","  \n","이를 어떻게 해결할 수 있을까요? 단어 사전으로 만들 수 있는 모든 문장을 만든 후, 실제 세계에 존재하는 우리가 직접 고르는 방법은 어떤가요?  \n","확실히 가장 멋진 문장을 골라낼 수는 있을 것 같습니다.  \n","1,000개의 단어를 갖는 사전으로 3개 단어 문장 하나를 만드는 데에 1,000,000,000개 문장이 서비스로 온다는 것은 조금 문제일 수도 있지만요!"],"metadata":{"id":"jYANjHUkRF04"}},{"cell_type":"markdown","source":["---\n","# 지적 생성을 위한 넓고 얕은 탐색 (2) Beam Search\n","---"],"metadata":{"id":"ux0WPaLOR8e2"}},{"cell_type":"markdown","source":["농담처럼 말했지만 모든 문장을 다 만들어보는 것은 $O(V^L)$의 복잡도를 갖는다는 것을 제외하곤 좋은 방법이긴 합니다.  \n","확실히 원하는 문장이 만들어질 거라는 보장이 있으니까요! 효율성 측면에서 문제가 있으니, 그 부분을 개선하면 되지 않을까요?\n","  \n","Beam Search 는 그런 아이디어에서 출발한 알고리즘입니다.  \n","단어 사전으로 만들 수 있는 모든 문장을 만드는 대신, 지금 상황에서 가장 높은 확률을 갖는 Top-k 문장만 남기는 거죠!  \n","  \n","앞서 사용한 예문을 다시 빌려와 볼까요?\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-04.max-800x600.png'>\n","<br>\n","\n","상위 몇 개의 문장을 기억할지는 Beam Size(혹은 Beam Width라고 함)로 정의해 줄 수 있습니다.  \n","위 예시는 Beam Size를 2로 하는 Beam Search를 표현한 것이죠.  \n","Beam Size는 연산량과 성능 간의 Trade-off 관계를 가지고 있습니다.  \n","다시 말해, 자원이 무한하다면 Beam Size를 키우면 키울수록 성능이 좋아진다는 겁니다.  \n","직접 실험을 통해 찾아보는 것이 좋으나 대체로 5 나 10 을 적합한 값으로 택하곤 합니다.\n","\n","구현을 모르곤 완벽히 이해했다고 할 수 없겠죠? 간단한 예제로 Beam Search를 완벽하게 이해해 봅시다."],"metadata":{"id":"gr4sY569SKu0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iZVtoOELpxj"},"outputs":[],"source":["import math\n","import numpy as np\n","\n","def beam_search_decoder(prob, beam_size):\n","    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n","\n","    for tok in prob:\n","        all_candidates = []\n","\n","        for seq, score in sequences:\n","            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n","                candidate = [seq + [idx], score * -math.log(-(p-1))]\n","                all_candidates.append(candidate)\n","\n","        ordered = sorted(all_candidates,\n","                         key=lambda tup:tup[1],\n","                         reverse=True) # 총점 순 정렬\n","        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n","\n","    return sequences"]},{"cell_type":"code","source":["vocab = {\n","    0: \"<pad>\",\n","    1: \"까요?\",\n","    2: \"커피\",\n","    3: \"마셔\",\n","    4: \"가져\",\n","    5: \"될\",\n","    6: \"를\",\n","    7: \"한\",\n","    8: \"잔\",\n","    9: \"도\",\n","}\n","\n","prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n","            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n","            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n","            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n","            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n","            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n","\n","prob_seq = np.array(prob_seq)\n","beam_size = 3\n","\n","result = beam_search_decoder(prob_seq, beam_size)\n","\n","for seq, score in result:\n","    sentence = \"\"\n","\n","    for word in seq:\n","        sentence += vocab[word] + \" \"\n","\n","    print(sentence, \"// Score: %.4f\" % score)"],"metadata":{"id":"KtPJS9uDSy8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터 세계에서는 커피 를 마셔 라고도 부르나 보네요(😂😂).  \n","우리가 확인하고자 했던 커피를 마셔도 될까요? 에 더하여 마셔를 가져도 될까요? 까지 확인할 수 있었습니다.  \n","굳이 고정된 개수의 문장을 얻지 않아도 된다면 적당한 Beam Size를 설정해 준 후 Score를 기준으로 필터링하는 방법도 좋겠죠?\n","\n",">❗유의  \n","Beam Search는 사람이 직접 좋은 번역을 고를 수 있게 상위 K개의 결과를 보여줄 뿐이라서 학습에 직접적으로 적용할 수는 없습니다.  \n","모델 입장에서는 뭐가 좋은 번역인지 알 수 없으니까요!  \n","즉 모델 학습 단계에서 Beam Search를 사용하지는 않습니다."],"metadata":{"id":"tm42e3DuS1Ip"}},{"cell_type":"markdown","source":["---\n","# 지적 생성을 위한 넓고 얕은 탐색 (3) Sampling\n","---"],"metadata":{"id":"AqeHQ4_-TDIf"}},{"cell_type":"markdown","source":["Sampling은 자주 사용되는 방법은 아니지만 굉장히 흥미롭습니다.  \n","앞서 살펴본 방법들은 다음의 단어로 나올 확률이 높은 단어를 선택하는 방식이었죠? 그게 가장 유력한 하나를 선택하든, 여러 개를 살피든 말입니다.  \n","이 방식은 동일한 입력에 대해 늘 동일한 결과가 만들어집니다. 안정적이라고 할 수도 있겠지만, 작문이라는 것은 이보다 조금 더 멋진 일인 걸요!  \n","\n","어떻게 하면 매번 새로우며 의미가 유지되는 문장을 만들 수 있을까요? 지금까지 확률을 기준으로 단어를 선택해왔으니, 확률적으로 단어를 뽑는 방법은 어떨까요?  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-05.max-800x600.png'>\n","<br><br>\n","Sampling은 그 아이디어를 기반으로 제안된 방법입니다.  \n","언어 모델은 반복적으로 다음 단어에 대한 확률 분포를 생성하기 때문에 그 확률 분포를 기반으로 랜덤하게 단어를 뽑아 보자는 거죠.  \n","위 예시를 따라서 100문장을 만들면 이상적인 경우 55개의 내가 사랑하는 귀여운 고양이, 37개의 내가 사랑하는 귀여운 강아지... 와 같이 문장이 생성되는 겁니다.  \n","높은 확률을 갖는 단어를 택하는 경우가 가장 많기 때문에 랜덤이지만 지나치게 뜬금없는 문장이 생성되지는 않겠죠?  \n","  \n",">\"Sampling은 대체 어디에 사용해야 하지..?\"  \n","\n",">Sampling은 간혹 정말 난해한 문장을 생성할 수도 있기 때문에 실제 서비스에서는 거의 사용되지 않습니다.  \n","대신 모델을 학습시킬 때 사용되는 경우가 있는데요,  \n","잠시 후에 배울 Back Translation(역번역) 이 대표적인 사례입니다.  \n","또한 강화 학습의 입실론 그리디(E-Greedy)와도 궤를 함께하기 때문에 자연어 처리에 강화 학습을 적용한 경우에 등장하기도 한답니다!"],"metadata":{"id":"4koYDjSzTc46"}},{"cell_type":"markdown","source":["---\n","#  방과 후 번역 수업 (1) Data Augmentation\n","---"],"metadata":{"id":"IU1jw123TzSA"}},{"cell_type":"markdown","source":["모델을 훈련시키는 법을 배웠고, 훈련된 모델이 문장을 생성하게 하는 법도 3가지나 배웠습니다!  \n","이 모든 것들이 정규 수업이었다면, 지금부터 배울 것은 방과 후 보충수업이랄까요?\n","\n","Data Augmentation은 '데이터 증가', '데이터 확대' 등으로 직역되며 훈련 데이터를 수십 배까지도 부풀리는 기술을 의미합니다!  \n","주로 이미지 처리 영역에서 많이 사용되는데요, 그 방법을 이해하고 나면 '그럴 수밖에 없겠구나...' 생각이 드실 겁니다. \n","  \n","이미지 데이터는 늘리고 돌리고 찌그러트리고... 어떤 변화를 시켜도 왜곡이 크지 않고 심지어 일괄적으로 처리할 수 있다는 장점이 있습니다.  \n","하지만 문장 데이터에 적용이 가능한 Augmentation 기법은 쉽게 떠오르지 않죠. 뭔가 '어투를 바꾼다거나, 단어를 대치시킨다던가?' 하는 어렴풋한 느낌만 오실 겁니다.  \n","그리고 떠오른 몇몇 방법들은 구현하려니 금방 예외 케이스가 떠올라버리고... 자연어 처리에서는 Augmentation을 할 수 없는 걸까요..?\n"," \n","당연히 아닙니다! 다행히도 멋진 연구자분들께서 자연어 처리에 적용할 수 있는 기법들을 만들어 주셨답니다!  \n","대신 이미지 처리처럼 단순한 방법은 아니라서 이해하는 데에 시간이 조금 필요할 수도 있습니다.  \n","천천히 시작해 보죠!"],"metadata":{"id":"B8DtLgh0VAyf"}},{"cell_type":"markdown","source":["---\n","# 방과 후 번역 수업 (2) Lexical Substitution\n","----\n","\n","Lexical Substitution은 '어휘 대체'로 직역됩니다.  \n","방금 여러분들이 어떤 방법을 떠올렸다면 그것은 아마도 이쪽에 속하는 방법일 거예요!  \n","대체할 어휘를 선정하는 기준에 따라서 기법이 세부적으로 나뉘는데,  \n","  \n","총 3가지를 배워보도록 하죠!"],"metadata":{"id":"E4lTtj-4V-lM"}},{"cell_type":"markdown","source":["---\n","### 동의어 기반 대체\n","---\n","\n"],"metadata":{"id":"40GXr_RMWGsc"}},{"cell_type":"markdown","source":["시소러스(Thesaurus) 란, 어떤 단어의 동의어나 유의어를 집중적으로 구축해놓은 사전을 의미합니다.  \n","동의어 기반 대체는 이 시소러스를 활용한 방법입니다.  \n","  \n","위 방식을 통해 Augmentation을 하면 아래와 같은 느낌이 되겠죠?\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-6-L-07.png'>\n","<br><br>\n","동의어를 기반으로 대체하는 것은 아주 좋은 방법이지만, 규칙 기반 기계 번역처럼 모든 것을 사람이 정의해야 한다는 것이 단점입니다.  \n","  \n","조금 더 편리한 방법은 없을까요?"],"metadata":{"id":"poadtgTUWLwb"}},{"cell_type":"markdown","source":["---\n","### Embedding 활용 대체\n","---"],"metadata":{"id":"WHHuhTzCXHy5"}},{"cell_type":"markdown","source":["Pre-training Word Embedding을 활용하는 방법이 바로 그것입니다.  \n","우리는 이미 Word2Vec이나 GloVe 등의 기법들을 배웠죠.  \n","이 기법들을 통해 학습된 Embedding은 유사한 단어들끼리 비슷한 공간에 밀집되던 것을 기억하실 겁니다.  \n","사람이 일일이 정의한 데이터베이스 대신, 이 Embedding의 유사도를 기반으로 단어를 대체하면 훨씬 편리하겠죠!  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-08.max-800x600.png'>\n","<br>\n","\n","gensim 라이브러리를 활용하면 이 방법은 아주 쉽게 사용할 수 있겠어요!  \n","단어를 유사도 순으로 정렬해 보여주는 most_similar() 함수, 알고 계시지요?"],"metadata":{"id":"z5ZyE1_iXLVd"}},{"cell_type":"markdown","source":["---\n","### TF-IDF 기반 대체\n","---"],"metadata":{"id":"L2CSFaO-XY92"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-09.max-800x600.png'>  \n","<br>\n","TF-IDF는 여러 문서를 기반으로 단어마다 중요도를 부여하는 알고리즘입니다.  \n","문서의 핵심이 되는 소년, 피리 같은 단어들은 높은 TF-IDF 값을 부여받고,  \n","한, 를 과 같은 단어들은 낮은 TF-IDF 값을 가지게 됩니다.  \n","  \n","이때, 낮은 TF-IDF 값을 갖는 단어들은 핵심 단어가 아니기 때문에 다른 단어로 대체해도 문맥이 크게 변하지 않는다는 것에 주목한 아이디어입니다."],"metadata":{"id":"4bLcFeyhXbUH"}},{"cell_type":"markdown","source":["---\n","# 방과 후 번역 수업 (3) Back Translation\n","---"],"metadata":{"id":"b0dw2FsvXltf"}},{"cell_type":"markdown","source":["Back Translation은 단일 언어 데이터는 구하기 쉽고 많지만 병렬 쌍을 이룬 언어 데이터를 찾기는 어렵다는 문제를 해결하고자 등장했습니다.  \n","번역 모델에 단일 언어 데이터를 학습시키는 방법이죠!\n","\n","Sequence-to-Sequence의 구조를 되새겨보면, Source 문장을 Encoding 하는 부분과 Target 문장을 Decoding 하는 부분을 분리하여 모듈들이 각 언어를 더 잘 처리할 수 있게 했었습니다.  \n","그렇다면 Encoder에는 Source 언어로 된 문장을, Decoder에는 Target 언어로 된 문장을 좀 더 훈련시키면 어떨까요?\n","\n","[Back Translation 정리](https://dev-sngwn.github.io/2020-01-07-back-translation/)\n","\n","변환할 단어를 선정하고 유사한 단어로 대체하던 기존 방식과는 사뭇 다르죠?  \n","딥러닝 모델을 사용한다는 점이 이미지 처리에서 GAN으로 Augmentation 하는 것을 떠올리게 합니다.  \n","Back Translation은 현재 번역 데이터셋으로 가장 유명한 WMT2014에서 State-of-the-art 성능을 보인 알고리즘이랍니다!"],"metadata":{"id":"mq0ClHVMcEiJ"}},{"cell_type":"markdown","source":["---\n","# 방과 후 번역 수업 (4) Random Noise Injection\n","---\n","\n","데이터에 포함된 적당한 노이즈는 때때로 학습에 도움이 되기도 합니다.  \n","그런 의미에서 문장에 노이즈를 주는 것도 괜찮은 Augmentation 기법이 될 수 있겠죠!  \n","이 또한 노이즈를 주는 기준에 따라서 방법이 여러 가지로 나뉘는데, 간단하게 3가지 정도만 알아봅시다."],"metadata":{"id":"6ozbiBDpc1XJ"}},{"cell_type":"markdown","source":["---\n","### 오타 노이즈 추가\n","---\n","\n","채팅 중 ㅋㅋㅋㄱㄱㅋㅋㄱ 와 ㅋㅋㅌㅋㅌㅌㅋㅋ 를 보고 뭘 알아낼 수 있을까요?  \n","바로 상대방이 사용하는 키보드 자판입니다! 전자는 천지인 자판을 쓰고 있고 후자는 QWERTY 자판을 쓰고 있네요.  \n","  \n","이유는 여러분들도 알고 계시겠죠?\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-10.max-800x600.png'>\n","<br><br>\n","타이핑을 할 때 주변 키가 눌려 발생하는 오타는 굉장히 자연스럽습니다.  \n","이를 이용한 Augmentation 기법이 바로 오타 노이즈를 추가하는 것이죠!  \n","올 때 아이스크림 사와 를 놀 때 아이스크림 사와 로 바꾸는 등 QWERTY 키보드 상에서 키의 거리를 기반으로 노이즈를 추가하는 방법입니다!"],"metadata":{"id":"kAli-Cv5c6wc"}},{"cell_type":"markdown","source":["---\n","### 공백 노이즈 추가\n","---"],"metadata":{"id":"9oTgNIASoJ5_"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-11.max-800x600.png'>\n","<br>\n","\n","엄연히 말하면 완벽한 공백이 아니고, _ 토큰을 활용하며 이를 Placeholder Token(이하 공백 토큰) 이라고 부릅니다.   \n","문장의 일부 단어를 공백 토큰으로 치환하는데요, 학습의 과적합을 방지하는 데에 좋은 효과를 볼 수 있다고 합니다."],"metadata":{"id":"0wF66kWuoL7K"}},{"cell_type":"markdown","source":["---\n","### 랜덤 유의어 추가\n","---"],"metadata":{"id":"ZwpWErNEoW-d"}},{"cell_type":"markdown","source":["위 방식들은 노이즈 추가라고 칭했지만 대체로 문장의 의미를 유지하며 문장을 변환하는 형태였죠.  \n","이 방식은 정말 노이즈를 추가합니다! 주어진 문장에서 불용어(Stop word)가 아닌 단어를 랜덤하게 뽑은 후, 해당 단어와 유사한 단어를 골라 문장에 아무렇게나 삽입하는 방식입니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-12.max-800x600.png'>\n","<br>\n","\n","앞서 배운 Lexical Substitution과 비슷한 느낌이지만 원본 단어가 손실되지 않는다는 것이 조금 더 매력적이죠?  \n","Word2Vec의 아이디어를 생각하면 유사어를 삽입하는 것이 모델의 Embedding 층을 더 견고하게 만들어 줄 것 같네요!"],"metadata":{"id":"oPOVmL7gpun4"}},{"cell_type":"markdown","source":["---\n","# 채점은 어떻게?\n","---"],"metadata":{"id":"-ZR9fWFWqFw6"}},{"cell_type":"markdown","source":["앞서 진행한 프로젝트에서도, Beam Search가 생성한 문장을 고를 때도, 우리는 번역을 직접 읽고 평가했습니다.  \n","직접 읽어보고 '좋다', '나쁘다' 라고 판단하는 것은 그럴듯하지만 너무나도 주관적이죠.  \n","게다가 번거로워요! 따라서 번역의 품질을 쉽고 빠르게 평가할 수 있는 객관적인 지표가 필요합니다.  \n","  \n","BLEU(Bilingual Evaluation Understudy) Score는 이에 발맞춰 등장한 번역 평가 지표입니다.  \n","'기계가 실제 번역을 얼마나 잘 재현했는가?' 를 평가하는 지표인데,  \n","'번역을 평가해야 한다!' 라는 연구자의 깊은 고민이 느껴지는 개념이니 아래 웹페이지들에서 자세한 내용을 알아보세요.  \n","두 개의 글을 첨부해 드릴 테니, 앞의 글을 가볍게 읽으시고 그다음 글로는 자세한 이해를 하도록 합시다.\n","\n","  \n","사실 번역도 단 한 가지 정답이 존재하는 것이 아닌데, 그것을 객관적으로 평가한다는 것에 대한 찝찝함이 있을 수 있습니다.  \n","하지만 이렇게나 많은 고민이 담긴 지표라면, BLEU Score가 번역을 평가하는 것도 제법 설득력이 있죠?  \n","언젠가 Reference 문장에 없는 단어라도 의미적으로 유사하다면 높은 점수를 줄 수 있는 더 멋진 지표가 탄생할 거라 기대합니다!\n","\n","기계 번역은 BLEU Score로 줄 세우기가 가능하다는 것을 알았습니다.  \n","혹시 다른 Task에 대한 평가 지표도 궁금하지 않으신가요?  \n","자연어 이해(Natural Language Understanding)에 관련된 최신 논문을 본 적이 있으시다면 GLUE 라는 것을 보셨을 거예요.  \n","  \n","GLUE는 기계 번역 너머의 자연어 이해를 평가하기 위해 고안된 지표입니다.  \n","지금은 자세한 내용보다 가볍게 읽기 좋은 글을 첨부하고 넘어갈게요!\n","\n","[GLUE: 벤치마크를 통해 BERT 이해하기 - Programador | Huffon Blog](https://huffon.github.io/2019/11/16/glue/)"],"metadata":{"id":"Csz0alGiq3wM"}},{"cell_type":"markdown","source":["---\n","# 실례지만, 어디 챗씨입니까? (1) 챗봇과 번역기\n","---"],"metadata":{"id":"DgoPDcpXu64W"}},{"cell_type":"markdown","source":["우리는 윗글에서 언급된 생성 모델에 대해서 알아볼 거예요! 번역 모델을 챗봇에 적용하는 아이디어는 Sequence-to-Sequence에서 시작하는데요,  \n","앞서 Back Translation을 설명할 때 사용한 문단을 인용하도록 하죠.\n","\n",">Sequence-to-Sequence의 구조를 되새겨보면, Source 문장을 Encoding하는 부분과 Target 문장을 Decoding하는 부분을 분리하여 모듈들이 각 언어를 더 잘 처리할 수 있게 했었습니다. ... (하략)\n","\n","챗봇은 사람과 대화를 하는 게 목적인 모델이죠?  \n","그렇다면 질문만 하는 나라의 언어를 Source언어라고 하고 답변만 하는 나라의 언어를 Target 언어라고 한다면  \n","Source 문장을 Target 문장으로 번역하는 행위가 곧 질문에 답하는 행위가 되는 셈이죠?!  \n","물론 다양한 일상적인 대화도 이 같은 방법으로 학습이 가능하고요.\n","  \n","조금 더 자세하게 설명하자면, Encoder는 Source 문장을 읽고 이해한 내용을 추상적인 고차원 문맥 벡터로 압축합니다.  \n","Decoder는 Encoder가 만든 문맥 벡터를 바탕으로 Target 문장을 생성하죠.  \n","이 과정은 Source 언어의 Embedding 공간 속 문장을 Target 언어의 Embedding 공간으로 매핑한다고 할 수 있습니다.  \n","수많은 데이터로 학습하는 것은 그 과정을 더 잘 해내기 위함이죠!  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-6-L-14.max-800x600.png'>\n","<br>  \n","이것은 모든 Source 문장에 대해 매핑될 수 있는 고유한 Target 번역이 존재하기 때문에 가능합니다.  \n","커피를 더 마시고 싶어 라는 문장을 영어로 올바르게 번역하는 경우의 수가 많아야 10개는 될까요?  \n","그 모든 경우의 수 또한 I want some more coffee 에서 크게 엇나가지 않겠죠.  \n","이처럼 사과는 무슨 색이야? 에 대한 답변도 빨간색입니다 에 수렴하기 때문에 번역기를 챗봇으로 사용하는 게 가능한 거죠! \n","\n"],"metadata":{"id":"GnF-25mKwbSs"}},{"cell_type":"markdown","source":["---\n","# 실례지만, 어디 챗씨입니까? (2) 좋은 챗봇이 되려면\n","----\n","대신 신경 써야 할 부분이 몇 가지 있습니다. 대표적인 4가지만 알아볼까요?"],"metadata":{"id":"5z4uFV3SxA9b"}},{"cell_type":"markdown","source":["---\n","### 200ms\n","---\n","\n","200ms는 대화가 자연스럽게 느껴지는 답변의 공백 마지노선입니다.  \n","인간이 챗봇한테 말을 걸었을 때, 0.2초 이내에 답변이 나오는 것이 바람직하다는 거죠!  \n","스마트폰에 탑재된 인공지능 비서는 가장 단순하게 생각해도 음성 인식, 답변 추론, 음성 생성까지 해야 하는데 0.2초면 제법 가혹하죠?  \n","실제로는 10개 이상의 모듈이 동작하기 때문에 각 모듈에 0.01초 수준의 시간이 주어진다고 합니다.  \n","우리, 조금만 더 관대해지도록 해요."],"metadata":{"id":"Opf-hgB_xEui"}},{"cell_type":"markdown","source":["---\n","### 시공간을 담은 질문\n","----\n","\n","거창한 제목이지만, 사실은 오늘 무슨 요일이야? 를 말하는 겁니다.  \n","앞서 배운 것처럼 질문이 고유한 답변을 가진다면 챗봇이 학습하기 좋지만 요일을 묻는 질문은 매일 답변이 변하니 학습을 할 수가 없겠죠?  \n","오늘 금천구 날씨 어때? 라고 묻는 것도 마찬가지입니다.  \n","특정 시공간에 의해 결정되는 질문은 단순한 학습으로 답변할 수가 없어요.  \n","그래서 보통은 질문의 의도를 파악한 후, 시공간에 대한 질문은 다른 모듈로 연결하여 예외 처리를 하곤 한답니다.  \n","앞의 인공지능 비서가 왜 10개 이상의 모듈을 갖는지 조금 이해가 가죠?"],"metadata":{"id":"S9DT4pArxQtd"}},{"cell_type":"markdown","source":["---\n","### 페르소나\n","---\n","\n","앞의 인용 글에서 인격의 일관성 (Coherent Personality) 이라는 주제로 다뤄진 것과 유사한 얘기입니다.  \n","학습에는 주로 많은 사람들의 채팅 데이터를 모아서 사용할 수밖에 없기 때문에 모델이 대답의 일관성을 갖는다는 것은 굉장히 도전적이죠.  \n","이때의 일관성을 모델의 인격이라고 칭하며, 그것을 페르소나라고 부릅니다.  \n","영화를 좋아하시는 분들이라면 한 번쯤 들어본 적이 있는 단어죠?\n","\n"],"metadata":{"id":"T4UaK7DgxcBb"}},{"cell_type":"markdown","source":["---\n","### 대화의 일관성\n","---\n","\n"],"metadata":{"id":"iY2oZVr3x1R2"}},{"cell_type":"markdown","source":["인공지능에게 본인이 좋아하는 색깔이나 장소를 말해 본 적이 있나요? 아마 대다수는 없겠지만,  \n","만약 있다면 아직 기억하고 있는지 다시 물어보세요.  \n","기억하고 있을 리가 없죠? 무심한 인공지능 같으니라고!  \n","   \n","챗봇이 페르소나를 가지는 것처럼, 사용자의 페르소나도 어딘가에 저장해놓고 적재적소에 사용한다면 정말 좋겠지만,  \n","아직까지 사용자에 관한 정보를 기억하고 이를 활용하는 모델은 없어요.  \n","대신 너무 뜬금없는 대답을 막아서 대화의 일관성을 유지하려는 시도는 있었죠.  \n","\n","기존의 Source 질문을 Target 답변으로 매핑하는 훈련법은 정답을 맞히게끔 학습하기 때문에 문제가 발생합니다.  \n","모든 질문에 대한 정답인 무슨 말인지 모르겠어요 가 존재하기 때문이죠.  \n","또한, See you later 를 See you later 로 대답하는 것은 정답이지만, 그 \n","대답에다 또 See you later 를 하는 것은 이상하죠?  \n","   \n","다음에 만나자는 무한 루프에 빠지고 맙니다!  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-6-L-15.png'>\n","<br>  \n","재밌는 부분은 저자들이 좋은 대화를 정의하고 그걸 학습하게끔 모델을 설계했다는 거예요!  \n","저자들은 \n","\n",">1) 상대방이 답변하기 좋고,  \n","2) 새로운 정보를 담고 있으며,  \n","3) 일관성이 있는 말을 각각 보상으로 정의하여 이를 최대화하는 방향으로 학습을 진행했습니다. \n","\n","딥러닝이 Loss를 최소화하듯이, 강화학습은 보상을 최대화하거든요.\n","\n","토픽이 '대화의 일관성' 이니만큼 3) 일관성이 있는 말에 대해 조금만 더 살펴볼게요.  \n","대화를 [ q1, a1, q2, a2, ... ] 라고 정의하고 q, a는 각각 질문과 답변입니다.  \n","a2를 생성할 때에는 a1과 q2를 보고 생성을 하고, 해당 문장이 생성될 확률이 곧 보상이 됩니다.  \n","하지만 앞서 말했듯이 모르겠어요 는 모든 질문에 대해 생성될 확률이 높은 문장이므로 자칫하면 a2 가 모르겠어요 여도 큰 보상을 주게 되겠죠?\n","  \n","이에 저자들은 a2를 보고 q2를 유추할 수 있는지를 보상에 추가했습니다.  \n","그렇게 되면 a2가 q2의 정보를 포함하지 않을 때 좋은 보상을 받을 수 없겠죠.  \n","이는 곧 일관성 있는 대화를 생성하는 방향으로 학습이 진행되게끔 만듭니다!  \n","   \n","이 Bidirectional 한 아이디어, 아주 멋지지 않나요?\n"],"metadata":{"id":"hZto5KcXya1O"}},{"cell_type":"markdown","source":["---\n","# 실례지만, 어디 챗씨입니까? (3) 대표적인 챗봇\n","---\n","\n","그럼 지금 최강인 챗봇은 강화학습 기반인가요..?\n","\n","다행스럽게도(?) 아니랍니다. 현재 가장 멋진 성능을 보여주고 있는 챗봇은 딥러닝 기반이에요!  \n","대신 엄청난 규모의 데이터를 엄청난 크기의 모델에 학습시킨...  \n","그야말로 거대한 녀석들이 선두 주자를 맡고 있죠."],"metadata":{"id":"iqI0MLfXzWWp"}},{"cell_type":"markdown","source":["---\n","### Meena\n","---\n","\n","Meena는 구글이 만든 챗봇입니다. \n","거대하기로 빼놓으면 섭섭한 GPT-2보다 2배가량 큰 모델을 9배 더 많은 데이터로 학습한 친구예요! 모델 구조는 Evolved Transformer를 사용했답니다.   \n","무엇보다 자체적인 대화 평가 지표인 SSA를 제안하고 이를 분석한 부분이 굉장히 인상적이에요.\n","  \n","[무슨 대화든 할 수 있는 에이전트에 대하여](https://brunch.co.kr/@synabreu/35)"],"metadata":{"id":"R7zh3HAbzeNh"}},{"cell_type":"markdown","source":["---\n","# Blender\n","---\n","\n","이후에 등장한 Facebook의 Blender도 못지않게 멋진 녀석입니다!  \n","모델에 페르소나를 부여하고자 하는 시도와 자체적인 평가 지표 ACUTE-Eval을 제안한 것이 인상적이죠.  \n","제법 난이도가 있기 때문에 정리 글 속 가볍게 읽히는 부분만 읽어보도록 해요!\n","\n","[블렌더(Blender) - Facebook AI의 오픈 도메인 챗봇](https://littlefoxdiary.tistory.com/39)"],"metadata":{"id":"8Dx2yzM52yFh"}},{"cell_type":"code","source":[""],"metadata":{"id":"Wu2NRdV1S_t1"},"execution_count":null,"outputs":[]}]}