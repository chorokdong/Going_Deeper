{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GD13_modern NLP의 흐름에 올라타보자.ipynb","provenance":[],"authorship_tag":"ABX9TyOsehyYsmJWhQGHId6ss7V4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","# 들어가며\n","---"],"metadata":{"id":"Y7c8vJn9JXoJ"}},{"cell_type":"markdown","source":["---\n","### 변화의 흐름 - Word Embedding과 Context\n","---\n","<br>  \n","우리는 NLP task를 풀기 위하여 인간의 언어를 컴퓨터의 언어로 바꾸어 주는 작업을 반드시 거쳐야만 합니다(임베딩). 단어가 가지고 있는 의미를 숫자에 녹여내기 위해서 tf-idf, word2vec, fasttext 등 다양한 임베딩 방법들을 공부해 보기도 했지요. 우리가 기존에 배웠던 임베딩 방식은 워드임베딩(Word Embedding)이라고 합니다. 즉, 단어 하나하나를 임베딩하는 방식이지요.\n","<Br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/01_qSVU9Sq.png'>\n","<br><br>\n","워드 임베딩은 학습이 완료되면 각 단어 하나하나가 특정 임베딩(벡터)을 가지게 됩니다. 그래서 우리는 위와 같이 단어 사이의 거리를 재고, 관계를 파악할 수 있었던 것이죠.\n","\n","혹시...\n","\n","생각해 본 적 있나요?\n","생긴 것은 똑같이 생겼는데 다른 뜻을 가지고 있는 단어들(동음이의어, 다의어 등)은 어떻게 표현될지?\n","\n","없어도 괜찮아요. 이제 생각해 보면 되죠.🤓\n","예시를 보면서 생각해 봅시다.\n","\n","- 예시 1) bank\n","  <img src='https://d3s0tskafalll9.cloudfront.net/media/images/03_MdCqjaS.max-800x600.png'>\n","\n","- 예시 2) 차\n","  - 시간이 늦었으니 커피 말고 차나 한 잔 마시자\n","  - 제주도는 버스로 움직이기 힘들어. 차가 있어야 해.\n","  - 축구공을 이쪽으로 차.\n","\n","이제 어떤 문제가 발생하는지 감이 오시나요?\n","\n","워드 임베딩 방법들은, 주변 단어를 학습할 때만 고려하게 됩니다.  \n","다른 모델의 입력으로써 사용할 때(학습된 벡터들을 이용하여 input 단어들을 벡터화시킬 때)는  \n","동음이의어, 다의어와 같은 경우들을 모두 고정된 하나의 벡터로 표현하게 되면서 주변의 단어들을 고려하지 않게 되는 것이죠.  \n","정작 학습할 때는 문맥을 열심히 보더니, 테스트할 때는 현재 단어 한 개만 보는 모양새가 되어버렸습니다.  \n","  \n","똑똑한 여러분은 '문맥을 고려하는 임베딩(Contextual Embedding)을 만들면 되지 않나?!'라고 생각하실 겁니다.\n","\n","네! 맞습니다! 바로 앞으로 등장하는 모델들은 이러한 문맥(context) 을 잘 반영하는 모델들입니다."],"metadata":{"id":"bCn9LlyQJZ-I"}},{"cell_type":"markdown","source":["----\n","# Transfer Learning과 Language Modeling\n","----"],"metadata":{"id":"UXSv_o5ZKJGB"}},{"cell_type":"markdown","source":["---\n","### Transfer Learning(전이 학습)\n","---"],"metadata":{"id":"OuKN9IM2KLNI"}},{"cell_type":"markdown","source":["전이 학습이란 말 들어보셨나요? 단어 자체는 들어보지 않아도 개념들은 한 번씩 접해보셨으리라 생각합니다.\n","\n","pretrain과 fine-tuning으로 더 많이 들었을지도 모르겠네요.\n","\n","전이 학습은 특정 환경에서 학습을 마친 신경망(일부 혹은 전부)을 유사하거나 다른 환경에서 사용하는 것을 의미합니다.  \n","쉽게 생각해 보면, 사과를 깎는 것을 배운 아이에게 배를 깎도록 다시 학습시키는 거죠.  \n","이미 사과를 깎는 것을 배웠으니 비슷하게 생긴 배도 유사한 방법으로 쉽게 깎을 수 있을 것이라고 생각이 들죠?\n","\n","이처럼 전이 학습을 이용하게 되면 적은 데이터로도 성능을 곧잘 만들어 낼 수 있다는 이점이 있습니다.\n","\n","그렇다면 자연어에서의 전이 학습은 언제, 어디서 사용되는 것일까요?\n"],"metadata":{"id":"hcRXBajkKMZd"}},{"cell_type":"markdown","source":["---\n","### Language Modeling(언어 모델)\n","---\n","<br>\n","\n","자연어 처리에서의 전이 학습은 보통 language model(언어 모델)과 관련이 깊습니다.\n","\n","언어 모델은 입력으로 주어진 시퀀스의 다음을 학습하는 과정에서 주어진 시퀀스가 얼마나 자연스러운지를 학습하게 됩니다.  \n","즉, 언어 모델은 철수가 밥을 마셨다인지 철수가 밥을 먹었다인지를 데이터로부터 학습을 하게 됩니다.  \n","이렇게 학습을 완료한 언 어모델은 언어의 패턴과 규칙을 학습하여 전반적인 언어의 특징을 익히게 됩니다.\n","\n","자연어 처리에서 바로 이 언어 모델이 pretrained model이 되는 것입니다.  \n","이미 언어의 전반적인 것을 아는 신경망에게 언어와 관련된 문제를 풀게 하는 것이지요.  \n","이처럼 주어진 문제(다운스트림 테스크 혹은 downstream task)를 잘 풀기 위해 pretrained model을 재학습시키는 것을 fine-tuning이라고 부릅니다."],"metadata":{"id":"gM09X7QYKiX8"}},{"cell_type":"markdown","source":["---\n","### Transformer\n","---\n","<br>\n","사실 최근 NLP의 가장 큰 흐름은 트랜스포머라고 해도 무방하다고 생각합니다.  \n","GPT, BERT 등 이후에 만들어지는 모델들은 트랜스포머가 기반이기 때문이죠.  \n","또한, BERT를 시작으로 BERT를 개선하는 수많은 모델들이 나오기 시작했습니다. 성능 또한 향상되었구요!\n","\n","따라서 modern NLP라고 하면 Transformer를 빼놓을 수 없답니다!\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/04_XApPtfX.png'>\n","<br><br>\n","여기까지 오신 여러분들은 이미 트랜스포머의 모델 구조나 특징에 대해 익숙하실 거로 생각합니다.  \n","트랜스포머에 대해 간략하게 짚고 넘어가 보죠.\n","\n","트랜스포머는 Encoder-Decoder 구조로, RNN이나 LSTM 등을 사용하지 않고 attention만을 이용한 모델입니다.  \n","그래서 비록 LSTM 등 RNN 구조를 사용하지 않지만 번역기 모델 등에 사용하는 seq2seq 모델과 구조적으로는 동일합니다.  \n","Encoder-Decoder 모델이라면 입력부터 출력까지의 파이프라인이 이미 고정되어 있어서 이를 이용해 임베딩을 구하거나 전이 학습을 통해 다른 태스크에 활용하기 어려울 것 같습니다.  \n","그렇다면 transformer 모델이 어떻게 modern NLP의 토대를 이루는 중요한 모델로 발전할 수 있었을까요?\n","  \n","'너무 간략한 거 아니야?' 라고 생각하실 수도 있지만, 갈 길이 멀기 때문에 핵심적인 구조만 짚었습니다\n","  \n","최근 자연어 처리에서 가장 중요한 부분인 contextualized embedding과 transfer learning, transformer까지 대략 짚어 보았습니다.  \n","  \n","이 개념들을 통하여 최근 흐름에 올라타 보실까요?\n"],"metadata":{"id":"sjrddoCyKrQ8"}},{"cell_type":"markdown","source":["---\n","# ELMO(Embedding from Language Models)\n","---\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/05_xDakup4.max-800x600.png'>\n","\n","ELMo는 문맥(context)을 반영한 임베딩을 pretrained model로 구현한 첫 번째 사례입니다.  \n","언어 모델을 이용하여 임베딩을 한 것인데요. 이름에 포함된 LM이 바로 이 언어 모델을 가리키죠."],"metadata":{"id":"Q4N_nVtnLp_j"}},{"cell_type":"markdown","source":["---\n","### ELMo의 구조\n","---"],"metadata":{"id":"eeU7vubaL47f"}},{"cell_type":"markdown","source":["ELMo는 세 가지 요소로 구성되어 있습니다. 첫 번째는 character-level CNN, 두 번째는 bidirectional LSTM, 마지막으로 ELMo 레이어가 있습니다. 각각 어떤 역할을 하고 있는지 하나씩 살펴보도록 하겠습니다.\n","\n","1. character-level CNN\n","\n","character-level CNN은 입력된 문자들 간의 관계를 파악하고 임베딩 벡터로 변환하는 역할을 합니다.\n","\n","ELMo는 character level로 문자를 인식합니다. 좀 더 구체적으로 말한다면, 해당 character의 유니코드 ID를 입력으로 받습니다.  \n","예를 들어, 밥이라는 단어를 입력으로 받으면 ㅂㅏㅂ에 해당되는 유니코드 235, 176, 165 세 개의 숫자가 됩니다.\n","\n","이렇게 각각 입력받은 단어의 시작과 끝에 해당하는 스페셜 토큰 \\<BOW>와 \\<EOW>에 해당하는 유니코드를 앞뒤로 붙여줍니다.  \n","이후 각 유니코드 아이디에 해당하는 행 벡터를 참조하여 붙입니다.(look-up table)\n","\n","만들어진 벡터에 (n x 임베딩 차원 수) 사이즈의 필터로 컨볼루션하여 피처맵을 만들고 max-pooling하여 하나의 값을 뽑아냅니다.  \n","이러한 작업을 반복하여 사용자가 원하는 크기만큼의 벡터로 만들어냅니다.\n","\n","여기서 n은 한 번에 몇 개의 문자들을 함께 볼 것인가를 의미합니다.  \n","n이 2라면 \\<BOW> ㅂ, ㅂ ㅏ, ㅏ ㅂ, ㅂ \\<EOW> 이렇게 문자 2개씩 보면서 2개 사이의 관계를 파악하는 CNN이 됩니다.\n","\n","ELMo의 original 코드에서는 각기 다른 사이즈를 가진 7개의 필터를 이용하여 2048차원의 벡터를 만든다고 합니다."],"metadata":{"id":"Z55bixJLMLfw"}},{"cell_type":"markdown","source":["2. bidirectional LSTM\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/06_EXF2a0T.png'>\n","\n","character-level CNN을 통과하여 만들어진 벡터($E_1,E_2,...,E_N$)들은 bidirectional LSTM을 통과하게 됩니다.\n","\n","pretrain시에 bidirectional LSTM은 주어진 입력에 다음에 올 단어들을 예측합니다.(bidirectional LM이라고도 불리웁니다.)\n","\n","bidirectional이라는 것은 양방향으로 학습하는 것을 의미합니다.  \n","주어진 입력을 한 번은 순방향으로, 한 번은 역방향으로 각각 2개의 LSTM layer를 통과하게 됩니다.   \n","이렇게 양방향으로 학습하는 것은 모델의 사이즈와 학습에 걸리는 시간이 늘어날 수는 있지만 그만큼 성능이 좋답니다.\n","\n","pretrain 시, 순방향과 역방향으로 LSTM을 통과한 히든 벡터들은 이후 softmax(소프트 맥스)를 취해 다음 단어를 예측하게 됩니다.  \n","이때 ELMo는 순방향과 역방향의 벡터를 합치거나 더하지 않습니다.  \n","각각의 독립적인 모델처럼 행동하게 되는데요. 이는 조금만 생각해 보면 이유를 알 수 있습니다.  \n","  \n","하나는 순방향으로, 하나는 역방향으로 진행하다 보니 cheating(다른 방향의 모델에게 정답을 가르쳐 줌)의 가능성이 있기 때문이죠.\n","\n","이렇게 ELMo는 단어를 하나씩 하나씩 슬라이딩하여 다음 단어를 예측하면서 문장 내의 단어와 단어들 사이의 관계를 학습하게 됩니다.\n","\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/07_vLCaNHo.max-800x600.png'>"],"metadata":{"id":"T4vyx_jbMUrh"}},{"cell_type":"markdown","source":["3. ELMO 임베딩 레이어\n","\n","ELMo 임베딩은 pretrain이 끝나고 finetuning을 하는 과정에서 만들어집니다.\n","\n","맨 위 그림에서 Elmo 인형과의 대화처럼 stick이란 단어의 임베딩을 구한다고 가정해 봅시다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/08_X8OuDsj.max-800x600.png'>\n","\n","우선 구하려고 하는 토큰에 대한 각 층의 출력값을 모두 가지고 옵니다. 여기서의 각 층의 출력값이라고 하면, 임베딩 벡터(character-level CNN을 통과한 후 나오는 벡터), 각각의 LSTM layer에서의 hidden vector를 의미합니다.\n","\n","각 층 1,2,...,l,...마다 가중치 \n","$ s_1,s_2,...s_l,...$를 곱해서 모두 더해줍니다.(weighted sum 혹은 가중합을 해준다고 말할 수 있습니다.)\n","\n","마지막으로 다운스트림 태스크의 가중치 γ를 곱하면 ELMo의 임베딩이 됩니다.\n","\n","여기서 갑자기 등장하는 가중치 $s_1$와 γ는 다운스트림 태스크별로 finetuning 시 학습되는 값들입니다.\n","\n","복잡해 보이지만, 구하고자 하는 토큰에 대한 각 층의 출력값을 가중합한 것이 ELMo 임베딩입니다."],"metadata":{"id":"_wPxT0JlNBlt"}},{"cell_type":"markdown","source":["---\n","### ELMo의 이용\n","---\n","\n","이렇게 구한 ELMo는 어디에, 어떻게 사용할 수 있을까요?\n","\n","ELMo 만으로도 임베딩에 이용할 수 있지만, 기존에 학습시켰던 워드임베딩과 같이 사용할 수도 있습니다.\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/09_gHtb9F2.png'>"],"metadata":{"id":"lrMHtnoUNtg5"}},{"cell_type":"markdown","source":["---\n","# GPT(Generative Pre-Training Transformer)\n","---\n","\n","GPT는 트랜스포머의 decoder 구조만을 이용하여 만든 네트워크입니다.  \n","  \n","트랜스포머의 decoder를 아주 깊고 깊게 쌓아 많은 데이터를 학습 시켜 성능을 높힌 네트워크죠!"],"metadata":{"id":"k1zqSPAMOPpg"}},{"cell_type":"markdown","source":["---\n","### GPT의 구조\n","---\n","<br>\n","'Decoder만을 이용했다는 게 뭐야?'🤔 라는 생각이 드실 겁니다.\n","\n","Transformer-Decoder를 한 번 더 보고 가시죠.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/10_egROfLK.png'>\n","<br><br>\n","Decoder는 masked Multi-Head Attention, Multi-Head Attention, Feed Forward Neural Network로 이루어져 있었습니다.  \n","바로 이 구조를 차용하여 Decoder block를 많이 쌓아 올리면?!\n","\n","따란- GPT가 되는 것이죠.\n","\n","🤔 '이게 다야?'라는 말이 절로 나오죠ㅎㅎㅎ;;;\n","\n","이제 GPT를 두 부분으로 나누어서 좀 더 자세히 보겠습니다.\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/11_HNGuRhf.png'>\n","<br><br>\n","Transfomer Decoder Block : Pretraining LM (Unspervised Learning)\n","\n","첫 번째로 볼 부분은 빨간색 박스 안입니다.  \n","pretrain 하는 부분으로 unsupervised learning(비지도 학습)을 하게 됩니다. \n","  \n","위에 트랜스포머의 구조 그대로죠?\n","\n","1. Embedding\n","\n","GPT는 텍스트의 Embedding으로 BPE(Byte-pair Encoding) 를 사용하고 있어요.\n","\n","BPE는 모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치는 subword tokenization이었죠!  \n","처음 보는 단어일지라도 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 해결할 수 있다는 장점이 있었습니다.\n","  \n","기존 트랜스포머와 마찬가지로 position encoding(포지션 인코딩)도 함께 사용한답니다.\n","\n","2. Masked Multi-Head Attention\n","\n","Masked Multi-Head Attention은 모든 것을 병렬적으로 처리하는 트랜스포머에게 자기회귀적(Autoregressive)인 특성을 부여하기 위해 만든 장치입니다.  \n","여기서 자기회귀적이라 함은 훈련 단계에서 디코더에게 정답 문장을 매 스텝 단위로 단어 하나씩 알려주고 그다음 단어를 예측(Next Token Prediction)하게 하는 형태로 학습되는 형태라는 뜻이죠.  \n","  \n","이는 마치 sequence-to-sequence 모델에서 디코더가 번역 문장을 생성할 때 time-step을 하나하나 거치듯이 만들어주는 것입니다.  \n","순차처리 방식의 RNN과 달리 정답 문장의 모든 단어를 한꺼번에 입력받는 트랜스포머의 decoder는 학습할 때 현재 자기보다 미래에 생성될 토큰을 보지 못하도록 masking이 필요하게 됩니다.  \n","  \n","'어!? 근데 그렇게 생겼다면 어디서 본 것 같은데?!'🤔 라고 생각하신 분 계신가요?\n","  \n","사실 위에서부터 언급했듯이, 이 구조는 저희가 열심히 공부했던 언어 모델과 같은 구조입니다.  \n","다시 한번 되짚어 보자면, 언어 모델은 비지도 학습을 통해 문장의 자연스러운 순서를 학습하게 됩니다.  그래서 GPT는 문자 생성에 매우 특화되어 있답니다.  \n","GPT를 연구한 Open AI도 너무나 자연스러운 문장을 만들어내서 그 악용이 두렵다며 전체 소스 코드를 공개하지 않았을 정도니깐요!  \n","그렇다고 해서 요약, 텍스트 분류 등 다른 task들에서 성능이 떨어지는 것은 아닙니다. \n","  \n","3. Text Prediction & Text classification: finetuning downstream task (Supervised Learning)\n","\n","pretrain이 끝나게 되면 GPT는 downstream task에 맞게 finetuning을 하게 됩니다.  \n","바로 파란색 박스 부분에 해당되는 부분이죠. 여기서 우리는 기존에 봐왔던 모델들과 조금 다른 점을 발견할 수 있습니다.  \n","바로 두 개의 Objective가 존재하는 것이죠.\n","\n","말 그대로 모델이 두 가지의 문제를 동시에 푸는 겁니다.  \n","text prediction과 text classification이 각각 다른 모델들을 이용하여 output을 만들어내는 것이 아니라 한 모델에서 동시에 output을 내는 겁니다.\n","\n","논문의 저자들은 이렇게 실제 풀어야 하는 문제인 주요 task와 동시에 보조적으로 또 다른 문제를 풀 때 (Auxiliary objective) 주요 task에 대한 정확도가 더 올라갔음을 확인했다고 하네요.\n","\n","생각해 보면 LM 또한, auxiliary로 얻어진 결과라고 생각할 수 있습니다.  \n","시퀀스의 다음 나올 단어들을 학습하다 보니 전체적인 언어의 구조를 알게 된 것이지요.\n","\n","그렇다면 GPT의 모델 부분을 코드로 한 번 확인해 볼까요?  \n","전체 코드를 한 번에 보면 복잡해 보일 수도 있겠지만, TFGPT2MainLayer라는 전체 모델 클래스 안에서 TFBlock 레이어 클래스를 반복해서 사용하고 있는 부분을 눈여겨 봐주세요.  \n","TFBlock 클래스 안에서 TFAttention, TFMLP 레이어가 사용되는 구조가 위에서 소개한 GPT의 모델 구조 그림에 표현되어 있습니다."],"metadata":{"id":"8fnCW25FOUX2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FMlxAlTJR1B"},"outputs":[],"source":["class TFAttention(tf.keras.layers.Layer):\n","    def __init__(self, nx, n_ctx, config, scale=False, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n","        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n","        assert n_state % config.n_head == 0\n","        self.n_ctx = n_ctx\n","        self.n_head = config.n_head\n","        self.split_size = n_state\n","        self.scale = scale\n","        self.output_attentions = config.output_attentions\n","\n","        self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n","        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n","        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n","        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n","        self.pruned_heads = set()\n","\n","    def prune_heads(self, heads):\n","        pass\n","\n","    @staticmethod\n","    def causal_attention_mask(nd, ns, dtype):\n","        \"\"\"\n","        1-2) masked attention에서 설명한 masking 부분\n","        \"\"\"\n","        i = tf.range(nd)[:, None]\n","        j = tf.range(ns)\n","        m = i >= j - ns + nd\n","        return tf.cast(m, dtype)\n","\n","    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n","\t\t\t\t\"\"\"\n","\t\t\t\t1-2) attention 계산\n","        q, k, v 의 shape : [batch, heads, sequence, features]\n","\t\t\t\t\"\"\"\n","\n","        w = tf.matmul(q, k, transpose_b=True)\n","        if self.scale:\n","            dk = tf.cast(shape_list(k)[-1], tf.float32)  # scale attention_scores\n","            w = w / tf.math.sqrt(dk)\n","\n","        # w shape : [batch, heads, dst_sequence, src_sequence]\n","        _, _, nd, ns = shape_list(w)\n","        b = self.causal_attention_mask(nd, ns, dtype=w.dtype)\n","        b = tf.reshape(b, [1, 1, nd, ns])\n","        w = w * b - 1e4 * (1 - b)\n","\n","        if attention_mask is not None:\n","            # attention mask 적용\n","            w = w + attention_mask\n","\n","        w = tf.nn.softmax(w, axis=-1)\n","        w = self.attn_dropout(w, training=training)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            w = w * head_mask\n","\n","        outputs = [tf.matmul(w, v)]\n","        if output_attentions:\n","            outputs.append(w)\n","        return outputs\n","\n","    def merge_heads(self, x):\n","        x = tf.transpose(x, [0, 2, 1, 3])\n","        x_shape = shape_list(x)\n","        new_x_shape = x_shape[:-2] + [x_shape[-2] * x_shape[-1]]\n","        return tf.reshape(x, new_x_shape)\n","\n","    def split_heads(self, x):\n","        x_shape = shape_list(x)\n","        new_x_shape = x_shape[:-1] + [self.n_head, x_shape[-1] // self.n_head]\n","        x = tf.reshape(x, new_x_shape)\n","        return tf.transpose(x, (0, 2, 1, 3))  # (batch, head, seq_length, head_features)\n","\n","    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n","        x = self.c_attn(x)\n","        query, key, value = tf.split(x, 3, axis=2)\n","        query = self.split_heads(query)\n","        key = self.split_heads(key)\n","        value = self.split_heads(value)\n","        if layer_past is not None:\n","            past_key, past_value = tf.unstack(layer_past, axis=0)\n","            key = tf.concat([past_key, key], axis=-2)\n","            value = tf.concat([past_value, value], axis=-2)\n","\n","        # keras serialization을 위한 코드\n","        if use_cache:\n","            present = tf.stack([key, value], axis=0)\n","        else:\n","            present = (None,)\n","\n","        attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n","        a = attn_outputs[0]\n","\n","        a = self.merge_heads(a)\n","        a = self.c_proj(a)\n","        a = self.resid_dropout(a, training=training)\n","\n","        outputs = [a, present] + attn_outputs[1:]\n","        return outputs  # a, present, (attentions)\n","\n","\n","class TFMLP(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer의 Decoder Block에서 Feed Foward를 구현해 둔 부분\n","\"\"\"\n","    def __init__(self, n_state, config, **kwargs):\n","        super().__init__(**kwargs)\n","        nx = config.n_embd\n","        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n","        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n","        self.act = get_tf_activation(\"gelu\")\n","        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n","\n","    def call(self, x, training=False):\n","        h = self.act(self.c_fc(x)) # conv1d로 flatten 후 activation 적용\n","        h2 = self.c_proj(h)\n","        h2 = self.dropout(h2, training=training)\n","        return h2\n","\n","\n","class TFBlock(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer의 Decoder Block을 구현해 둔 부분\n","\"\"\"\n","    def __init__(self, n_ctx, config, scale=False, **kwargs):\n","        super().__init__(**kwargs)\n","        nx = config.n_embd\n","        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n","        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n","        self.attn = TFAttention(nx, n_ctx, config, scale, name=\"attn\")\n","        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")\n","        self.mlp = TFMLP(inner_dim, config, name=\"mlp\")\n","\n","    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n","        a = self.ln_1(x)\n","        output_attn = self.attn(\n","            a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training\n","        )\n","        a = output_attn[0]  # output_attn: a, present, (attentions)\n","        x = x + a\n","\n","        m = self.ln_2(x)\n","        m = self.mlp(m, training=training)\n","        x = x + m\n","\n","        outputs = [x] + output_attn[1:]\n","        return outputs  # x, present, (attentions)\n","\n","\n","@keras_serializable\n","class TFGPT2MainLayer(tf.keras.layers.Layer):\n","\"\"\"\n","모델의 전체 구조\n","\"\"\"\n","    config_class = GPT2Config\n","\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(*inputs, **kwargs)\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.use_cache = config.use_cache\n","        self.return_dict = config.use_return_dict\n","\n","        self.num_hidden_layers = config.n_layer\n","        self.vocab_size = config.vocab_size\n","        self.n_embd = config.n_embd\n","\n","        self.wte = TFSharedEmbeddings(\n","            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n","        )\n","        self.wpe = tf.keras.layers.Embedding(\n","            config.n_positions,\n","            config.n_embd,\n","            embeddings_initializer=get_initializer(config.initializer_range),\n","            name=\"wpe\",\n","        )\n","        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n","        self.h = [TFBlock(config.n_ctx, config, scale=True, name=\"h_._{}\".format(i)) for i in range(config.n_layer)]\n","        self.ln_f = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_f\")\n","\n","    def get_input_embeddings(self):\n","        return self.wte\n","\n","    def set_input_embeddings(self, value):\n","        self.wte.weight = value\n","        self.wte.vocab_size = self.wte.weight.shape[0]\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def call(\n","        self,\n","        inputs,\n","        past=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            past = inputs[1] if len(inputs) > 1 else past\n","            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n","            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n","            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n","            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n","            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n","            use_cache = inputs[7] if len(inputs) > 7 else use_cache\n","            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n","            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n","            return_dict = inputs[10] if len(inputs) > 10 else return_dict\n","            assert len(inputs) <= 11, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            past = inputs.get(\"past\", past)\n","            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n","            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n","            position_ids = inputs.get(\"position_ids\", position_ids)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            use_cache = inputs.get(\"use_cache\", use_cache)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 11, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        use_cache = use_cache if use_cache is not None else self.use_cache\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if past is None:\n","            past_length = 0\n","            past = [None] * len(self.h)\n","        else:\n","            past_length = shape_list(past[0][0])[-2]\n","        if position_ids is None:\n","            position_ids = tf.range(past_length, input_shape[-1] + past_length, dtype=tf.int32)[tf.newaxis, :]\n","\n","        if attention_mask is not None:\n","            # 3D attention mask 만들기\n","            # Sizes : [batch_size, 1, 1, to_seq_length]\n","            # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n","\n","            attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","            # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n","            attention_mask = tf.cast(attention_mask, tf.float32)\n","            attention_mask = (1.0 - attention_mask) * -10000.0\n","        else:\n","            attention_mask = None\n","\n","        # head_mask가 1.0이면, head를 유지\n","        # attention_probs : shape bsz x n_heads x N x N\n","        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n","        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.num_hidden_layers\n","            # head_mask = tf.constant([0] * self.num_hidden_layers)\n","\n","        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n","        position_embeds = self.wpe(position_ids)\n","        if token_type_ids is not None:\n","            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n","            token_type_embeds = self.wte(token_type_ids, mode=\"embedding\")\n","        else:\n","            token_type_embeds = 0\n","        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n","        hidden_states = self.drop(hidden_states, training=training)\n","\n","        output_shape = input_shape + [shape_list(hidden_states)[-1]]\n","\n","        presents = () if use_cache else None\n","        all_attentions = () if output_attentions else None\n","        all_hidden_states = () if output_hidden_states else None\n","        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (tf.reshape(hidden_states, output_shape),)\n","\n","            outputs = block(\n","                hidden_states,\n","                layer_past,\n","                attention_mask,\n","                head_mask[i],\n","                use_cache,\n","                output_attentions,\n","                training=training,\n","            )\n","\n","            hidden_states, present = outputs[:2]\n","            if use_cache:\n","                presents = presents + (present,)\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (outputs[2],)\n","\n","        hidden_states = self.ln_f(hidden_states)\n","\n","        hidden_states = tf.reshape(hidden_states, output_shape)\n","        # Add last hidden state\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if output_attentions:\n","            attention_output_shape = input_shape[:-1] + [-1] + shape_list(all_attentions[0])[-2:]\n","            all_attentions = tuple(tf.reshape(t, attention_output_shape) for t in all_attentions)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n","\n","        return TFBaseModelOutputWithPast(\n","            last_hidden_state=hidden_states,\n","            past_key_values=presents,\n","            hidden_states=all_hidden_states,\n","            attentions=all_attentions,\n","        )"]},{"cell_type":"markdown","source":["Input Transformation\n","\n","모델이 한 개인데 어떻게 classification, entailment 등등 다양한 문제를 풀 수 있을까요?  \n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/12_m2lKyFg.max-800x600.png'>\n","<br><br>\n","바로 input을 변형시켜서 입니다.\n","\n","GPT는 언어 모델이기 때문에 pretrain시에는 문장(단어의 시퀀스들)을 그대로 input을 줍니다.   \n","finetuning 시에도 똑같이 단어의 시퀀스들을 주면 되는데요 이때, input을 아주 조금만 변형시켜 주면 됩니다.\n","\n","예를 들어 classification task를 풀기 위해 finetuning을 하게 된다면 \\<start> \\<input text> \\<extract> \\<class> 이렇게 구성된 데이터셋을 학습시키면 되는 것이죠.  \n","GPT는 이 데이터셋에 맞추어서 weight들을 조정하게 될 것입니다.\n","  \n","finetuning이 끝나고 테스트 시에는 \\<start> \\<input text> \\<extract>을 input으로 주면 해당 시퀀스에 뒤이어 나올 토큰 즉 \\<class>를 생성하게 되는 것이죠.\n","\n","\n"],"metadata":{"id":"YOmotXZ0Q3sH"}},{"cell_type":"markdown","source":["GPT vs. GPT2\n","\n","원리는 똑같아요. GPT의 모델 구조를 그대로 사용하면서 파라미터 사이즈를 10배 정도 키우고 성능을 개선시킨 모델이 바로 GPT2입니다.\n","\n","성능 개선을 위해서 쓴 테크닉들은 자세히는 다루지 않겠습니다. 넌지시 언급만 해보자면, 이전보다 더욱 정제된 데이터셋을 더욱 많이 사용해서 학습시킨 것과 토크나이저를 더 강력하게 만든 것 등등이 있습니다.\n","\n","자세한 부분은 논문을 읽어보시는 것을 추천해드립니다!\n","\n","- [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n","- [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n","- [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)  \n","\n","쉬는 시간을 잠시 가져볼까요?\n","\n","- [AllenNLP - Demo](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)\n","\n","학습이 완료된 GPT를 이용해 볼 수 있는 사이트입니다.  \n","원하는 대로 글을 적으면 다음 시퀀스를 만들어주는데요! GPT가 확률을 계산해서 확률이 높은 k 개의 단어를 생성하는 것을 볼 수 있습니다.  \n","생각보다 그럴싸한 말들을 만들어 냅니다."],"metadata":{"id":"_Y7XHfNSR5eg"}},{"cell_type":"markdown","source":["---\n","### GPT Neo\n","---\n","\n","GPT3를 복제한 GPT-Neo라는 모델이 EleutherAI라는 비영리 오픈 소스 연구 단체에서 오픈 소스로 공개되었습니다. 모델 뿐 아니라 대규모 데이터과 사전학습된 모델도 공개되었죠. 최근에는 60억 개의 파라미터를 가진 GPT-J-6B를 공개하기도 하였습니다. GPT3보다 작은 파라미터를 가지고 있지만 GPT3와 유사한 수준의 파라미터 수를 갖는 모델인 GPT-NeoX을 만들고 있다고 하니 기대해 보아도 좋을 것 같습니다.\n","\n","- [GPT-Neo](https://github.com/EleutherAI/gpt-neo)\n","- [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)\n","- [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)"],"metadata":{"id":"yriNCbwOT0KB"}},{"cell_type":"markdown","source":["---\n","# BERT(Bidirectional Encoder Representations from Transformers)\n","----\n"],"metadata":{"id":"JqUY4VfeUANn"}},{"cell_type":"markdown","source":["\n","대망의 BERT입니다.\n","\n","2018년 돌연 혜성처럼 나타나 NLP계를 점령했던 BERT, 성능도 성능이지만 이후로도 많은 연구들에 영감을 주었던 모델입니다.\n","\n","ELMo에 이어 BERT까지 새서미 스트리트라는 캐릭터 이름들을 따는 센스까지 ㅎㅎ\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/14_VGS9u7n.max-800x600.png'>\n","<br><br>\n","먼저 GPT와 비교해 보겠습니다.\n","  \n","트랜스포머의 decoder를 이용하여 만든 모델인 만큼 GPT는 input을 한 방향으로만(uni-direction) 보게 됩니다. 다음 단어를 예측해야만 하는 LM의 특징이었죠.\n","  \n","이와는 다르게 BERT는 양방향(bi-direction)으로 input을 보고 있는 것을 확인할 수 있습니다. (사실 BERT의 B는 Bi-direction을 의미하고 있습니다.)\n","  \n","그림에서는 보이지 않지만 또 다른 큰 차이점이 존재합니다.  \n","BERT는 트랜스포머의 encoder만을 사용한 모델입니다.\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/15_AHA5nKS.png'>\n","<br><br>\n","다음은 ELMo와의 비교입니다.\n","\n","ELMo가 bidirection LSTM을 사용하긴 했지만 독립적인 모델처럼 학습하고 마지막 layer에서만 합쳐준다는 것을 잊지 않으셨죠?!  \n","따라서 ELMo는 가장 위에 layer만 양방향 정보를 가지고 있습니다. 이는 모든 layer들이 양방향으로 보는 BERT와 가장 큰 차이점이라고 할 수 있죠.\n","\n","BERT는 이처럼 진짜 bi-direction이 뭔지 보여주기 위해 만들어진 모델이랍니다. 이제 BERT의 핵심 아이디어를 알았으니 더 자세하게 알아볼까요?\n"],"metadata":{"id":"BbAvtCXKUJHq"}},{"cell_type":"markdown","source":["---\n","### 1. BERT의 구조\n","---\n"],"metadata":{"id":"8pXDn42TUrAH"}},{"cell_type":"markdown","source":["앞서 말했듯이 BERT는 트랜스포머의 encoder 구조를 이용한 모델입니다.  \n","이제부터는 트랜스포머와의 차이점을 위주로 이야기해 보겠습니다.\n","\n","# 1. Transformer Encoder Block\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/16_8aNWN0Q.png'>\n","<br><br>\n","1) Embedding(임베딩)\n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/30.max-800x600.png'>\n","<br><br>\n","BERT는 기존의 트랜스포머와는 유사하지만 다른 임베딩 체계를 가지고 있습니다.  \n","세 가지 임베딩을 가지고 있는데요. 하나하나 살펴보겠습니다.\n","\n","- Token Embedding\n","\n","우선 Wordpiece을 이용하여 3만 개의 vocabulary를 학습합니다.  \n","학습한 Wordpiece model을 이용하여 token들을 임베딩해줍니다.\n","\n","- Segment Embedding\n","\n","BERT는 두 가지 sentence(BERT의 논문에서 나오는 sentence는 문장의 의미보다는 텍스트 덩어리의 의미입니다.)를 입력으로 받기 때문에 이 두 가지 sentence를 구분해야 할 필요가 있습니다.  \n","  \n","segment embedding은 바로 이를 위해서 존재하는데요.  \n","모델 입장에서 주르륵 이어진 텍스트들의 덩어리를 나누어주는 역할을 합니다.  \n","그림에서도 [SEP]토큰으로 구분된 두 sentence에서 앞부분은 A, 뒷부분은 B로 나누어 임베딩하는 것을 확인하실 수 있습니다.\n","\n","- Position Embedding\n","\n","마지막으로 position embedding입니다.  \n","두 sentence를 나누는 것은 segment embedding에서 해주지만, sentence 안에서의 순서는 아직 모르는 상태입니다.  \n","따라서 문장 내에 절대적인 위치(순서)를 알려주기 위해 필요한 것이 position embedding입니다.\n","\n","여기서 잠깐!\n","\n","positional encoding과 position embedding 어떤 말이 맞는 것일까요?  \n","트랜스포머와 GPT에서는 positional encoding이라고 했는데... 똑같은 것 아닐까요?\n","\n","정답은 NO!\n","\n","혼용하여 사용하긴 하지만 엄밀하게 말하면 다른 개념입니다.  \n","### encoding은 one-hot-encoding(원핫 인코딩)처럼 미리 정해진 값을 주는 것이지만  \n","### embedding은 그 값이 정해진 것이 아니라 학습을 통해 습득하는 것을 의미합니다.\n","\n","다시 말해서, BERT는 학습을 통해 position 정보를 습득한다고 생각하시면 됩니다.\n","\n","이렇게 얻은 세 가지 임베딩을 모두 합산해 주면 BERT의 임베딩이 완성됩니다.  \n","이후 layer normalization과 dropout까지 해주면 트랜스포머 첫 블록의 입력이 완성됩니다.\n","  \n","[SEP]토큰은 위에서 아주 넌-지시 말하고 넘어갔죠. 이것들은 special token이라고 해서 실제 단어에는 쓰이지 않지만 특별한 역할들을 가지고 있는 아이들이에요.  \n","sequence-to-sequence 모델에서도 슬-쩍 본 적이 있으실 거예요! 문장의 시작과 끝을 알려주기 위해 쓰였던 \\<BOS>,\\<EOS> 같은 토큰들 말이에요!\n","\n","그럼 BERT에는 어떤 토큰들이 어떻게 쓰였나 알아보고 가실까요?\n","  \n","- [CLS] : sentence의 시작을 알리는 토큰\n","- [SEP] : sentence의 종결을 알리는 토큰, sentence를 구분하는 데에 사용하기도 함\n","- [MASK]: 마스크 토큰\n","- [PAD] : 배치 데이터의 길이를 맞춰주기 위한 토큰\n","\n","# 2. Activation Function(활성화 함수) : GELU\n","\n","Feedforward Networks에서 BERT는 ReLU대신 GELU를 사용합니다.  \n","음수 값은 0이 되어버리는 ReLU와는 달리 GELU는 음수에서도 완만한 곡선을 그리며 미분을 가능하게 합니다.  \n","GELU를 사용하면 성능이 더욱 좋아지기 때문에 BERT의 저자들은 GELU를 사용했다고 합니다."],"metadata":{"id":"Qy4WnuKvUxnC"}},{"cell_type":"markdown","source":["---\n","### 2. BERT의 학습\n","---"],"metadata":{"id":"bbDv1EA1V7MY"}},{"cell_type":"markdown","source":["BERT는 양방향(Bi-direction)을 강조한 모델입니다. BERT는 그럼 어떻게 학습을 하는 것일까요? 두 가지 핵심 아이디어를 들여다보죠.\n","\n","# 1) Masked LM(MLM)\n","\n","다음 단어를 예측해야만 하는 일반적인 LM은 그 task의 특성상 한 방향(uni-direction)일 수밖에 없습니다.  \n","이와 달리, BERT는 마스크 된 토큰([MASK)만 맞추면 되는 masked LM(MLM)을 제안했습니다.  \n","즉, input sequence의 순서에 상관없이 전체 문장을 모두 볼 수 있게 되는 거죠.\n","  \n","(word2vec의 CBOW와 비슷한 아이디어처럼 보이죠?)\n","  \n","MLM을 위해서 BERT는 학습 데이터의 전체에서 15% 를 [MASK] 토큰으로 랜덤하게 바꿉니다.  \n","여기서 재미있게도 15%에 해당하는 모든 토큰들을 마스크하는 것이 아니라 80%는 [MASK]토큰, 10%는 무작위로 랜덤한 토큰으로 바꿔줍니다.  \n","나머지 10%는 원래의 토큰을 그대로 사용하구요.\n","  \n","그 이유는 바로 finetuning에 있는데요.  \n","pretrain을 끝낸 모델을 finetuning할 때에는 input에 [MASK]토큰이 등장하지 않기 때문입니다.  \n","아무래도 finetuning시에 [MASK]토큰이 보이지 않는다면, 당연히 성능에 영향을 미치게 될 것이라고 생각한 거죠.  \n","따라서, [MASK]토큰이 아닌 것들도 예측을 하도록 학습하여 문장 자체에 대한 전반적인 이해(문맥에 대한 이해)를 할 수 있도록 해주는 겁니다.\n","  \n","다시 정리하자면, 전체 학습 데이터의 토큰들 중 12%(15% 중에서 80%)는 [MASK]토큰으로,  \n","1.5%(15% 중에서 10%)는 무작위로 랜덤한 토큰으로 대체하고,  \n","1.5%((15% 중에서 10%)는 변경하지 않고 원래의 토큰을 사용하는 겁니다.\n","\n","# 2) Next Sentence Prediction (NSP)\n","\n","BERT는 마스크 된 토큰을 맞추는 것과 동시에 또 다른 task를 함께 학습합니다.  \n","바로 Next Sentence Prediction (NSP), 다음 문장인지 확인하기 입니다.\n","\n","예시를 한 번 들어볼까요?\n","\n",">여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나  \n","그 사람이 바다의 뚜껑 닫지 않고 돌아가  \n","그때부터 바다의 뚜껑 열린 채 그대로 있네  \n","-하라 마스미 「바다의 뚜껑」 중\n","\n","제가 좋아하는 시인데요. 한 행을 하나의 sentence라고 가정해 봅시다.\n","  \n","그렇다면 '여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나' 다음의 sentence는 '그 사람이 바다의 뚜껑 닫지 않고 돌아가'가 될 것입니다.\n","\n","BERT는 이처럼 두 sentence가 연속해서 오는지의 여부를 학습하게 됩니다.  \n","바로 아래와 같이 말이죠.\n","\n","    [CLS]여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 그 사람이 바다의 뚜껑 닫지 않고 돌아가[SEP] → TRUE(IsNext)\n","\n","    [CLS]여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 한강에서 자전거 타며 아이스 아메리카노를 마시고 싶다[SEP] → FALSE(NotNext)\n","\n","NSP를 위해서 BERT의 학습 데이터는 1건당 두 개의 문장으로 구성합니다.  \n","50%의 확률로 TRUE와 FALSE를 부여하도록 만들죠.\n","\n","또한, task가 너무 쉬워지는 것을 방지하기 위해 max_num_tokens라는 것을 정의합니다.  \n","데이터의 90%는 max_num_tokens가 max_sequence_length가 같도록 만들고,  \n","나머지 10%의 데이터는 max_num_tokens가 max_sequence_length보다 짧게 되도록 랜덤으로 정합니다.\n","\n","이후, 두 개의 sentence의 단어 총수가 max_num_tokens보다 작아질 때까지 두 sentence 중 단어 수가 많은 쪽의 문장 맨 앞 또는 맨 뒤 단어를 하나씩 제거합니다.  \n","이때 문장 맨 앞의 단어를 선택할지 맨 뒤의 단어의 선택할지는 50%의 확률로 정합니다.\n","\n","이렇게 NSP를 학습하게 되면, 문장과 문장 사이의 관계를 학습할 수 있게 됩니다.  \n","문장의 길이를 임의적으로 조정하면서, 짧은 문장에 대해서도 성능이 크게 떨어지지 않게 되며,  \n","문장의 단어들을 랜덤하게 삭제하는 과정에서 문장에서 일부 단어들이 없어져도 그 영향을 크게 받지 않게 됩니다.  \n","\n","지금까지 보신 MLM과 NSP는 따로 학습되는 것이 아니라 동시에 이뤄집니다.  \n","따라서 실제 BERT의 학습 데이터셋은 아래와 같은 구조일 것입니다.(편의상 토큰을 띄어쓰기 단위로 나누겠습니다)\n","\n","    [CLS]여름의 마지막 [MASK] 누가 제일 늦게 [MASK] 나왔나 [SEP] 그 사람이 바다의 [MASK] 닫지 않고 돌아가[SEP] → Label : TRUE(IsNext)\n","\n","    [CLS]여름의 [MASK] 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 한강에서 [MASK] 아이스 아메리카노를 마시고 싶다[SEP] → Label : FALSE(NotNext)\n","\n"],"metadata":{"id":"DOE9SvL-WSKN"}},{"cell_type":"markdown","source":["---\n","### 3. Fine-tuning Task\n","---"],"metadata":{"id":"F7ZWR4veXalz"}},{"cell_type":"markdown","source":["BERT의 finetuning은 그럼 어떻게 진행할까요?\n","\n","BERT 또한 한 모델이 다양한 task들을 수행하기 때문에 input transformation을 이용합니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/18_Qdk6eaL.max-800x600.png'>\n","<br>\n","\n","classification 같은 경우는 [CLS]토큰을, QA와 같이 문장이나 단어들이 나와야 하는 경우에는 토큰들의 벡터를 output layer에 넣어 output을 산출해냅니다."],"metadata":{"id":"w8vzdmTiX7Zi"}},{"cell_type":"markdown","source":["---\n","### 4. BERT의 모델 코드\n","----\n","\n","Transformer, GPT와 다른 부분을 유심히 보면서 모델을 훑어본다면 더욱 도움이 될 거예요! \n","메인 모델 구조인 TFBertEncoder 안에 반복적으로 사용되고 있는 TFBertLayer 레이어 구성을 자세히 살펴주세요.  \n","전체 구조를 한눈에 보기에는 복잡하게 느껴지시더라도, 다음 노드에서 단계별로 차례차례 구현해 보면서 좀 더 명확하게 이해하실 수 있으실 겁니다."],"metadata":{"id":"Wd5U-kCSYMP6"}},{"cell_type":"code","source":["class TFBertPreTrainingLoss:\n","    \"\"\"\n","    BERT의 경우 Pretraining으로 NSP + MLM 두 가지를 함께 학습하게 됨. 그것을 위한 loss \n","\t\t-100으로 label(logit)이 되어있는 경우 loss 계산 시 제외\n","    \"\"\"\n","\n","    def compute_loss(self, labels, logits):\n","        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n","        )\n","\n","        masked_lm_active_loss = tf.not_equal(tf.reshape(labels[\"labels\"], (-1,)), -100)\n","        masked_lm_reduced_logits = tf.boolean_mask(\n","            tf.reshape(logits[0], (-1, shape_list(logits[0])[2])),\n","            masked_lm_active_loss,\n","        )\n","        masked_lm_labels = tf.boolean_mask(tf.reshape(labels[\"labels\"], (-1,)), masked_lm_active_loss)\n","        next_sentence_active_loss = tf.not_equal(tf.reshape(labels[\"next_sentence_label\"], (-1,)), -100)\n","        next_sentence_reduced_logits = tf.boolean_mask(tf.reshape(logits[1], (-1, 2)), next_sentence_active_loss)\n","        next_sentence_label = tf.boolean_mask(\n","            tf.reshape(labels[\"next_sentence_label\"], (-1,)), mask=next_sentence_active_loss\n","        )\n","        masked_lm_loss = loss_fn(masked_lm_labels, masked_lm_reduced_logits)\n","        next_sentence_loss = loss_fn(next_sentence_label, next_sentence_reduced_logits)\n","        masked_lm_loss = tf.reshape(masked_lm_loss, (-1, shape_list(next_sentence_loss)[0]))\n","        masked_lm_loss = tf.reduce_mean(masked_lm_loss, 0)\n","\n","        return masked_lm_loss + next_sentence_loss\n","\n","\n","class TFBertEmbeddings(tf.keras.layers.Layer):\n","    \"\"\"\n","\t\t1-1)에 해당하는 부분으로 3가지 embedding을 만들고 그 embedding을 모두 합산하여 layer normalize와 dropout을 적용\n","\t\t\"\"\"\n","\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = config.vocab_size\n","        self.hidden_size = config.hidden_size\n","        self.initializer_range = config.initializer_range\n","        self.position_embeddings = tf.keras.layers.Embedding(\n","            config.max_position_embeddings,\n","            config.hidden_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"position_embeddings\",\n","        )\n","        self.token_type_embeddings = tf.keras.layers.Embedding(\n","            config.type_vocab_size,\n","            config.hidden_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"token_type_embeddings\",\n","        )\n","\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def build(self, input_shape):\n","        \"\"\"shared word embedding layer \"\"\"\n","        with tf.name_scope(\"word_embeddings\"):\n","            # Create and initialize weights. The random normal initializer was chosen\n","            # arbitrarily, and works well.\n","            self.word_embeddings = self.add_weight(\n","                \"weight\",\n","                shape=[self.vocab_size, self.hidden_size],\n","                initializer=get_initializer(self.initializer_range),\n","            )\n","\n","        super().build(input_shape)\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        position_ids=None,\n","        token_type_ids=None,\n","        inputs_embeds=None,\n","        mode=\"embedding\",\n","        training=False,\n","    ):\n","        \"\"\"\n","        input의 token embeddings\n","        Args:\n","            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n","            mode: \"embedding\" | \"linear\"\n","        Returns:\n","            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n","\t\t\t\t\t\t\t\t\t\t mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n","        Raises:\n","            ValueError: if mode is not valid.\n","\t\t\t\t\"\"\"\n","\n","        if mode == \"embedding\":\n","            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","        elif mode == \"linear\":\n","            return self._linear(input_ids)\n","        else:\n","            raise ValueError(\"mode {} is not valid.\".format(mode))\n","\n","    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n","        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n","        assert not (input_ids is None and inputs_embeds is None)\n","\n","        if input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        else:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","\n","        seq_length = input_shape[1]\n","\n","        if position_ids is None:\n","            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n","\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n","\n","        position_embeddings = tf.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)\n","        token_type_embeddings = tf.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)\n","        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings, training=training)\n","\n","        return embeddings\n","\n","    def _linear(self, inputs):\n","        \"\"\"\n"," \t\t\t  linear layer를 통해서 input의 logit을 계산\n","        Args:\n","            inputs: float32 tensor (shape [batch_size, length, hidden_size])\n","        Returns:\n","            float32 tensor (shape [batch_size, length, vocab_size])\n","        \"\"\"\n","        batch_size = shape_list(inputs)[0]\n","        length = shape_list(inputs)[1]\n","        x = tf.reshape(inputs, [-1, self.hidden_size])\n","        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n","\n","        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n","\n","\n","class TFBertSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n","            )\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        assert config.hidden_size % config.num_attention_heads == 0\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","        self.query = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n","        )\n","        self.key = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n","        )\n","        self.value = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n","        batch_size = shape_list(hidden_states)[0]\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n","        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n","        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n","\n","        # \"query\"와 \"key\"의 dot product : raw attention scores\n","        attention_scores = tf.matmul(\n","            query_layer, key_layer, transpose_b=True\n","        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n","        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n","        attention_scores = attention_scores / tf.math.sqrt(dk)\n","\n","        if attention_mask is not None:\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n","\n","        attention_probs = self.dropout(attention_probs, training=training)\n","\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = tf.matmul(attention_probs, value_layer)\n","        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n","        context_layer = tf.reshape(\n","            context_layer, (batch_size, -1, self.all_head_size)\n","        )  # (batch_size, seq_len_q, all_head_size)\n","        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        return outputs\n","\n","\n","class TFBertSelfOutput(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, input_tensor, training=False):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class TFBertAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.self_attention = TFBertSelfAttention(config, name=\"self\")\n","        self.dense_output = TFBertSelfOutput(config, name=\"output\")\n","\n","    def prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n","        self_outputs = self.self_attention(\n","            input_tensor, attention_mask, head_mask, output_attentions, training=training\n","        )\n","        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\n","        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n","\n","        return outputs\n","\n","\n","class TFBertIntermediate(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Block에서의 feedforward\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","\n","        if isinstance(config.hidden_act, str):\n","            self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TFBertOutput(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, input_tensor, training=False):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class TFBertLayer(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Encoder Block과 동일한 구조 : Attention,Feedforward,dropout,layer nomalization\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.attention = TFBertAttention(config, name=\"attention\")\n","        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\n","        self.bert_output = TFBertOutput(config, name=\"output\")\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n","        attention_outputs = self.attention(\n","            hidden_states, attention_mask, head_mask, output_attentions, training=training\n","        )\n","        attention_output = attention_outputs[0]\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\n","        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n","\n","        return outputs\n","\n","\n","class TFBertEncoder(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Encoder Block(코드 상에서 TFBertLayer)를 n_layer만큼 여러개 쌓은 구조\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.layer = [TFBertLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask,\n","        head_mask,\n","        output_attentions,\n","        output_hidden_states,\n","        return_dict,\n","        training=False,\n","    ):\n","        all_hidden_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","\n","        for i, layer_module in enumerate(self.layer):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_outputs = layer_module(\n","                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\n","            )\n","            hidden_states = layer_outputs[0]\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[1],)\n","\n","        # Add last layer\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n","\n","        return TFBaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n","        )\n","\n","\n","class TFBertPooler(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size,\n","            kernel_initializer=get_initializer(config.initializer_range),\n","            activation=\"tanh\",\n","            name=\"dense\",\n","        )\n","\n","    def call(self, hidden_states):\n","        # 첫 번째 토큰의 hidden state를 얻기 위해 pool\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","\n","        return pooled_output\n","\n","\n","class TFBertPredictionHeadTransform(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","\n","        if isinstance(config.hidden_act, str):\n","            self.transform_act_fn = get_tf_activation(config.hidden_act)\n","        else:\n","            self.transform_act_fn = config.hidden_act\n","\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.transform_act_fn(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TFBertLMPredictionHead(tf.keras.layers.Layer):\n","    def __init__(self, config, input_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = config.vocab_size\n","        self.transform = TFBertPredictionHeadTransform(config, name=\"transform\")\n","\n","        # input embeddings과 동일한 weight를 가지고 있지만 각각의 token에 대하여 output만 바이어스를 가지고 있음\n","        self.input_embeddings = input_embeddings\n","\n","    def build(self, input_shape):\n","        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n","\n","        super().build(input_shape)\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.transform(hidden_states)\n","        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n","        hidden_states = hidden_states + self.bias\n","\n","        return hidden_states\n","\n","\n","class TFBertMLMHead(tf.keras.layers.Layer):\n","\"\"\"\n","2-1)Masked LM을 위한 class\n","\"\"\"\n","    def __init__(self, config, input_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.predictions = TFBertLMPredictionHead(config, input_embeddings, name=\"predictions\")\n","\n","    def call(self, sequence_output):\n","        prediction_scores = self.predictions(sequence_output)\n","\n","        return prediction_scores\n","\n","\n","class TFBertNSPHead(tf.keras.layers.Layer):\n","\"\"\"\n","2-2)NSP를 위한 class\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.seq_relationship = tf.keras.layers.Dense(\n","            2, kernel_initializer=get_initializer(config.initializer_range), name=\"seq_relationship\"\n","        )\n","\n","    def call(self, pooled_output):\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","\n","        return seq_relationship_score\n","\n","\n","@keras_serializable\n","class TFBertMainLayer(tf.keras.layers.Layer):\n","\"\"\"\n","모델의 전체 구조\n","\"\"\"\n","    config_class = BertConfig\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.num_hidden_layers = config.num_hidden_layers\n","        self.initializer_range = config.initializer_range\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.return_dict = config.use_return_dict\n","        self.embeddings = TFBertEmbeddings(config, name=\"embeddings\")\n","        self.encoder = TFBertEncoder(config, name=\"encoder\")\n","        self.pooler = TFBertPooler(config, name=\"pooler\")\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","        self.embeddings.vocab_size = value.shape[0]\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def call(\n","        self,\n","        inputs,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n","            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n","            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n","            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n","            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n","            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n","            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n","            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n","            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n","            position_ids = inputs.get(\"position_ids\", position_ids)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if attention_mask is None:\n","            attention_mask = tf.fill(input_shape, 1)\n","\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","\n","\t\t\t\t# 3D attention mask 만들기\n","        # Sizes : [batch_size, 1, 1, to_seq_length]\n","        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n","\n","        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","\t\t\t\t# attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n","        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # head_mask가 1.0이면, head를 유지\n","        # attention_probs : shape bsz x n_heads x N x N\n","        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n","        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.num_hidden_layers\n","            # head_mask = tf.constant([0] * self.num_hidden_layers)\n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            extended_attention_mask,\n","            head_mask,\n","            output_attentions,\n","            output_hidden_states,\n","            return_dict,\n","            training=training,\n","        )\n","\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        if not return_dict:\n","            return (\n","                sequence_output,\n","                pooled_output,\n","            ) + encoder_outputs[1:]\n","\n","        return TFBaseModelOutputWithPooling(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )"],"metadata":{"id":"Hpwl8Z1kRGLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Transformer-XL(Transformer Extra Long)\n","---\n","\n","transformer-XL은 기존의 언어 모델과 트랜스포머가 가지고 있던 한계점을 개선한 모델입니다.  '트랜스포머 이전의 LM에서도 늘 한계점으로 꼽히던 context를 반영하기가 트랜스포머에서도 문제로 떠오릅니다.  \n","비교적 짧은 문장에서의 context는 잘 학습했는데, sequence가 길어질수록 그 상관관계(long-term dependency)가 점점 떨어진다는 것이 문제였죠.  \n","주제에 대해서 잘 말하다가 갑자기 다른 이야기를 한다던가 하는 문제 말이에요.\n","  \n","transformer-XL은 직관적인 그 이름에서도 드러나듯이 좀 더 긴 context를 어떻게 담을 것인가에 대해 고민한 모델입니다. \n","\n","어떻게 해서 transformer-XL은 context를 이어나갈까요?"],"metadata":{"id":"pgjUKptdb4aH"}},{"cell_type":"markdown","source":["---\n","### transformer-XL의 구조\n","---"],"metadata":{"id":"mncGI95pcGmL"}},{"cell_type":"markdown","source":["transformer-XL을 알아보기에 앞서 트랜스포머를 좀 더 자세히 보죠.\n","\n","이번에 조금 다른 측면에서 접근해 보겠습니다. context를 중점적으로 볼 거예요.\n","\n","# 1. Vanilla Transformer LMs\n","\n","트랜스포머는 max_seq_length가 정해져 있습니다.  \n","즉, 모델이 감당할 수 있을 만큼 텍스트를 잘라서 학습하고, 학습한 이후부터 다시 일정 길이만큼 잘라서 학습을 하게 되죠.  \n","이때, 이전 segment에서 학습했던 context는 무시되고, 지금 학습을 하고 있는 segment 안에서만 context를 고려하게 됩니다.  \n","다시 말해서 segment1과 segment2는 전혀 공유하는 context가 없이 단절되었다(context fragmentation)는 말입니다.  \n","분명 사람이 볼 때는 이어지는 흐름인데 말이죠.  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/19_RTNDWar.png'>\n","<br>\n","\n","테스트 시 또 다른 문제가 등장하게 되죠. 바로 슬라이딩을 하면서 생기는 문제입니다.  \n","모델은 일정 길이의 context를 보고 한 단어를 예측합니다.  \n","그 다음에 딱 한 개만큼만 슬라이딩하여 새로운 context를 만들고 다시 연산하여 하나의 단어를 예측합니다.\n","\n","이렇게 하면 이전 context를 조금씩이나마 유지할 수 있을지 모르지만 연산에 드는 비용이 엄청나겠죠?  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/20_87zCAda.max-800x600.png'>\n","<br>\n","\n","# 2. Segment-level recurrence with state reuse\n","\n","이러한 문제들을 해결하기 위해 저자들은 recurrence 메커니즘을 도입합니다.  \n","학습 시에 이전 segment에서 계산했었던 hidden state를 사용하는 것이죠.  \n","이를 통해 context fragmentation을 해결하고 long-term dependency를 유지할 수 있게 됩니다.\n","\n","RNN의 원리를 떠올리면 쉽게 와닿으실 겁니다.  \n","그러나 RNN과는 달리, transformer-XL은 다음 layer에서 이전 layer의 hidden state를 재사용하게 됩니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/21_35Nt3rH.max-800x600.png'>\n","<br>\n","\n","단, 이때 이전 segment들의 정보를 가진 hidden state들의 gradient는 더 이상 변하지 않도록 고정을 시킵니다.(메모리에 있던 값을 불러온다고 하여 cache라고 부르기도 합니다)\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/22_vr7wEs2.png'>\n","<br>  \n","\n","트랜스포머 구조의 LM이 테스트 시 가지고 있던 문제도 recurrence 메커니즘을 이용하면 쉽게 해결할 수 있습니다.  \n","이미 계산한 hidden state의 정보를 메모리에 가지고 있다가 cache를 하기 때문에 계속해서 반복하여 똑같은 연산을 할 필요가 없어집니다.  \n","이 덕분에 속도도 더욱 빨라질 수 있습니다.\n","\n","# 3. Relative Positional Encodings\n","\n","트랜스포머에 segment-level의 recurrence 메커니즘을 적용하면 한 가지 문제가 발생합니다. 바로 포지션 정보를 어떻게 추가할 것인가에 대한 문제이죠.\n","\n","기존 트랜스포머의 포지션 인코딩은 각 segment 내에서의 절대적인 위치 정보를 인코딩하게 됩니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/23_hKefK2A.png'>\n","<br>\n","\n","그러나 recurrence 메커니즘을 사용하게 되면 segment들 사이에서의 상대적인 위치 정보가 필요합니다.\n","\n","이를 위해 저자들은 상대적인 포지션 인코딩(Relative Positional Encodings) 방법을 제안합니다.\n","\n","수식 때문에 조금 복잡해 보일 수도 있지만, 기억하실 것은 상대적인 정보를 임베딩 레이어가 아닌 attention 연산 시에 주입한다는 것입니다.\n","\n","## 1) 트랜스포머의 attention 연산\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/kaebceo.PNG'>\n","<br>\n","\n","트랜스포머의 attention 연산을 수식으로 나타내면 위와 같습니다. 여기서 E는 토큰 임베딩, U는 포지션 인코딩을 의미하죠.  \n","attention 연산에 Q와 K를 대입하면 아래와 같은 식이 나타납니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/24_yv8pDKf.png'>\n","<br>\n","\n","U에 인코딩된 i,j번째 포지션의 정보를 가지고 연산을 하는 모양새입니다.\n","\n","그러나 위에서 언급했듯이 이제는 상대적인 위치 정보가 필요합니다.  \n","i=1,j=2이든 i=7,j=8이든 상관없이 j가 i보다 1뒤에 있다만 알면 되는 것이죠.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/25_0HjxcB3.png'>\n","<br>\n","  \n","이를 위해 U 대신 R을 사용합니다. R은 상대적인 포지션 정보를 encoding한 매트릭스입니다. \n","R의 i번째 행은 i와 다른 단어들 간의 상대적인 포지션을 알려주죠.  \n","따라서, 아래 첨자 i−j에서 보이듯이, i와 j의 상대적인 거리를 이용하고 있습니다.  \n","여기서 R은 기존의 트랜스포머와 마찬가지로 학습되는 것이 아닌 sinusoid encoding matrix를 사용합니다.\n","\n","또한, 학습 가능한 파라미터로 벡터 u와 v가 추가되었습니다.  \n","기존 트랜스포머는 query의 위치에 따라 그 query vector가 영향을 받았습니다.  \n","그러나 절대적인 포지션이 아닌 상대적인 포지션 정보를 이용하게 되면서 query는 그 위치에 상관없이 똑같은 query vector를 사용하게 됩니다.  \n","  \n","따라서 포지션에 상관없이 같은 값인 벡터 u와 v로 대신하게 되었습니다.\n","\n","마지막으로, content-based key vectors와 location-based key vectors를 독립적으로 만들기 위해 $W_{k,E} 와 W_{k,R}$ 를 분리했습니다.\n","  \n","이렇게 recurrence 메커니즘과 relative position encoding을 통해 transformer-XL은 auxiliary losses 없이도 뛰어난 성능을 낼 수 있었다고 말합니다.\n","\n","# 4. 그렇다면 Transformer-XL의 모델 부분을 코드로 한 번 확인해 볼까요?\n","\n","위에서 읽었던 내용들을 상기시키면서 코드를 확인해 봅시다!  \n","TFTransfoXLMainLayer이라는 메인 클래스 안에서 Transformer-XL의 주요 특징인 State Reuse를 위한 메모리 관리가 구현된 _update_mems() 메소드,  \n","그리고 TFRelPartialLearnableMultiHeadAttn 레이어에 구현된 Relative attention이 TFRelPartialLearnableDecoderLayer 안에서 어떻게 recurrent하게 사용되는지를 눈여겨 봐주세요."],"metadata":{"id":"9fkBvX5IcPbS"}},{"cell_type":"code","source":["class TFPositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, demb, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))\n","\n","    def call(self, pos_seq, bsz=None):\n","        sinusoid_inp = tf.einsum(\"i,j->ij\", pos_seq, self.inv_freq)\n","        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n","\n","        if bsz is not None:\n","            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n","        else:\n","            return pos_emb[:, None, :]\n","\n","\n","class TFPositionwiseFF(tf.keras.layers.Layer):\n","    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.d_model = d_model\n","        self.d_inner = d_inner\n","        self.dropout = dropout\n","\n","        self.layer_1 = tf.keras.layers.Dense(\n","            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n","        )\n","        self.drop_1 = tf.keras.layers.Dropout(dropout)\n","        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n","        self.drop_2 = tf.keras.layers.Dropout(dropout)\n","\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n","\n","        self.pre_lnorm = pre_lnorm\n","\n","    def call(self, inp, training=False):\n","        if self.pre_lnorm:\n","            # layer normalization + positionwise feed-forward\n","            core_out = self.layer_norm(inp)\n","            core_out = self.layer_1(core_out)\n","            core_out = self.drop_1(core_out, training=training)\n","            core_out = self.layer_2(core_out)\n","            core_out = self.drop_2(core_out, training=training)\n","\n","            # residual connection\n","            output = core_out + inp\n","        else:\n","            # positionwise feed-forward\n","            core_out = self.layer_1(inp)\n","            core_out = self.drop_1(core_out, training=training)\n","            core_out = self.layer_2(core_out)\n","            core_out = self.drop_2(core_out, training=training)\n","\n","            # residual connection + layer normalization\n","            output = self.layer_norm(inp + core_out)\n","\n","        return output\n","\n","\n","class TFRelPartialLearnableMultiHeadAttn(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        n_head,\n","        d_model,\n","        d_head,\n","        dropout,\n","        dropatt=0.0,\n","        pre_lnorm=False,\n","        r_r_bias=None,\n","        r_w_bias=None,\n","        layer_norm_epsilon=1e-5,\n","        init_std=0.02,\n","        output_attentions=False,\n","        **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.n_head = n_head\n","        self.d_model = d_model\n","        self.d_head = d_head\n","        self.dropout = dropout\n","        self.output_attentions = output_attentions\n","\n","        self.qkv_net = tf.keras.layers.Dense(\n","            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n","        )\n","\n","        self.drop = tf.keras.layers.Dropout(dropout)\n","        self.dropatt = tf.keras.layers.Dropout(dropatt)\n","        self.o_net = tf.keras.layers.Dense(\n","            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n","        )\n","\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n","\n","        self.scale = 1 / (d_head ** 0.5)\n","\n","        self.pre_lnorm = pre_lnorm\n","\n","        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n","            self.r_r_bias = r_r_bias\n","            self.r_w_bias = r_w_bias\n","        else:\n","            self.r_r_bias = None\n","            self.r_w_bias = None\n","\n","        self.r_net = tf.keras.layers.Dense(\n","            self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"r_net\"\n","        )\n","\n","    def build(self, input_shape):\n","        if self.r_r_bias is None or self.r_w_bias is None:  # Biases are not shared\n","            self.r_r_bias = self.add_weight(\n","                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n","            )\n","            self.r_w_bias = self.add_weight(\n","                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n","            )\n","        super().build(input_shape)\n","\n","    def _rel_shift(self, x):\n","\t\t\"\"\"\n","\t\trelative attention을 수행하기 위한 masking\n","\t\t\"\"\"\n","\n","        x_size = shape_list(x)\n","\n","        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n","        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n","        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n","        x = tf.reshape(x, x_size)\n","\n","        return x\n","\n","    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n","        \"\"\"\n","\t\t\t\tw는 embedding, r은 postional embedding\n","\t\t\t\t\"\"\"\n","\n","\t\t\t\tqlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n","\n","        if mems is not None:\n","            cat = tf.concat([mems, w], 0)\n","            if self.pre_lnorm:\n","                w_heads = self.qkv_net(self.layer_norm(cat))\n","            else:\n","                w_heads = self.qkv_net(cat)\n","            r_head_k = self.r_net(r)\n","\n","            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n","            w_head_q = w_head_q[-qlen:]\n","        else:\n","            if self.pre_lnorm:\n","                w_heads = self.qkv_net(self.layer_norm(w))\n","            else:\n","                w_heads = self.qkv_net(w)\n","            r_head_k = self.r_net(r)\n","\n","            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n","\n","        klen = shape_list(w_head_k)[0]\n","\n","        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n","        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n","        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n","\n","        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n","\n","        # compute attention score\n","        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n","        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n","\n","        rr_head_q = w_head_q + self.r_r_bias\n","        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n","        BD = self._rel_shift(BD)\n","\n","        # [qlen x klen x bsz x n_head]\n","        attn_score = AC + BD\n","        attn_score = attn_score * self.scale\n","\n","        # compute attention probability\n","        if attn_mask is not None:\n","            attn_mask_t = attn_mask[:, :, None, None]\n","            attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n","\n","        # [qlen x klen x bsz x n_head]\n","        attn_prob = tf.nn.softmax(attn_score, axis=1)\n","        attn_prob = self.dropatt(attn_prob, training=training)\n","\n","        if head_mask is not None:\n","            attn_prob = attn_prob * head_mask\n","\n","        # compute attention vector\n","        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)\n","\n","        # [qlen x bsz x n_head x d_head]\n","        attn_vec_sizes = shape_list(attn_vec)\n","        attn_vec = tf.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))\n","\n","        # linear projection\n","        attn_out = self.o_net(attn_vec)\n","        attn_out = self.drop(attn_out, training=training)\n","\n","        if self.pre_lnorm:\n","            # residual connection\n","            outputs = [w + attn_out]\n","        else:\n","            # residual connection + layer normalization\n","            outputs = [self.layer_norm(w + attn_out)]\n","\n","        if output_attentions:\n","            outputs.append(attn_prob)\n","\n","        return outputs\n","\n","\n","class TFRelPartialLearnableDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        n_head,\n","        d_model,\n","        d_head,\n","        d_inner,\n","        dropout,\n","        dropatt=0.0,\n","        pre_lnorm=False,\n","        r_w_bias=None,\n","        r_r_bias=None,\n","        layer_norm_epsilon=1e-5,\n","        init_std=0.02,\n","        output_attentions=False,\n","        **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.dec_attn = TFRelPartialLearnableMultiHeadAttn(\n","            n_head,\n","            d_model,\n","            d_head,\n","            dropout,\n","            dropatt=dropatt,\n","            pre_lnorm=pre_lnorm,\n","            r_w_bias=r_w_bias,\n","            r_r_bias=r_r_bias,\n","            init_std=init_std,\n","            layer_norm_epsilon=layer_norm_epsilon,\n","            output_attentions=output_attentions,\n","            name=\"dec_attn\",\n","        )\n","        self.pos_ff = TFPositionwiseFF(\n","            d_model,\n","            d_inner,\n","            dropout,\n","            pre_lnorm=pre_lnorm,\n","            init_std=init_std,\n","            layer_norm_epsilon=layer_norm_epsilon,\n","            name=\"pos_ff\",\n","        )\n","\n","    def call(self, dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=False):\n","        attn_outputs = self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)\n","        ff_output = self.pos_ff(attn_outputs[0], training=training)\n","\n","        outputs = [ff_output] + attn_outputs[1:]\n","\n","        return outputs\n","\n","\n","class TFAdaptiveEmbedding(tf.keras.layers.Layer):\n","\"\"\"\n","n개의 token을 한 번에 임베딩하는 것이 아니라 먼저 cutoff를 통해 n개의 tokens을 나누고 mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx) 을 만족하는 토큰들만을 임베딩하는 방식\n","\"\"\"\n","\n","    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, init_std=0.02, sample_softmax=False, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.n_token = n_token\n","        self.d_embed = d_embed\n","        self.init_std = init_std\n","\n","        self.cutoffs = cutoffs + [n_token]\n","        self.div_val = div_val\n","        self.d_proj = d_proj\n","\n","        self.emb_scale = d_proj ** 0.5\n","\n","        self.cutoff_ends = [0] + self.cutoffs\n","\n","        self.emb_layers = []\n","        self.emb_projs = []\n","        if div_val == 1:\n","            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n","        else:\n","            for i in range(len(self.cutoffs)):\n","                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n","                d_emb_i = d_embed // (div_val ** i)\n","                self.emb_layers.append(\n","                    tf.keras.layers.Embedding(\n","                        r_idx - l_idx,\n","                        d_emb_i,\n","                        embeddings_initializer=get_initializer(init_std),\n","                        name=\"emb_layers_._{}\".format(i),\n","                    )\n","                )\n","\n","    def build(self, input_shape):\n","        for i in range(len(self.cutoffs)):\n","            d_emb_i = self.d_embed // (self.div_val ** i)\n","            self.emb_projs.append(\n","                self.add_weight(\n","                    shape=(d_emb_i, self.d_proj),\n","                    initializer=get_initializer(self.init_std),\n","                    trainable=True,\n","                    name=\"emb_projs_._{}\".format(i),\n","                )\n","            )\n","        super().build(input_shape)\n","\n","    def call(self, inp):\n","        if self.div_val == 1:\n","            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n","        else:\n","            inp_flat = tf.reshape(inp, (-1,))\n","            emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])\n","            for i in range(len(self.cutoffs)):\n","                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n","\n","                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n","\n","                inp_i = tf.boolean_mask(inp_flat, mask_i) - l_idx\n","                emb_i = self.emb_layers[i](inp_i)\n","                emb_i = tf.einsum(\"id,de->ie\", emb_i, self.emb_projs[i])\n","\n","                mask_idx = tf.cast(tf.where(mask_i), dtype=tf.int64)\n","                emb_flat += tf.scatter_nd(mask_idx, emb_i, tf.cast(shape_list(emb_flat), dtype=tf.int64))\n","\n","            embed_shape = shape_list(inp) + [self.d_proj]\n","            embed = tf.reshape(emb_flat, embed_shape)\n","\n","        embed *= self.emb_scale\n","\n","        return embed\n","\n","\n","@keras_serializable\n","class TFTransfoXLMainLayer(tf.keras.layers.Layer):\n","    config_class = TransfoXLConfig\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.output_hidden_states = config.output_hidden_states\n","        self.output_attentions = config.output_attentions\n","        self.return_dict = config.use_return_dict\n","\n","        self.n_token = config.vocab_size\n","\n","        self.d_embed = config.d_embed\n","        self.d_model = config.d_model\n","        self.n_head = config.n_head\n","        self.d_head = config.d_head\n","        self.untie_r = config.untie_r\n","\n","        self.word_emb = TFAdaptiveEmbedding(\n","            config.vocab_size,\n","            config.d_embed,\n","            config.d_model,\n","            config.cutoffs,\n","            div_val=config.div_val,\n","            init_std=config.init_std,\n","            name=\"word_emb\",\n","        )\n","\n","        self.drop = tf.keras.layers.Dropout(config.dropout)\n","\n","        self.n_layer = config.n_layer\n","        self.mem_len = config.mem_len\n","        self.attn_type = config.attn_type\n","\n","        self.layers = []\n","        if config.attn_type == 0:  # the default attention\n","            for i in range(config.n_layer):\n","                self.layers.append(\n","                    TFRelPartialLearnableDecoderLayer(\n","                        config.n_head,\n","                        config.d_model,\n","                        config.d_head,\n","                        config.d_inner,\n","                        config.dropout,\n","                        dropatt=config.dropatt,\n","                        pre_lnorm=config.pre_lnorm,\n","                        r_w_bias=None if self.untie_r else self.r_w_bias,\n","                        r_r_bias=None if self.untie_r else self.r_r_bias,\n","                        layer_norm_epsilon=config.layer_norm_epsilon,\n","                        init_std=config.init_std,\n","                        output_attentions=self.output_attentions,\n","                        name=\"layers_._{}\".format(i),\n","                    )\n","                )\n","        else:  # learnable embeddings and absolute embeddings\n","            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n","\n","        self.same_length = config.same_length\n","        self.clamp_len = config.clamp_len\n","\n","        if self.attn_type == 0:  # default attention\n","            self.pos_emb = TFPositionalEmbedding(self.d_model, name=\"pos_emb\")\n","        else:  # learnable embeddings and absolute embeddings\n","            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n","\n","    def build(self, input_shape):\n","        if not self.untie_r:\n","            self.r_w_bias = self.add_weight(\n","                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n","            )\n","            self.r_r_bias = self.add_weight(\n","                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n","            )\n","        super().build(input_shape)\n","\n","    def get_input_embeddings(self):\n","        return self.word_emb\n","\n","    def set_input_embeddings(self, value):\n","        raise NotImplementedError\n","\n","    def _resize_token_embeddings(self, new_num_tokens):\n","        return self.word_emb\n","\n","    def backward_compatible(self):\n","        self.sample_softmax = -1\n","\n","    def reset_memory_length(self, mem_len):\n","        self.mem_len = mem_len\n","\n","    def _prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    def init_mems(self, bsz):\n","        if self.mem_len > 0:\n","            mems = []\n","            for i in range(self.n_layer):\n","                empty = tf.zeros([self.mem_len, bsz, self.d_model])\n","                mems.append(empty)\n","\n","            return mems\n","        else:\n","            return None\n","\n","    def _update_mems(self, hids, mems, mlen, qlen):\n","\t\t\"\"\"\n","\t\t한 칸씩 슬라이딩하며, memory에 새로운 segment를 추가합니다. 이때, tf.stop_gradient를 통해 이전부터 보았던 segment는 gradient가 더 이상 흐르지 않도록 tf.stop_gradient를 사용\n","\t\t\"\"\"\n","        if mems is None:\n","            return None\n","\n","        # mems is not None\n","        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n","\n","        # `mlen + qlen` steps\n","        new_mems = []\n","        end_idx = mlen + max(0, qlen)\n","        beg_idx = max(0, end_idx - self.mem_len)\n","        for i in range(len(hids)):\n","\n","            cat = tf.concat([mems[i], hids[i]], axis=0)\n","            tf.stop_gradient(cat)\n","            new_mems.append(cat[beg_idx:end_idx])\n","\n","        return new_mems\n","\n","    def call(\n","        self,\n","        inputs,\n","        mems=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            mems = inputs[1] if len(inputs) > 1 else mems\n","            head_mask = inputs[2] if len(inputs) > 2 else head_mask\n","            inputs_embeds = inputs[3] if len(inputs) > 3 else inputs_embeds\n","            output_attentions = inputs[4] if len(inputs) > 4 else output_attentions\n","            output_hidden_states = inputs[5] if len(inputs) > 5 else output_hidden_states\n","            return_dict = inputs[6] if len(inputs) > 6 else return_dict\n","            assert len(inputs) <= 7, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            mems = inputs.get(\"mems\", mems)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 7, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n","        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_ids = tf.transpose(input_ids, perm=(1, 0))\n","            qlen, bsz = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n","            qlen, bsz = shape_list(inputs_embeds)[:2]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if mems is None:\n","            mems = self.init_mems(bsz)\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n","        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.n_layer\n","\n","        if inputs_embeds is not None:\n","            word_emb = inputs_embeds\n","        else:\n","            word_emb = self.word_emb(input_ids)\n","\n","        mlen = shape_list(mems[0])[0] if mems is not None else 0\n","        klen = mlen + qlen\n","\n","        attn_mask = tf.ones([qlen, qlen])\n","        mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n","        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n","        attn_mask_pad = tf.zeros([qlen, mlen])\n","        dec_attn_mask = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n","        if self.same_length:\n","            mask_l = tf.linalg.band_part(attn_mask, -1, 0)\n","            dec_attn_mask = tf.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)\n","        # ::: PyTorch masking code for reference :::\n","        # if self.same_length:\n","        #     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)\n","        #     mask_len = klen - self.mem_len\n","        #     if mask_len > 0:\n","        #         mask_shift_len = qlen - mask_len\n","        #     else:\n","        #         mask_shift_len = qlen\n","        #     dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n","        #             + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1\n","        # else:\n","        #     dec_attn_mask = torch.triu(\n","        #         word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]\n","\n","        hids = []\n","        attentions = [] if output_attentions else None\n","        if self.attn_type == 0:  # default\n","            pos_seq = tf.range(klen - 1, -1, -1.0)\n","            if self.clamp_len > 0:\n","                pos_seq = tf.minimum(pos_seq, self.clamp_len)\n","            pos_emb = self.pos_emb(pos_seq)\n","\n","            core_out = self.drop(word_emb, training=training)\n","            pos_emb = self.drop(pos_emb, training=training)\n","\n","            for i, layer in enumerate(self.layers):\n","                hids.append(core_out)\n","                mems_i = None if mems is None else mems[i]\n","                layer_outputs = layer(\n","                    core_out,\n","                    pos_emb,\n","                    dec_attn_mask,\n","                    mems_i,\n","                    head_mask[i],\n","                    output_attentions,\n","                    training=training,\n","                )\n","                core_out = layer_outputs[0]\n","                if output_attentions:\n","                    attentions.append(layer_outputs[1])\n","        else:  # learnable embeddings and absolute embeddings\n","            raise NotImplementedError\n","\n","        core_out = self.drop(core_out, training=training)\n","\n","        new_mems = self._update_mems(hids, mems, mlen, qlen)\n","\n","        # [bsz, len, hidden_dim]\n","        core_out = tf.transpose(core_out, perm=(1, 0, 2))\n","\n","        if output_hidden_states:\n","            # last layer를 추가하고 다시 library standard shape [bsz, len, hidden_dim]으로 transpose\n","            hids.append(core_out)\n","            hids = tuple(tf.transpose(t, perm=(1, 0, 2)) for t in hids)\n","        else:\n","            hids = None\n","        if output_attentions:\n","            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]\n","            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n","\n","        if not return_dict:\n","            return tuple(v for v in [core_out, new_mems, hids, attentions] if v is not None)\n","\n","        return TFTransfoXLModelOutput(\n","            last_hidden_state=core_out,\n","            mems=new_mems,\n","            hidden_states=hids,\n","            attentions=attentions,\n","        )\n","\n","class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, d_embed, d_proj, cutoffs, div_val=1, keep_order=False, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = vocab_size\n","        self.d_embed = d_embed\n","        self.d_proj = d_proj\n","\n","        self.cutoffs = cutoffs + [vocab_size]\n","        self.cutoff_ends = [0] + self.cutoffs\n","        self.div_val = div_val\n","\n","        self.shortlist_size = self.cutoffs[0]\n","        self.n_clusters = len(self.cutoffs) - 1\n","        self.head_size = self.shortlist_size + self.n_clusters\n","        self.keep_order = keep_order\n","\n","        self.out_layers = []\n","        self.out_projs = []\n","\n","    def build(self, input_shape):\n","        if self.n_clusters > 0:\n","            self.cluster_weight = self.add_weight(\n","                shape=(self.n_clusters, self.d_embed), initializer=\"zeros\", trainable=True, name=\"cluster_weight\"\n","            )\n","            self.cluster_bias = self.add_weight(\n","                shape=(self.n_clusters,), initializer=\"zeros\", trainable=True, name=\"cluster_bias\"\n","            )\n","\n","        if self.div_val == 1:\n","            for i in range(len(self.cutoffs)):\n","                if self.d_proj != self.d_embed:\n","                    weight = self.add_weight(\n","                        shape=(self.d_embed, self.d_proj),\n","                        initializer=\"zeros\",\n","                        trainable=True,\n","                        name=\"out_projs_._{}\".format(i),\n","                    )\n","                    self.out_projs.append(weight)\n","                else:\n","                    self.out_projs.append(None)\n","                weight = self.add_weight(\n","                    shape=(\n","                        self.vocab_size,\n","                        self.d_embed,\n","                    ),\n","                    initializer=\"zeros\",\n","                    trainable=True,\n","                    name=\"out_layers_._{}_._weight\".format(i),\n","                )\n","                bias = self.add_weight(\n","                    shape=(self.vocab_size,),\n","                    initializer=\"zeros\",\n","                    trainable=True,\n","                    name=\"out_layers_._{}_._bias\".format(i),\n","                )\n","                self.out_layers.append((weight, bias))\n","        else:\n","            for i in range(len(self.cutoffs)):\n","                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n","                d_emb_i = self.d_embed // (self.div_val ** i)\n","\n","                weight = self.add_weight(\n","                    shape=(d_emb_i, self.d_proj), initializer=\"zeros\", trainable=True, name=\"out_projs_._{}\".format(i)\n","                )\n","                self.out_projs.append(weight)\n","                weight = self.add_weight(\n","                    shape=(\n","                        r_idx - l_idx,\n","                        d_emb_i,\n","                    ),\n","                    initializer=\"zeros\",\n","                    trainable=True,\n","                    name=\"out_layers_._{}_._weight\".format(i),\n","                )\n","                bias = self.add_weight(\n","                    shape=(r_idx - l_idx,),\n","                    initializer=\"zeros\",\n","                    trainable=True,\n","                    name=\"out_layers_._{}_._bias\".format(i),\n","                )\n","                self.out_layers.append((weight, bias))\n","        super().build(input_shape)\n","\n","    @staticmethod\n","    def _logit(x, W, b, proj=None):\n","        y = x\n","        if proj is not None:\n","            y = tf.einsum(\"ibd,ed->ibe\", y, proj)\n","        return tf.einsum(\"ibd,nd->ibn\", y, W) + b\n","\n","    @staticmethod\n","    def _gather_logprob(logprob, target):\n","        lp_size = shape_list(logprob)\n","        r = tf.range(lp_size[0])\n","        idx = tf.stack([r, target], 1)\n","        return tf.gather_nd(logprob, idx)\n","\n","    def call(self, hidden, target, return_mean=True, training=False):\n","        head_logprob = 0\n","        if self.n_clusters == 0:\n","            output = self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])\n","            if target is not None:\n","                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n","            out = tf.nn.log_softmax(output, axis=-1)\n","        else:\n","            hidden_sizes = shape_list(hidden)\n","            out = []\n","            loss = tf.zeros(hidden_sizes[:2], dtype=tf.float32)\n","            for i in range(len(self.cutoffs)):\n","                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n","                if target is not None:\n","                    mask = (target >= l_idx) & (target < r_idx)\n","                    mask_idx = tf.where(mask)\n","                    cur_target = tf.boolean_mask(target, mask) - l_idx\n","\n","                if self.div_val == 1:\n","                    cur_W = self.out_layers[0][0][l_idx:r_idx]\n","                    cur_b = self.out_layers[0][1][l_idx:r_idx]\n","                else:\n","                    cur_W = self.out_layers[i][0]\n","                    cur_b = self.out_layers[i][1]\n","\n","                if i == 0:\n","                    cur_W = tf.concat([cur_W, self.cluster_weight], 0)\n","                    cur_b = tf.concat([cur_b, self.cluster_bias], 0)\n","\n","                    head_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[0])\n","                    head_logprob = tf.nn.log_softmax(head_logit)\n","                    out.append(head_logprob[..., : self.cutoffs[0]])\n","                    if target is not None:\n","                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n","                        cur_logprob = self._gather_logprob(cur_head_logprob, cur_target)\n","                else:\n","                    tail_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[i])\n","                    tail_logprob = tf.nn.log_softmax(tail_logit)\n","                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n","                    logprob_i = head_logprob[..., cluster_prob_idx, None] + tail_logprob\n","                    out.append(logprob_i)\n","                    if target is not None:\n","                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n","                        cur_tail_logprob = tf.boolean_mask(tail_logprob, mask)\n","                        cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)\n","                        cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]\n","                if target is not None:\n","                    loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))\n","            out = tf.concat(out, axis=-1)\n","\n","        if target is not None:\n","            if return_mean:\n","                loss = tf.reduce_mean(loss)\n","            # `self.add_loss()`를 통해 training시의 loss 추가\n","            self.add_loss(loss)\n","\n","            # Log the loss as a metric\n","            self.add_metric(loss, name=self.name, aggregation=\"mean\" if return_mean else \"\")\n","\n","        return out"],"metadata":{"id":"uA6CkzN8b6S8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# XLNet, BART\n","---"],"metadata":{"id":"yYMy9urHewal"}},{"cell_type":"markdown","source":["---\n","### XLNet\n","----\n","\n","XLNET은 구글 연구팀에서 발표한 논문으로 이전 스텝에 다루었던 transformer-XL을 이용한 아키텍처입니다.  \n","참고로 XLNet에서 XL이란 'eXtra-Long'에서 나왔으며, 트랜스포머보다 더 넓은 범위의 문맥을 볼 수 있다는 것을 강조하게 위해서 XLNet이라는 이름이 붙었다고 합니다.\n","  \n","[논문 XLNet: Generalized Autoregressive Pretraining for Language](https://arxiv.org/abs/1906.08237) \n","  \n","Understanding에서는 GPT의 AR(AutoRegressive) 언어 모델과 BERT의 AE(AutoEncoding) 언어 모델과는 다른 퍼뮤테이션(Permutation) 언어 모델을 통해 더욱 정교한 언어 모델 성능을 선보이고 있습니다."],"metadata":{"id":"psfziFfXe2-H"}},{"cell_type":"markdown","source":["---\n","### 퍼뮤테이션(Permutation) 언어 모델\n","---"],"metadata":{"id":"zsZCnW0SfGQJ"}},{"cell_type":"markdown","source":["임베딩 모델의 최근 흐름은 AR 모델과 AE 모델로 나눠집니다. 각각의 모델은 아래와 같이 정리할 수 있습니다.\n","\n","- AR(AutoRegressive) 모델: 이전 문맥을 바탕으로 다음 단어를 예측한다.\n","- AE(AutoEncoding) 모델: 앞뒤 문맥을 모두 살펴 [MASK] 단어를 예측한다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-18_16-58-49.max-800x600.png'>\n","<br>\n","  \n","하지만 AR 모델과 AE 모델에는 각각 치명적인 단점이 있었습니다.  \n","AR 모델은 문맥을 양방향으로 볼 수 없었고, AE 모델은 마스킹 처리한 토큰이 독립적으로 예측되므로 토큰 사이의 의존 관계를 학습할 수 없는데다가  \n","fine-tuning과 evaluation 시 [MASK] 토큰이 보이지 않아 불일치가 발생했습니다.\n","\n","이런 단점을 해결하기 위해 XLNet은 permutation LM을 사용하였죠.  \n","XLNet은 AR과 AE의 장점만을 취하여 AE처럼 양방향 context를 모두 볼 수 있는 동시에,  \n","AR처럼 예측해야 할 토큰들 간의 dependency를 놓치지 않고 학습할 수 있었습니다.\n","\n","아래 그림에서와 같이 토큰 4개의 문장을 랜덤을 뒤섞은 결과가 [3, 2, 4, 1]일 경우 \n","$x_3$을 예측하려고 합니다.  \n","이 경우에는 3번 토큰 정보를 넣으면 문제가 너무 쉬우므로 3번 토큰 정보를 주어서는 안 되고,  \n","2, 4, 1번 토큰 역시 3번 토큰 이후에 등장하므로 입력에서 제외해야 합니다.  \n","  \n","따라서 입력값은 세그먼트의 메모리 정보입니다.\n","  \n","또다시 토큰을 랜덤으로 섞은 결과가 [2, 4, 3, 1]이고 3번 토큰을 예측해야 한다면 3번 토큰의 이전 문맥인 2번, 4번 토큰과 메모리가 입력됩니다.\n","\n","토큰을 다시 섞은 후의 시퀀스가 [1, 4, 2, 3]이고 3번 토큰을 예측할 경우의 입력 벡터는 1번, 4번, 2번 토큰과 메모리입니다.  \n","문장 시퀀스가 [4, 3, 1, 2]이고 3번 토큰을 예측할 경우는 4번 토큰과 메모리를 입력으로 넣어 줍니다.  \n"," \n","이를 표현한 것이 아래의 그림입니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-18_17-08-52_HUihMF8.max-800x600.png'>\n","<br>\n","  \n","설명은 토큰을 섞는다고 하였지만 실제 구현은 어텐션 마스크로 구현되었습니다.  \n","그 이유는 XLNet의 기반이 트랜스포머 네트워크이고, 핵심이 쿼리(query)와 키(key) 벡터간 셀프 어텐션 기법이기 때문입니다.\n","\n","permutation 언어 모델에도 단점이 있었습니다.  \n","만약 셔플된 토큰 시퀀스가 [3, 2, 4, 1]과 [3, 2, 1, 4]일 경우 모델은 동일한 입력을 받아 다른 단어를 예측해야 하죠.  \n","  \n","이런 경우는 어떻게 해결해야 할까요?\n","\n"],"metadata":{"id":"QZN00uFGfKPc"}},{"cell_type":"markdown","source":["---\n","### Two-Stream Self-Attention\n","----"],"metadata":{"id":"2F81jGy6gE6G"}},{"cell_type":"markdown","source":["위와 같은 문제를 해결하기 위해 XLNet은 Target-Aware Representation for Transformer라는 것을 제안했습니다.  \n","아래의 식에서 $p_θ$ 는 다음에 나올 토큰의 분포입니다.  \n","기존의 Transformer과 달리 타겟 포지션 $z_t$를 식에 추가함으로써 동일한 representation으로 다른 타깃을 맞출 수 있었습니다.\n","\n","$$ pθ(X_{zt} = x ∣x_{z<t}) = \\frac{∑_{x}′exp(e(x^′)^⊤g θ(x_{z <t},z_t))}{exp(e(x)^⊤g θ(x_{z <t},z_t))} $$\n","\n","아래 그림과 같이 T 시점에서 타깃 토큰을 예측하기 위해 $g(x_{z<t},z_t$)는 T 시점 이전의 콘텍스트와 타깃 포지션을 사용해야 하고,  \n","T 시점 이후의 토큰을 예측하려면 $g(x_{z<t},z_t$)는 T 시점의 콘텍스트도 가지고 있어야 합니다.  \n","즉 T 시점과 T 시점 이후를 모두 고려하기 위해 2가지 hidden representation을 사용해야 하고,  \n","이 두 가지를 모두 사용할 수 있는 transformer 구조가 Two-Steam Self-Attention입니다.\n","  \n","  <Img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-18_20-29-13.png'>\n","  <br>\n","\n","Two-Stream self-attention은 쿼리 스트림(query stream)과 컨텐트 스트림(content stream)을 혼합한 것으로,  \n","쿼리 스트림은 T 시점 이전의 토큰 정보와 T 시점의 위치 정보를 나타내고,  \n","컨텐트 스트림은 기존의 self-attention과 같이 T 시점과 T 시점 이전의 토큰 정보를 사용합니다.\n","\n","아래의 그림에서 (a)는 콘텐트 스트림을 나타내고, (b)는 쿼리 스트림입니다.  \n","(c)는 두 개의 스트림 어텐션을 사용한 permutation 언어 모델을 나타낸 것입니다.  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-18_17-39-17.max-800x600.png'>  \n","<br>\n","\n"],"metadata":{"id":"Mg5UAdidgIz1"}},{"cell_type":"code","source":["class TFXLNetRelativeAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        if config.d_model % config.n_head != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.d_model, config.n_head)\n","            )\n","\n","        self.n_head = config.n_head\n","        self.d_head = config.d_head\n","        self.d_model = config.d_model\n","        self.scale = 1 / (config.d_head ** 0.5)\n","        self.initializer_range = config.initializer_range\n","        self.output_attentions = config.output_attentions\n","\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def build(self, input_shape):\n","        initializer = get_initializer(self.initializer_range)\n","        self.q = self.add_weight(\n","            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"q\"\n","        )\n","        self.k = self.add_weight(\n","            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"k\"\n","        )\n","        self.v = self.add_weight(\n","            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"v\"\n","        )\n","        self.o = self.add_weight(\n","            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"o\"\n","        )\n","        self.r = self.add_weight(\n","            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"r\"\n","        )\n","        self.r_r_bias = self.add_weight(\n","            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n","        )\n","        self.r_s_bias = self.add_weight(\n","            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_s_bias\"\n","        )\n","        self.r_w_bias = self.add_weight(\n","            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n","        )\n","        self.seg_embed = self.add_weight(\n","            shape=(2, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"seg_embed\"\n","        )\n","        super().build(input_shape)\n","\n","    def prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    def rel_shift(self, x, klen=-1):\n","        \"\"\"perform relative shift to form the relative attention score.\"\"\"\n","        x_size = shape_list(x)\n","\n","        x = tf.reshape(x, (x_size[1], x_size[0], x_size[2], x_size[3]))\n","        x = x[1:, ...]\n","        x = tf.reshape(x, (x_size[0], x_size[1] - 1, x_size[2], x_size[3]))\n","        x = x[:, 0:klen, :, :]\n","        # x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n","\n","        return x\n","\n","    def rel_attn_core(\n","        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n","    ):\n","        \"\"\"Core relative positional attention operations.\"\"\"\n","        # content based attention score\n","        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n","\n","        # position based attention score\n","        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n","        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n","\n","        # segment based attention score\n","        if seg_mat is None:\n","            ef = 0\n","        else:\n","            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)\n","            ef = tf.einsum(\"ijbs,ibns->ijbn\", seg_mat, ef)\n","\n","        # merge attention scores and perform masking\n","        attn_score = (ac + bd + ef) * self.scale\n","        if attn_mask is not None:\n","            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n","            if attn_mask.dtype == tf.float16:\n","                attn_score = attn_score - 65500 * attn_mask\n","            else:\n","                attn_score = attn_score - 1e30 * attn_mask\n","\n","        # attention probability\n","        attn_prob = tf.nn.softmax(attn_score, axis=1)\n","\n","        attn_prob = self.dropout(attn_prob, training=training)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attn_prob = attn_prob * head_mask\n","\n","        # attention output\n","        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, v_head_h)\n","\n","        if output_attentions:\n","            return attn_vec, attn_prob\n","\n","        return attn_vec\n","\n","    def post_attention(self, h, attn_vec, residual=True, training=False):\n","        \"\"\"Post-attention processing.\"\"\"\n","        # post-attention projection (back to `d_model`)\n","        attn_out = tf.einsum(\"ibnd,hnd->ibh\", attn_vec, self.o)\n","\n","        attn_out = self.dropout(attn_out, training=training)\n","\n","        if residual:\n","            attn_out = attn_out + h\n","        output = self.layer_norm(attn_out)\n","\n","        return output\n","\n","    def call(\n","        self,\n","        h,\n","        g,\n","        attn_mask_h,\n","        attn_mask_g,\n","        r,\n","        seg_mat,\n","        mems,\n","        target_mapping,\n","        head_mask,\n","        output_attentions,\n","        training=False,\n","    ):\n","        if g is not None:\n","            # Two-stream attention with relative positional encoding.\n","            # content based attention score\n","            if mems is not None and len(shape_list(mems)) > 1:\n","                cat = tf.concat([mems, h], axis=0)\n","            else:\n","                cat = h\n","\n","            # content-based key head\n","            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n","\n","            # content-based value head\n","            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n","\n","            # position-based key head\n","            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)\n","\n","            # h-stream\n","            # content-stream query head\n","            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n","\n","            # core attention ops\n","            attn_vec_h = self.rel_attn_core(\n","                q_head_h,\n","                k_head_h,\n","                v_head_h,\n","                k_head_r,\n","                seg_mat,\n","                attn_mask_h,\n","                head_mask,\n","                output_attentions,\n","                training=training,\n","            )\n","\n","            if output_attentions:\n","                attn_vec_h, attn_prob_h = attn_vec_h\n","\n","            # post processing\n","            output_h = self.post_attention(h, attn_vec_h, training=training)\n","\n","            # g-stream\n","            # query-stream query head\n","            q_head_g = tf.einsum(\"ibh,hnd->ibnd\", g, self.q)\n","\n","            # core attention ops\n","            if target_mapping is not None:\n","                q_head_g = tf.einsum(\"mbnd,mlb->lbnd\", q_head_g, target_mapping)\n","                attn_vec_g = self.rel_attn_core(\n","                    q_head_g,\n","                    k_head_h,\n","                    v_head_h,\n","                    k_head_r,\n","                    seg_mat,\n","                    attn_mask_g,\n","                    head_mask,\n","                    output_attentions,\n","                    training=training,\n","                )\n","\n","                if output_attentions:\n","                    attn_vec_g, attn_prob_g = attn_vec_g\n","\n","                attn_vec_g = tf.einsum(\"lbnd,mlb->mbnd\", attn_vec_g, target_mapping)\n","            else:\n","                attn_vec_g = self.rel_attn_core(\n","                    q_head_g,\n","                    k_head_h,\n","                    v_head_h,\n","                    k_head_r,\n","                    seg_mat,\n","                    attn_mask_g,\n","                    head_mask,\n","                    output_attentions,\n","                    training=training,\n","                )\n","\n","                if output_attentions:\n","                    attn_vec_g, attn_prob_g = attn_vec_g\n","\n","            # post processing\n","            output_g = self.post_attention(g, attn_vec_g, training=training)\n","\n","            if output_attentions:\n","                attn_prob = attn_prob_h, attn_prob_g\n","\n","        else:\n","            # Multi-head attention with relative positional encoding\n","            if mems is not None and len(shape_list(mems)) > 1:\n","                cat = tf.concat([mems, h], axis=0)\n","            else:\n","                cat = h\n","\n","            # content heads\n","            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n","            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n","            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n","\n","            # positional heads\n","            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)\n","\n","            # core attention ops\n","            attn_vec = self.rel_attn_core(\n","                q_head_h,\n","                k_head_h,\n","                v_head_h,\n","                k_head_r,\n","                seg_mat,\n","                attn_mask_h,\n","                head_mask,\n","                output_attentions,\n","                training=training,\n","            )\n","\n","            if output_attentions:\n","                attn_vec, attn_prob = attn_vec\n","\n","            # post processing\n","            output_h = self.post_attention(h, attn_vec, training=training)\n","            output_g = None\n","\n","        outputs = (output_h, output_g)\n","        if output_attentions:\n","            outputs = outputs + (attn_prob,)\n","        return outputs\n","\n","\n","class TFXLNetFeedForward(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n","        self.layer_1 = tf.keras.layers.Dense(\n","            config.d_inner, kernel_initializer=get_initializer(config.initializer_range), name=\"layer_1\"\n","        )\n","        self.layer_2 = tf.keras.layers.Dense(\n","            config.d_model, kernel_initializer=get_initializer(config.initializer_range), name=\"layer_2\"\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        if isinstance(config.ff_activation, str):\n","            self.activation_function = get_tf_activation(config.ff_activation)\n","        else:\n","            self.activation_function = config.ff_activation\n","\n","    def call(self, inp, training=False):\n","        output = inp\n","        output = self.layer_1(output)\n","        output = self.activation_function(output)\n","        output = self.dropout(output, training=training)\n","        output = self.layer_2(output)\n","        output = self.dropout(output, training=training)\n","        output = self.layer_norm(output + inp)\n","        return output\n","\n","\n","class TFXLNetLayer(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.rel_attn = TFXLNetRelativeAttention(config, name=\"rel_attn\")\n","        self.ff = TFXLNetFeedForward(config, name=\"ff\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def call(\n","        self,\n","        output_h,\n","        output_g,\n","        non_tgt_mask,\n","        attn_mask,\n","        pos_emb,\n","        seg_mat,\n","        mems,\n","        target_mapping,\n","        head_mask,\n","        output_attentions,\n","        training=False,\n","    ):\n","        outputs = self.rel_attn(\n","            output_h,\n","            output_g,\n","            non_tgt_mask,\n","            attn_mask,\n","            pos_emb,\n","            seg_mat,\n","            mems,\n","            target_mapping,\n","            head_mask,\n","            output_attentions,\n","            training=training,\n","        )\n","        output_h, output_g = outputs[:2]\n","\n","        if output_g is not None:\n","            output_g = self.ff(output_g, training=training)\n","        output_h = self.ff(output_h, training=training)\n","\n","        outputs = (output_h, output_g) + outputs[2:]  # Add again attentions if there are there\n","        return outputs\n","\n","\n","class TFXLNetLMHead(tf.keras.layers.Layer):\n","    def __init__(self, config, input_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","        self.vocab_size = config.vocab_size\n","        # The output weights are the same as the input embeddings, but there is\n","        # an output-only bias for each token.\n","        self.input_embeddings = input_embeddings\n","\n","    def build(self, input_shape):\n","        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n","        super().build(input_shape)\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n","        hidden_states = hidden_states + self.bias\n","        return hidden_states\n","\n","\n","@keras_serializable\n","class TFXLNetMainLayer(tf.keras.layers.Layer):\n","    config_class = XLNetConfig\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.config = config\n","        self.output_hidden_states = config.output_hidden_states\n","        self.output_attentions = config.output_attentions\n","        self.return_dict = config.return_dict\n","\n","        self.mem_len = config.mem_len\n","        self.reuse_len = config.reuse_len\n","        self.d_model = config.d_model\n","        self.same_length = config.same_length\n","        self.attn_type = config.attn_type\n","        self.bi_data = config.bi_data\n","        self.clamp_len = config.clamp_len\n","        self.n_layer = config.n_layer\n","        self.use_bfloat16 = config.use_bfloat16\n","        self.initializer_range = config.initializer_range\n","\n","        self.word_embedding = TFSharedEmbeddings(\n","            config.vocab_size, config.d_model, initializer_range=config.initializer_range, name=\"word_embedding\"\n","        )\n","        self.layer = [TFXLNetLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.n_layer)]\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","        self.use_mems_eval = config.use_mems_eval\n","        self.use_mems_train = config.use_mems_train\n","\n","    def get_input_embeddings(self):\n","        return self.word_embedding\n","\n","    def set_input_embeddings(self, value):\n","        self.word_embedding.weight = value\n","        self.word_embedding.vocab_size = value.shape[0]\n","\n","    def build(self, input_shape):\n","        initializer = get_initializer(self.initializer_range)\n","        self.mask_emb = self.add_weight(\n","            shape=(1, 1, self.d_model), initializer=initializer, trainable=True, name=\"mask_emb\"\n","        )\n","\n","    def _resize_token_embeddings(self, new_num_tokens):\n","        raise NotImplementedError\n","\n","    def _prune_heads(self, heads_to_prune):\n","        raise NotImplementedError\n","\n","    def create_mask(self, qlen, mlen, dtype=tf.float32):\n","        \"\"\"\n","        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n","\n","        Args:\n","            qlen: TODO Lysandre didn't fill\n","            mlen: TODO Lysandre didn't fill\n","\n","        ::\n","\n","                  same_length=False:      same_length=True:\n","                   <  qlen >        <  qlen >\n","               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n","                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n","            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n","                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n","               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n","\n","        \"\"\"\n","        attn_mask = tf.ones([qlen, qlen], dtype=dtype)\n","        mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n","        mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n","        attn_mask_pad = tf.zeros([qlen, mlen], dtype=dtype)\n","        ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n","        if self.same_length:\n","            mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n","            ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n","        return ret\n","\n","    def cache_mem(self, curr_out, prev_mem):\n","        # cache hidden states into memory.\n","        if self.reuse_len is not None and self.reuse_len > 0:\n","            curr_out = curr_out[: self.reuse_len]\n","\n","        if self.mem_len is None or self.mem_len == 0:\n","            # If :obj:`use_mems` is active but no `mem_len` is defined, the model behaves like GPT-2 at inference time\n","            # and returns all of the past and current hidden states.\n","            cutoff = 0\n","        else:\n","            # If :obj:`use_mems` is active and `mem_len` is defined, the model returns the last `mem_len` hidden\n","            # states. This is the preferred setting for training and long-form generation.\n","            cutoff = -self.mem_len\n","        if prev_mem is None:\n","            # if :obj:`use_mems` is active and `mem_len` is defined, the model\n","            new_mem = curr_out[cutoff:]\n","        else:\n","            new_mem = tf.concat([prev_mem, curr_out], 0)[cutoff:]\n","\n","        return tf.stop_gradient(new_mem)\n","\n","    @staticmethod\n","    def positional_embedding(pos_seq, inv_freq, bsz=None):\n","        sinusoid_inp = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n","        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], axis=-1)\n","        pos_emb = pos_emb[:, None, :]\n","\n","        if bsz is not None:\n","            pos_emb = tf.tile(pos_emb, [1, bsz, 1])\n","\n","        return pos_emb\n","\n","    def relative_positional_encoding(self, qlen, klen, bsz=None, dtype=None):\n","        \"\"\"create relative positional encoding.\"\"\"\n","        freq_seq = tf.range(0, self.d_model, 2.0)\n","        if dtype is not None and dtype != tf.float32:\n","            freq_seq = tf.cast(freq_seq, dtype=dtype)\n","        inv_freq = 1 / (10000 ** (freq_seq / self.d_model))\n","\n","        if self.attn_type == \"bi\":\n","            # beg, end = klen - 1, -qlen\n","            beg, end = klen, -qlen\n","        elif self.attn_type == \"uni\":\n","            # beg, end = klen - 1, -1\n","            beg, end = klen, -1\n","        else:\n","            raise ValueError(\"Unknown `attn_type` {}.\".format(self.attn_type))\n","\n","        if self.bi_data:\n","            fwd_pos_seq = tf.range(beg, end, -1.0)\n","            bwd_pos_seq = tf.range(-beg, -end, 1.0)\n","\n","            if dtype is not None and dtype != tf.float32:\n","                fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n","                bwd_pos_seq = tf.cast(bwd_pos_seq, dtype=dtype)\n","\n","            if self.clamp_len > 0:\n","                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)\n","                bwd_pos_seq = tf.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)\n","\n","            if bsz is not None:\n","                assert bsz % 2 == 0, f\"With bi_data, the batch size {bsz} should be divisible by 2\"\n","                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n","                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n","            else:\n","                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n","                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n","\n","            pos_emb = tf.concat([fwd_pos_emb, bwd_pos_emb], axis=1)\n","        else:\n","            fwd_pos_seq = tf.range(beg, end, -1.0)\n","            if dtype is not None and dtype != tf.float32:\n","                fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n","            if self.clamp_len > 0:\n","                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)\n","            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n","\n","        return pos_emb\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        mems=None,\n","        perm_mask=None,\n","        target_mapping=None,\n","        token_type_ids=None,\n","        input_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        use_mems=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ):\n","        inputs = input_processing(\n","            func=self.call,\n","            config=self.config,\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            mems=mems,\n","            perm_mask=perm_mask,\n","            target_mapping=target_mapping,\n","            token_type_ids=token_type_ids,\n","            input_mask=input_mask,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            use_mems=use_mems,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","            kwargs_call=kwargs,\n","        )\n","\n","        if training and inputs[\"use_mems\"] is None:\n","            inputs[\"use_mems\"] = self.use_mems_train\n","        else:\n","            inputs[\"use_mems\"] = self.use_mems_eval\n","\n","        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n","        # but we want a unified interface in the library with the batch size on the first dimension\n","        # so we move here the first dimension (batch) to the end\n","\n","        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif inputs[\"input_ids\"] is not None:\n","            inputs[\"input_ids\"] = tf.transpose(inputs[\"input_ids\"], perm=(1, 0))\n","            qlen, bsz = shape_list(inputs[\"input_ids\"])[:2]\n","        elif inputs[\"inputs_embeds\"] is not None:\n","            inputs[\"inputs_embeds\"] = tf.transpose(inputs[\"inputs_embeds\"], perm=(1, 0, 2))\n","            qlen, bsz = shape_list(inputs[\"inputs_embeds\"])[:2]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        inputs[\"token_type_ids\"] = (\n","            tf.transpose(inputs[\"token_type_ids\"], perm=(1, 0)) if inputs[\"token_type_ids\"] is not None else None\n","        )\n","        inputs[\"input_mask\"] = (\n","            tf.transpose(inputs[\"input_mask\"], perm=(1, 0)) if inputs[\"input_mask\"] is not None else None\n","        )\n","        inputs[\"attention_mask\"] = (\n","            tf.transpose(inputs[\"attention_mask\"], perm=(1, 0)) if inputs[\"attention_mask\"] is not None else None\n","        )\n","        inputs[\"perm_mask\"] = (\n","            tf.transpose(inputs[\"perm_mask\"], perm=(1, 2, 0)) if inputs[\"perm_mask\"] is not None else None\n","        )\n","        inputs[\"target_mapping\"] = (\n","            tf.transpose(inputs[\"target_mapping\"], perm=(1, 2, 0)) if inputs[\"target_mapping\"] is not None else None\n","        )\n","\n","        mlen = shape_list(inputs[\"mems\"][0])[0] if inputs[\"mems\"] is not None and inputs[\"mems\"][0] is not None else 0\n","        klen = mlen + qlen\n","\n","        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n","\n","        # Attention mask\n","        # causal attention mask\n","        if self.attn_type == \"uni\":\n","            attn_mask = self.create_mask(qlen, mlen)\n","            attn_mask = attn_mask[:, :, None, None]\n","        elif self.attn_type == \"bi\":\n","            attn_mask = None\n","        else:\n","            raise ValueError(\"Unsupported attention type: {}\".format(self.attn_type))\n","\n","        # data mask: input mask & perm mask\n","        assert inputs[\"input_mask\"] is None or inputs[\"attention_mask\"] is None, (\n","            \"You can only use one of input_mask (uses 1 for padding) \"\n","            \"or attention_mask (uses 0 for padding, added for compatibility with BERT). Please choose one.\"\n","        )\n","        if inputs[\"input_mask\"] is None and inputs[\"attention_mask\"] is not None:\n","            inputs[\"input_mask\"] = 1.0 - tf.cast(inputs[\"attention_mask\"], dtype=dtype_float)\n","        if inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is not None:\n","            data_mask = inputs[\"input_mask\"][None] + inputs[\"perm_mask\"]\n","        elif inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is None:\n","            data_mask = inputs[\"input_mask\"][None]\n","        elif inputs[\"input_mask\"] is None and inputs[\"perm_mask\"] is not None:\n","            data_mask = inputs[\"perm_mask\"]\n","        else:\n","            data_mask = None\n","\n","        if data_mask is not None:\n","            # all mems can be attended to\n","            if mlen > 0:\n","                mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz], dtype=dtype_float)\n","                data_mask = tf.concat([mems_mask, data_mask], axis=1)\n","            if attn_mask is None:\n","                attn_mask = data_mask[:, :, :, None]\n","            else:\n","                attn_mask += data_mask[:, :, :, None]\n","\n","        if attn_mask is not None:\n","            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n","\n","        if attn_mask is not None:\n","            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n","            if mlen > 0:\n","                non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n","            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n","        else:\n","            non_tgt_mask = None\n","\n","        # Word embeddings and prepare h & g hidden states\n","        if inputs[\"inputs_embeds\"] is not None:\n","            word_emb_k = inputs[\"inputs_embeds\"]\n","        else:\n","            word_emb_k = self.word_embedding(inputs[\"input_ids\"])\n","        output_h = self.dropout(word_emb_k, training=inputs[\"training\"])\n","        if inputs[\"target_mapping\"] is not None:\n","            word_emb_q = tf.tile(self.mask_emb, [shape_list(inputs[\"target_mapping\"])[0], bsz, 1])\n","            # else:  # We removed the inp_q input which was same as target mapping\n","            #     inp_q_ext = inp_q[:, :, None]\n","            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n","            output_g = self.dropout(word_emb_q, training=inputs[\"training\"])\n","        else:\n","            output_g = None\n","\n","        # Segment embedding\n","        if inputs[\"token_type_ids\"] is not None:\n","            # Convert `token_type_ids` to one-hot `seg_mat`\n","            if mlen > 0:\n","                mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n","                cat_ids = tf.concat([mem_pad, inputs[\"token_type_ids\"]], 0)\n","            else:\n","                cat_ids = inputs[\"token_type_ids\"]\n","\n","            # `1` indicates not in the same segment [qlen x klen x bsz]\n","            seg_mat = tf.cast(tf.logical_not(tf.equal(inputs[\"token_type_ids\"][:, None], cat_ids[None, :])), tf.int32)\n","            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n","        else:\n","            seg_mat = None\n","\n","        # Positional encoding\n","        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n","        pos_emb = self.dropout(pos_emb, training=inputs[\"training\"])\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n","        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n","        if inputs[\"head_mask\"] is not None:\n","            raise NotImplementedError\n","        else:\n","            inputs[\"head_mask\"] = [None] * self.n_layer\n","\n","        new_mems = ()\n","        if inputs[\"mems\"] is None:\n","            inputs[\"mems\"] = [None] * len(self.layer)\n","\n","        attentions = [] if inputs[\"output_attentions\"] else None\n","        hidden_states = [] if inputs[\"output_hidden_states\"] else None\n","        for i, layer_module in enumerate(self.layer):\n","            # cache new mems\n","            if inputs[\"use_mems\"]:\n","                new_mems = new_mems + (self.cache_mem(output_h, inputs[\"mems\"][i]),)\n","            if inputs[\"output_hidden_states\"]:\n","                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n","\n","            outputs = layer_module(\n","                output_h,\n","                output_g,\n","                non_tgt_mask,\n","                attn_mask,\n","                pos_emb,\n","                seg_mat,\n","                inputs[\"mems\"][i],\n","                inputs[\"target_mapping\"],\n","                inputs[\"head_mask\"][i],\n","                inputs[\"output_attentions\"],\n","                training=inputs[\"training\"],\n","            )\n","            output_h, output_g = outputs[:2]\n","            if inputs[\"output_attentions\"]:\n","                attentions.append(outputs[2])\n","\n","        # Add last hidden state\n","        if inputs[\"output_hidden_states\"]:\n","            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n","\n","        output = self.dropout(output_g if output_g is not None else output_h, training=inputs[\"training\"])\n","\n","        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n","        output = tf.transpose(output, perm=(1, 0, 2))\n","\n","        if not inputs[\"use_mems\"]:\n","            new_mems = None\n","        if inputs[\"output_hidden_states\"]:\n","            if output_g is not None:\n","                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n","            else:\n","                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n","        if inputs[\"output_attentions\"]:\n","            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n","\n","        if not inputs[\"return_dict\"]:\n","            return tuple(v for v in [output, new_mems, hidden_states, attentions] if v is not None)\n","\n","        return TFXLNetModelOutput(\n","            last_hidden_state=output, mems=new_mems, hidden_states=hidden_states, attentions=attentions\n","        )"],"metadata":{"id":"Y3FhO7VygD6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### BART\n","---"],"metadata":{"id":"i3noEELwia_O"}},{"cell_type":"markdown","source":["BART는 특정 task가 아닌 다양한 task에 적용할 수 있도록 seq2seq 구조로 만들어진 denoising auto-encoder입니다.  \n","BART의 경우도 XLNet과 같이 AR 모델과 AE 모델의 장점을 사용한 모델이라고 보시면 됩니다.  \n","아래의 그림과 같이 Encoder에서는 BERT와 같은 구조를, Decoder에서는 GPT와 같은 구조를 가지고 있죠.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-18_20-45-32.png'>  \n","<br>\n","\n","즉 손상된 text를 입력받아 bidirectional 모델로 인코딩하고, 정답 text에 대한 likelihood를 Autoregressive decoder로 계산합니다.  \n","fine-tuning할 때는 손상되지 않은 문서가 encoder와 decoder 모두에 들어가고, decoder의 최종 hidden state로부터의 representations를 사용하게 됩니다.\n","  \n","이런 설정은 noising이 자유롭다는 장점이 있기 때문에 BART에서는 아래와 같은 5가지 noising 방식을 사용하였습니다.  \n","  \n","- Token Masking: 임의의 토큰을 마스킹하고 복구하는 방식\n","- Token Deletion: 임의의 토큰을 삭제하고 그 토큰의 위치를 찾는 방식\n","- Text Infilling: 포아송 분포를 따르는 길이의 text span을 생성해 하나의 마스크 토큰으로 마스킹하고, 그 토큰에 몇 개의 토큰이 존재하는지 예측\n","- Sentence Permutaion: 문장의 순서를 랜덤으로 섞는 방식\n","- Document Rotation: 토큰 하나를 정해 그 토큰을 시작점으로 하여 회전시킨 후, 문서의 시작점을 찾도록 학습\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-18_20-49-41.max-800x600.png'>\n","<br>\n","\n"],"metadata":{"id":"dkP1tGcMiilK"}},{"cell_type":"code","source":["class TFBartLearnedPositionalEmbedding(TFSharedEmbeddings):\n","    \"\"\"\n","    This module learns positional embeddings up to a fixed maximum size. Padding ids are ignored by either offsetting\n","    based on padding_idx or by setting padding_idx to None and ensuring that the appropriate position ids are passed to\n","    the forward function.\n","    \"\"\"\n","\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset, **kwargs):\n","        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n","        # and adjust num_embeddings appropriately. Other models dont have this hack\n","        self.offset = offset\n","        assert padding_idx is not None, \"padding_idx cannot be None\"\n","        num_embeddings += offset\n","        super().__init__(num_embeddings, embedding_dim, **kwargs)\n","\n","    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        bsz, seq_len = input_shape[:2]\n","\n","        positions = tf.range(\n","            past_key_values_length, seq_len + past_key_values_length, delta=1, dtype=tf.int32, name=\"range\"\n","        )\n","        return super().call(positions + self.offset)  # super object is not callable for some reason\n","\n","\n","class TFBartSinusoidalPositionalEmbedding(tf.keras.layers.Embedding):\n","    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n","\n","    def __init__(self, num_positions: int, embedding_dim: int, **kwargs):\n","\n","        if embedding_dim % 2 != 0:\n","            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n","        super().__init__(\n","            num_positions,\n","            embedding_dim,\n","            **kwargs,\n","        )\n","\n","    def build(self, input_shape: tf.TensorShape):\n","        \"\"\"\n","        Build shared token embedding layer Shared weights logic adapted from\n","        https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n","        \"\"\"\n","        super().build(input_shape)  # Instantiates self.weight so it can be loaded\n","        weight: np.ndarray = self._init_weight(self.input_dim, self.output_dim)\n","        self.set_weights([weight])  # overwrite self.weight to correct value\n","\n","    @staticmethod\n","    def _init_weight(n_pos: int, dim: int):\n","        \"\"\"\n","        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n","        the 2nd half of the vector. [dim // 2:]\n","        \"\"\"\n","        position_enc = np.array(\n","            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n","        )\n","        # index 0 is all zero\n","        position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\n","        position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\n","        # convert to tensor\n","        table = tf.convert_to_tensor(position_enc, dtype=tf.float32)\n","        tf.stop_gradient(table)\n","        return table\n","\n","    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        bsz, seq_len = input_shape[:2]\n","\n","        positions = tf.range(\n","            past_key_values_length, seq_len + past_key_values_length, delta=1, dtype=tf.int32, name=\"range\"\n","        )\n","        return super().call(positions)\n","\n","\n","class TFBartAttention(tf.keras.layers.Layer):\n","    \"\"\"Multi-headed attention from \"Attention Is All You Need\"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        dropout: float = 0.0,\n","        is_decoder: bool = False,\n","        bias: bool = True,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","\n","        self.num_heads = num_heads\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","        self.head_dim = embed_dim // num_heads\n","        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n","        self.scaling = self.head_dim ** -0.5\n","        self.is_decoder = is_decoder\n","\n","        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n","        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n","        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n","        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")\n","\n","    def _shape(self, tensor: tf.Tensor, seq_len: int, bsz: int):\n","        return tf.transpose(tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim)), (0, 2, 1, 3))\n","\n","    def call(\n","        self,\n","        hidden_states: tf.Tensor,\n","        key_value_states: Optional[tf.Tensor] = None,\n","        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n","        attention_mask: Optional[tf.Tensor] = None,\n","        training=False,\n","    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","        # if key_value_states are provided this layer is used as a cross-attention layer\n","        # for the decoder\n","        is_cross_attention = key_value_states is not None\n","        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n","\n","        # get query proj\n","        query_states = self.q_proj(hidden_states) * self.scaling\n","        # get key, value proj\n","        if is_cross_attention and past_key_value is not None:\n","            # reuse k,v, cross_attentions\n","            key_states = past_key_value[0]\n","            value_states = past_key_value[1]\n","        elif is_cross_attention:\n","            # cross_attentions\n","            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n","        elif past_key_value is not None:\n","            # reuse k, v, self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n","            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n","        else:\n","            # self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","        if self.is_decoder:\n","            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n","            # Further calls to cross_attention layer can then reuse all cross-attention\n","            # key/value_states (first \"if\" case)\n","            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n","            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","            # if encoder bi-directional self-attention `past_key_value` is always `None`\n","            past_key_value = (key_states, value_states)\n","\n","        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n","        key_states = tf.reshape(key_states, proj_shape)\n","        value_states = tf.reshape(value_states, proj_shape)\n","\n","        src_len = shape_list(key_states)[1]\n","        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n","\n","        tf.debugging.assert_equal(\n","            shape_list(attn_weights),\n","            [bsz * self.num_heads, tgt_len, src_len],\n","            message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n","        )\n","\n","        if attention_mask is not None:\n","            tf.debugging.assert_equal(\n","                shape_list(attention_mask),\n","                [bsz, 1, tgt_len, src_len],\n","                message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n","            )\n","            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n","            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n","\n","        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n","\n","        attn_probs = self.dropout(attn_weights, training=training)\n","\n","        attn_output = tf.matmul(attn_probs, value_states)\n","\n","        tf.debugging.assert_equal(\n","            shape_list(attn_output),\n","            [bsz * self.num_heads, tgt_len, self.head_dim],\n","            message=f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {shape_list(attn_output)}\",\n","        )\n","\n","        attn_output = tf.transpose(\n","            tf.reshape(attn_output, (bsz, self.num_heads, tgt_len, self.head_dim)), (0, 2, 1, 3)\n","        )\n","        attn_output = tf.reshape(attn_output, (bsz, tgt_len, embed_dim))\n","\n","        attn_output = self.out_proj(attn_output)\n","        attn_weights: tf.Tensor = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len))\n","\n","        return attn_output, attn_weights, past_key_value\n","\n","\n","class TFBartEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, config: BartConfig, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = config.d_model\n","        self.self_attn = TFBartAttention(\n","            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n","        )\n","        self.normalize_before = config.normalize_before\n","        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.activation_fn = ACT2FN[config.activation_function]\n","        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n","        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n","        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n","        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n","\n","    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor, training=False):\n","        \"\"\"\n","        Args:\n","            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n","            attention_mask (:obj:`tf.Tensor`): attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","        \"\"\"\n","        residual = hidden_states\n","        if self.normalize_before:\n","            hidden_states = self.self_attn_layer_norm(hidden_states)\n","        hidden_states, self_attn_weights, _ = self.self_attn(\n","            hidden_states=hidden_states, attention_mask=attention_mask\n","        )\n","        tf.debugging.assert_equal(\n","            shape_list(hidden_states),\n","            shape_list(residual),\n","            message=f\"Self attn modified the shape of query {shape_list(residual)} to {shape_list(hidden_states)}\",\n","        )\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","        if not self.normalize_before:\n","            hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","        residual = hidden_states\n","        if self.normalize_before:\n","            hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.activation_fn(self.fc1(hidden_states))\n","        hidden_states = self.activation_dropout(hidden_states, training=training)\n","        hidden_states = self.fc2(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","        if not self.normalize_before:\n","            hidden_states = self.final_layer_norm(hidden_states)\n","\n","        return hidden_states, self_attn_weights\n","\n","\n","class TFBartDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, config: BartConfig, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = config.d_model\n","        self.self_attn = TFBartAttention(\n","            embed_dim=self.embed_dim,\n","            num_heads=config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","            name=\"self_attn\",\n","            is_decoder=True,\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.activation_fn = ACT2FN[config.activation_function]\n","        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n","        self.normalize_before = config.normalize_before\n","\n","        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n","        self.encoder_attn = TFBartAttention(\n","            self.embed_dim,\n","            config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","            name=\"encoder_attn\",\n","            is_decoder=True,\n","        )\n","        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n","        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n","        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n","        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask: Optional[tf.Tensor] = None,\n","        encoder_hidden_states: Optional[tf.Tensor] = None,\n","        encoder_attention_mask: Optional[tf.Tensor] = None,\n","        past_key_value: Optional[Tuple[tf.Tensor]] = None,\n","        training=False,\n","    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n","        \"\"\"\n","        Args:\n","            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n","            attention_mask (:obj:`tf.Tensor`): attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","            encoder_hidden_states (:obj:`tf.Tensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n","            encoder_attention_mask (:obj:`tf.Tensor`): encoder attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","            past_key_value (:obj:`Tuple(tf.Tensor)`): cached past key and value projection states\n","        \"\"\"\n","        residual = hidden_states\n","        if self.normalize_before:\n","            hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","        # Self Attention\n","        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n","        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n","        # add present self-attn cache to positions 1,2 of present_key_value tuple\n","        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","            hidden_states=hidden_states,\n","            past_key_value=self_attn_past_key_value,\n","            attention_mask=attention_mask,\n","        )\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","        if not self.normalize_before:\n","            hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","        # Cross-Attention Block\n","        cross_attn_present_key_value = None\n","        if encoder_hidden_states is not None:\n","            residual = hidden_states\n","            if self.normalize_before:\n","                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n","\n","            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n","            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n","            hidden_states, _, cross_attn_present_key_value = self.encoder_attn(\n","                hidden_states=hidden_states,\n","                key_value_states=encoder_hidden_states,\n","                attention_mask=encoder_attention_mask,\n","                past_key_value=cross_attn_past_key_value,\n","            )\n","            hidden_states = self.dropout(hidden_states, training=training)\n","            hidden_states = residual + hidden_states\n","            if not self.normalize_before:\n","                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n","\n","            # add cross-attn to positions 3,4 of present_key_value tuple\n","            present_key_value = present_key_value + cross_attn_present_key_value\n","\n","        # Fully Connected\n","        residual = hidden_states\n","        if self.normalize_before:\n","            hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.activation_fn(self.fc1(hidden_states))\n","        hidden_states = self.activation_dropout(hidden_states, training=training)\n","        hidden_states = self.fc2(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","\n","        if not self.normalize_before:\n","            hidden_states = self.final_layer_norm(hidden_states)\n","\n","        return (\n","            hidden_states,\n","            self_attn_weights,\n","            present_key_value,\n","        )\n","@keras_serializable\n","class TFBartEncoder(tf.keras.layers.Layer):\n","    config_class = BartConfig\n","    \"\"\"\n","    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n","    :class:`TFBartEncoderLayer`.\n","\n","    Args:\n","        config: BartConfig\n","    \"\"\"\n","\n","    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.config = config\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.layerdrop = config.encoder_layerdrop\n","        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n","        self.padding_idx = config.pad_token_id\n","        self.max_source_positions = config.max_position_embeddings\n","\n","        self.embed_tokens = embed_tokens\n","        if config.static_position_embeddings:\n","            self.embed_positions = TFBartSinusoidalPositionalEmbedding(\n","                config.max_position_embeddings,\n","                config.d_model,\n","                name=\"embed_positions\",\n","            )\n","        else:\n","            self.embed_positions = TFBartLearnedPositionalEmbedding(\n","                config.max_position_embeddings,\n","                config.d_model,\n","                self.padding_idx,\n","                config.extra_pos_embeddings,\n","                name=\"embed_positions\",\n","            )\n","        self.layers = [TFBartEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n","        self.layernorm_embedding = (\n","            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n","            if config.normalize_embedding\n","            else tf.keras.layers.Layer()\n","        )\n","        self.layer_norm = (\n","            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n","            if config.add_final_layer_norm\n","            else None\n","        )\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        inputs_embeds=None,\n","        attention_mask=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Args:\n","            input_ids (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`):\n","                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n","                provide it.\n","\n","                Indices can be obtained using :class:`~transformers.BartTokenizer`. See\n","                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n","                for details.\n","\n","                `What are input IDs? <../glossary.html#input-ids>`__\n","            attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                `What are attention masks? <../glossary.html#attention-mask>`__\n","            inputs_embeds (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n","                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n","                into associated vectors than the model's internal embedding lookup matrix.\n","            output_attentions (:obj:`bool`, `optional`):\n","                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n","                returned tensors for more detail.\n","            output_hidden_states (:obj:`bool`, `optional`):\n","                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n","                for more detail.\n","            return_dict (:obj:`bool`, `optional`):\n","                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n","        \"\"\"\n","        inputs = input_processing(\n","            func=self.call,\n","            config=self.config,\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","            kwargs_call=kwargs,\n","        )\n","\n","        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif inputs[\"input_ids\"] is not None:\n","            input_shape = shape_list(inputs[\"input_ids\"])\n","        elif inputs[\"inputs_embeds\"] is not None:\n","            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if inputs[\"inputs_embeds\"] is None:\n","            inputs_embeds = self.embed_tokens(inputs[\"input_ids\"])\n","        else:\n","            inputs_embeds = inputs[\"inputs_embeds\"]\n","\n","        inputs_embeds = inputs_embeds * self.embed_scale\n","\n","        embed_pos = self.embed_positions(input_shape)\n","        hidden_states = inputs_embeds + embed_pos\n","        hidden_states = self.layernorm_embedding(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n","\n","        # check attention mask and invert\n","        if inputs[\"attention_mask\"] is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            attention_mask = _expand_mask(inputs[\"attention_mask\"])\n","        else:\n","            attention_mask = None\n","\n","        encoder_states = () if inputs[\"output_hidden_states\"] else None\n","        all_attentions = () if inputs[\"output_attentions\"] else None\n","\n","        # encoder layers\n","        for encoder_layer in self.layers:\n","\n","            if inputs[\"output_hidden_states\"]:\n","                encoder_states = encoder_states + (hidden_states,)\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            dropout_probability = random.uniform(0, 1)\n","            if inputs[\"training\"] and (dropout_probability < self.layerdrop):  # skip the layer\n","                continue\n","\n","            hidden_states, attn = encoder_layer(hidden_states, attention_mask)\n","\n","            if inputs[\"output_attentions\"]:\n","                all_attentions += (attn,)\n","        if self.layer_norm:\n","            hidden_states = self.layer_norm(hidden_states)\n","        if inputs[\"output_hidden_states\"]:\n","            encoder_states = encoder_states + (hidden_states,)\n","\n","        if not inputs[\"return_dict\"]:\n","            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n","        return TFBaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n","        )\n","\n","\n","@keras_serializable\n","class TFBartDecoder(tf.keras.layers.Layer):\n","    config_class = BartConfig\n","    \"\"\"\n","    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`TFBartDecoderLayer`\n","\n","    Args:\n","        config: BartConfig\n","        embed_tokens: output embedding\n","    \"\"\"\n","\n","    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.config = config\n","        self.padding_idx = config.pad_token_id\n","        self.embed_tokens = embed_tokens\n","        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n","        self.layerdrop = config.decoder_layerdrop\n","        if config.static_position_embeddings:\n","            self.embed_positions = TFBartSinusoidalPositionalEmbedding(\n","                config.max_position_embeddings,\n","                config.d_model,\n","                name=\"embed_positions\",\n","            )\n","        else:\n","            self.embed_positions = TFBartLearnedPositionalEmbedding(\n","                config.max_position_embeddings,\n","                config.d_model,\n","                self.padding_idx,\n","                config.extra_pos_embeddings,\n","                name=\"embed_positions\",\n","            )\n","        self.layers = [TFBartDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n","        self.layernorm_embedding = (\n","            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n","            if config.normalize_embedding\n","            else tf.keras.layers.Layer()\n","        )\n","        self.layer_norm = (\n","            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n","            if config.add_final_layer_norm\n","            else None\n","        )\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        inputs_embeds=None,\n","        attention_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ):\n","        r\"\"\"\n","        Args:\n","            input_ids (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`):\n","                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n","                provide it.\n","\n","                Indices can be obtained using :class:`~transformers.BartTokenizer`. See\n","                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n","                for details.\n","\n","                `What are input IDs? <../glossary.html#input-ids>`__\n","            attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                `What are attention masks? <../glossary.html#attention-mask>`__\n","            encoder_hidden_states (:obj:`tf.Tensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n","                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n","                of the decoder.\n","            encoder_attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n","                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n","                selected in ``[0, 1]``:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                `What are attention masks? <../glossary.html#attention-mask>`__\n","            past_key_values (:obj:`Tuple[Tuple[tf.Tensor]]` of length :obj:`config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n","                Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n","                decoding.\n","\n","                If :obj:`past_key_values` are used, the user can optionally input only the last\n","                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n","                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n","                sequence_length)`.\n","            inputs_embeds (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n","                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n","                into associated vectors than the model's internal embedding lookup matrix.\n","            output_attentions (:obj:`bool`, `optional`):\n","                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n","                returned tensors for more detail.\n","            output_hidden_states (:obj:`bool`, `optional`):\n","                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n","                for more detail.\n","            return_dict (:obj:`bool`, `optional`):\n","                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n","        \"\"\"\n","        inputs = input_processing(\n","            func=self.call,\n","            config=self.config,\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","            kwargs_call=kwargs,\n","        )\n","\n","        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n","            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n","        elif inputs[\"input_ids\"] is not None:\n","            input_shape = shape_list(inputs[\"input_ids\"])\n","        elif inputs[\"inputs_embeds\"] is not None:\n","            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n","\n","        past_key_values_length = (\n","            inputs[\"past_key_values\"][0][0].shape[2] if inputs[\"past_key_values\"] is not None else 0\n","        )\n","\n","        # embed positions\n","        positions = self.embed_positions(input_shape, past_key_values_length)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.embed_tokens(inputs[\"input_ids\"])\n","        else:\n","            inputs_embeds = inputs[\"inputs_embeds\"]\n","\n","        hidden_states = inputs_embeds * self.embed_scale\n","\n","        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","        combined_attention_mask = None\n","        if input_shape[-1] > 1:\n","            combined_attention_mask = _make_causal_mask(input_shape, past_key_values_length=past_key_values_length)\n","\n","        if inputs[\"attention_mask\"] is None and inputs[\"input_ids\"] is not None and input_shape[-1] > 1:\n","            attention_mask = tf.cast(\n","                tf.math.not_equal(inputs[\"input_ids\"], self.config.pad_token_id), inputs[\"input_ids\"].dtype\n","            )\n","        else:\n","            attention_mask = tf.ones(input_shape, dtype=tf.int32)\n","\n","        if attention_mask is not None and combined_attention_mask is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            combined_attention_mask = combined_attention_mask + _expand_mask(\n","                attention_mask, past_key_values_length=past_key_values_length\n","            )\n","\n","        encoder_hidden_states = inputs[\"encoder_hidden_states\"]\n","        if encoder_hidden_states is not None and inputs[\"encoder_attention_mask\"] is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            encoder_attention_mask = _expand_mask(inputs[\"encoder_attention_mask\"], tgt_len=input_shape[-1])\n","\n","        if self.do_blenderbot_90_layernorm:\n","            hidden_states = self.layernorm_embedding(hidden_states) + positions\n","        else:\n","            hidden_states = self.layernorm_embedding(hidden_states + positions)\n","        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n","\n","        # decoder layers\n","        all_hidden_states = ()\n","        all_self_attns = ()\n","        present_key_values = ()\n","        for idx, decoder_layer in enumerate(self.layers):\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            if inputs[\"output_hidden_states\"]:\n","                all_hidden_states += (hidden_states,)\n","            dropout_probability = random.uniform(0, 1)\n","\n","            if inputs[\"training\"] and (dropout_probability < self.layerdrop):\n","                continue\n","\n","            past_key_value = inputs[\"past_key_values\"][idx] if inputs[\"past_key_values\"] is not None else None\n","\n","            hidden_states, layer_self_attn, present_key_value = decoder_layer(\n","                hidden_states,\n","                attention_mask=combined_attention_mask,\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_attention_mask,\n","                past_key_value=past_key_value,\n","            )\n","\n","            if inputs[\"use_cache\"]:\n","                present_key_values += (present_key_value,)\n","\n","            if inputs[\"output_attentions\"]:\n","                all_self_attns += (layer_self_attn,)\n","\n","        if self.layer_norm is not None:  # same as if config.add_final_layer_norm\n","            hidden_states = self.layer_norm(hidden_states)\n","\n","        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n","        if inputs[\"output_hidden_states\"]:\n","            all_hidden_states += (hidden_states,)\n","        else:\n","            all_hidden_states = None\n","\n","        all_self_attns = list(all_self_attns) if inputs[\"output_attentions\"] else None\n","\n","        present_key_values = (encoder_hidden_states, present_key_values) if inputs[\"use_cache\"] else None\n","\n","        if not inputs[\"return_dict\"]:\n","            return hidden_states, present_key_values, all_hidden_states, all_self_attns\n","        else:\n","            return TFBaseModelOutputWithPast(\n","                last_hidden_state=hidden_states,\n","                past_key_values=present_key_values,\n","                hidden_states=all_hidden_states,\n","                attentions=all_self_attns,\n","            )\n","\n","\n","[DOCS]@add_start_docstrings(\n","    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n","    BART_START_DOCSTRING,\n",")\n","@keras_serializable\n","class TFBartModel(TFBartPretrainedModel):\n","    base_model_prefix = \"model\"\n","\n","    def __init__(self, config: BartConfig, *inputs, **kwargs):\n","        super().__init__(config, *inputs, **kwargs)\n","        self.shared = TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name=\"model.shared\")\n","\n","        with tf.compat.v1.variable_scope(\"model.shared\") as shared_abs_scope_name:\n","            pass\n","\n","        # Wraps layer to avoid problems with weight restoring and ensuring we're in the correct TF scope.\n","        embed_tokens = TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)\n","        embed_tokens.vocab_size = self.shared.vocab_size\n","        embed_tokens.hidden_size = self.shared.hidden_size\n","\n","        self.encoder = TFBartEncoder(config, embed_tokens, name=\"encoder\")\n","        self.decoder = TFBartDecoder(config, embed_tokens, name=\"decoder\")\n","\n","    def get_decoder(self):\n","        return self.decoder\n","\n","[DOCS]    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n","    @add_code_sample_docstrings(\n","        tokenizer_class=_TOKENIZER_FOR_DOC,\n","        checkpoint=\"facebook/bart-large\",\n","        output_type=TFSeq2SeqModelOutput,\n","        config_class=_CONFIG_FOR_DOC,\n","    )\n","    def call(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n","        past_key_values=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs\n","    ):\n","        inputs = input_processing(\n","            func=self.call,\n","            config=self.config,\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","            kwargs_call=kwargs,\n","        )\n","\n","        if inputs[\"decoder_input_ids\"] is None and inputs[\"decoder_inputs_embeds\"] is None:\n","            inputs[\"use_cache\"] = False\n","\n","        inputs[\"output_hidden_states\"] = (\n","            inputs[\"output_hidden_states\"]\n","            if inputs[\"output_hidden_states\"] is not None\n","            else self.config.output_hidden_states\n","        )\n","\n","        if inputs[\"decoder_input_ids\"] is None and inputs[\"input_ids\"] is not None:\n","            inputs[\"decoder_input_ids\"] = shift_tokens_right(\n","                inputs[\"input_ids\"], self.config.pad_token_id, self.config.eos_token_id\n","            )\n","\n","        if inputs[\"encoder_outputs\"] is None:\n","            inputs[\"encoder_outputs\"] = self.encoder(\n","                input_ids=inputs[\"input_ids\"],\n","                attention_mask=inputs[\"attention_mask\"],\n","                inputs_embeds=inputs[\"inputs_embeds\"],\n","                output_attentions=inputs[\"output_attentions\"],\n","                output_hidden_states=inputs[\"output_hidden_states\"],\n","                return_dict=inputs[\"return_dict\"],\n","                training=inputs[\"training\"],\n","            )\n","        # If the user passed a tuple for encoder_outputs, we wrap it in a TFBaseModelOutput when return_dict=True\n","        elif inputs[\"return_dict\"] and not isinstance(inputs[\"encoder_outputs\"], TFBaseModelOutput):\n","            inputs[\"encoder_outputs\"] = TFBaseModelOutput(\n","                last_hidden_state=inputs[\"encoder_outputs\"][0],\n","                hidden_states=inputs[\"encoder_outputs\"][1] if len(inputs[\"encoder_outputs\"]) > 1 else None,\n","                attentions=inputs[\"encoder_outputs\"][2] if len(inputs[\"encoder_outputs\"]) > 2 else None,\n","            )\n","        # If the user passed a TFBaseModelOutput for encoder_outputs, we wrap it in a tuple when return_dict=False\n","        elif not inputs[\"return_dict\"] and not isinstance(inputs[\"encoder_outputs\"], tuple):\n","            inputs[\"encoder_outputs\"] = inputs[\"encoder_outputs\"].to_tuple()\n","\n","        decoder_outputs = self.decoder(\n","            inputs[\"decoder_input_ids\"],\n","            attention_mask=decoder_attention_mask,\n","            encoder_hidden_states=inputs[\"encoder_outputs\"][0],\n","            encoder_attention_mask=inputs[\"attention_mask\"],\n","            past_key_values=inputs[\"past_key_values\"],\n","            inputs_embeds=inputs[\"decoder_inputs_embeds\"],\n","            use_cache=inputs[\"use_cache\"],\n","            output_attentions=inputs[\"output_attentions\"],\n","            output_hidden_states=inputs[\"output_hidden_states\"],\n","            return_dict=inputs[\"return_dict\"],\n","            training=inputs[\"training\"],\n","        )\n","\n","        if not inputs[\"return_dict\"]:\n","            return decoder_outputs + inputs[\"encoder_outputs\"]\n","\n","        return TFSeq2SeqModelOutput(\n","            last_hidden_state=decoder_outputs.last_hidden_state,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            encoder_last_hidden_state=inputs[\"encoder_outputs\"].last_hidden_state,\n","            encoder_hidden_states=inputs[\"encoder_outputs\"].hidden_states,\n","            encoder_attentions=inputs[\"encoder_outputs\"].attentions,\n","        )\n","\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, value):\n","        self.shared = value\n","\n","    def get_output_embeddings(self):\n","        return self.shared"],"metadata":{"id":"fFPESi-nieBp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# ALBERT(A Lite BERT for Self-supervised Learning of Language Representations)\n","---\n","\n","BERT와 같은 pretrain 모델들이 이제 NLP의 대세라는 것은 잘 알았습니다. 그러나 이런 모델들은 개인의 장비에서 학습을 시키는 데에 조금 무리가 있습니다. TPU가 있다면 모를까요.\n","\n","ALBERT는 '성능은 유지하면서 메모리는 적게 쓰는 좀 더 가벼운 BERT를 만들 수 없을까?'라는 질문에서부터 출발합니다.\n","\n"],"metadata":{"id":"nOwTHKJqjHIt"}},{"cell_type":"markdown","source":["---\n","### ALBERT의 구조\n","---"],"metadata":{"id":"b5oo0MxNkJXO"}},{"cell_type":"markdown","source":["ALBERT의 기본 구조는 BERT를 따릅니다. 트랜스포머의 encoder를 기반으로 하며, activation function(활성화 함수)으로는 GELU를 사용하고 있죠.\n","\n","더 가벼운 BERT를 만들기 위해 ALBERT는 아래의 세 가지 아이디어를 적용합니다.\n","\n","# 1. Factorized embedding parameterization\n","\n","BERT는 input의 token embedding(E)과 hidden layer(H)의 size가 같습니다.(E≡H) ALBERT는 E를 H보다 작게 만들어서 parameter의 수를 줄입니다.\n","\n","그 이유가 무엇일까요? 왜 토큰 임베딩 사이즈(E)를 줄일까요?\n","\n","저자는 BERT와 같은 모델들의 성공 요인 중 하나를 context를 반영한 임베딩에 있다고 이야기합니다. 이는 위에서 word2vec의 한계를 설명하면서 이야기했던 부분이죠.\n","\n","모델의 입장에서 context를 반영하는 부분은 트랜스포머 layer(hidden layer)를 통과하는 부분들입니다.  \n","token embedding은 context와 무관한, 그저 token을 벡터화한 것에 불과하죠.  \n","따라서, E와 H의 사이즈가 꼭 같을 필요는 없습니다.  \n","오히려 H의 사이즈가 클수록 성능이 높아질 가능성이 있으며, E의 사이즈를 줄여도 성능과는 크게 연관이 없어 보입니다.\n","\n","그러나, 무작정 E나 H의 사이즈를 변경할 수는 없습니다. BERT에서는 E와 H를 같도록(tying) 모델을 설계했기 때문이죠.\n","\n","BERT에서의 임베딩 벡터는 V * H(혹은 E)의 매트릭스였습니다.  \n","성능을 위해서라면 V도 너무 작아서는 안 됩니다. 따라서 V도, H(E)도 커지게 되면 엄청난 수의 parameter들을 가지게 됩니다.  \n","이는 parameter를 줄이는 방법이 아니지요.\n","\n","그렇다면 어떻게 효과적으로 적당한 V를 유지하면서, H는 키우고 E는 줄일 수 있을까요?\n","\n","정답은 matrix factorization(행렬 분해) 에 있습니다.\n","\n","V * H의 매트릭스를 V * E와 E * H의 매트릭스로 나누는 것입니다. 이런 기법은 추천 시스템 등 다양한 분야에서 널리 쓰이고 있는 기법이기도 합니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/26_tMdvsFG.max-800x600.png'>\n","<br>\n","\n","이렇게 행렬 분해를 통해 H의 사이즈는 유지한 채 E의 사이즈를 줄일 수 있습니다.  \n","또한, parameter의 수도 줄일 수 있지요.\n","\n","# 2. Cross-layer parameter sharing\n","\n","parameter를 공유하는 방법은 parameter의 수를 줄이는 방법 중 하나입니다. ALBERT에서도 이 방식을 사용합니다. layer간의 모든 parameter들을 공유하는 것이죠.\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/27_2QMR6MC.max-800x600.png'>\n","<br>\n","\n","그럼 어떤 layer를 어디까지 공유하는 걸까요?\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/28.max-800x600.png'>\n","<br>\n","\n","사실 모델이 parameter를 공유한다는 것은 일장일단(trade-off)이 있습니다.  \n","더 많은 부분을 공유할수록 모델의 parameter의 수는 줄어들지만(경량화) 성능이 떨어질 우려가 있죠. \n","그럼에도 불구하고 ALBERT는 기본적으로 모든 parameter들을 공유하고 있습니다.  \n","마치 RNN이 한 개의 hidden state를 계속 사용하는 것처럼 ALBERT는 transformer block 1개를 이용하여 재사용합니다.\n","\n","# 3. Inter-sentence coherence loss\n","\n","BERT는 Masked-LM(MLM) Next Sentence Prediction(NSP)을 이용하여 model을 pre-train시켰습니다.\n","\n","NSP의 경우 실제 이어지는 두 문장(positive)과, 임의로 뽑은 두 문장(negative)을 구분하는 것을 학습하게 됩니다.  \n","그러나 이렇게 임의로 뽑은 문장은 첫 번째 문장과 두 번째 문장의 topic이 매우 다를 가능성이 있습니다.  \n","다시 말하면, NSP는 두 문장의 연관 관계 보다 두 문장의 topic 차이를 구별하는 것에 가까울 수 있다는 것입니다.\n","  \n","ALBERT는 이런 NSP를 과감하게 삭제하고 이를 Sentence Order Prediction(SOP)로 대체합니다.  \n","SOP는 임의로 문장을 샘플링하는 것이 아니라, 실제 두 문장의 순서를 바꾸어 학습 데이터를 만듭니다.  \n","두 문장의 순서가 원래의 데이터의 순서와 일치한다면 positive, 순서가 원래 데이터와 반대로 되어있다면 negative이 되는 것이죠.  \n","이러한 학습을 통해 모델은 기존의 NSP보다 훨씬 더 복잡한 언어적 사이의 연관성(논문에서는 discourse-level coherence properties라는 표현을 사용합니다)을 이해할 수 있게 되는 것입니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/29.max-800x600.png'>\n","<br>\n","\n","이러한 구조들을 이용하여 ALBERT-xxlarge는 BERT-large의 약 70%의 parameter들을 가지고도 오히려 성능을 개선할 수 있게 해줍니다.  \n","이 밖에도 논문에서는 '같은 시간은 학습한다면?' 등 다양한 실험들을 통해 여러 시사점을 주고 있습니다.\n","\n","그렇다면 ALBERT의 모델 부분을 코드로 한 번 확인해 볼까요? 얼핏 BERT와 유사한 구조의 코드로 보이지만, TFAlbertEmbeddings의 임베딩 방식과 TFAlbertTransformer 안에서 group 단위로 파라미터가 재사용되는 부분을 유심히 살펴봐 주세요.\n"],"metadata":{"id":"yAvGVEnekdLc"}},{"cell_type":"code","source":["class TFAlbertEmbeddings(tf.keras.layers.Layer):\n","    \"\"\"word, position and token_type embeddings\"\"\"\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = config.vocab_size\n","        self.embedding_size = config.embedding_size \n","        self.initializer_range = config.initializer_range\n","        self.max_position_embeddings = config.max_position_embeddings\n","        self.type_vocab_size = config.type_vocab_size\n","        self.layer_norm_eps = config.layer_norm_eps\n","        self.hidden_dropout_prob = config.hidden_dropout_prob\n","\n","        ## Albert에서는 hidden_size가 아닌 embedding_size로 embedding을 함\n","        self.position_embeddings = tf.keras.layers.Embedding(\n","            self.max_position_embeddings,\n","            self.embedding_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"position_embeddings\",\n","        )\n","        self.token_type_embeddings = tf.keras.layers.Embedding(\n","            self.type_vocab_size,\n","            self.embedding_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"token_type_embeddings\",\n","        )\n","\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(self.hidden_dropout_prob)\n","\n","    def build(self, input_shape):\n","        \"\"\"shared word embedding layer \"\"\"\n","        with tf.name_scope(\"word_embeddings\"):\n","            self.word_embeddings = self.add_weight(\n","                \"weight\",\n","                shape=[self.vocab_size, self.embedding_size],\n","                initializer=get_initializer(self.initializer_range),\n","            )\n","        super().build(input_shape)\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        position_ids=None,\n","        token_type_ids=None,\n","        inputs_embeds=None,\n","        mode=\"embedding\",\n","        training=False,\n","    ):\n","        \"\"\"\n","        input의 token embeddings\n","        Args:\n","            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n","            mode: \"embedding\" | \"linear\"\n","        Returns:\n","            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n","\t\t\t\t\t\t\t\t\t\t mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n","        Raises:\n","            ValueError: if mode is not valid.\n","\t\t\t\t\"\"\"\n","\n","        if mode == \"embedding\":\n","            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","        elif mode == \"linear\":\n","            return self._linear(input_ids)\n","        else:\n","            raise ValueError(\"mode {} is not valid.\".format(mode))\n","\n","    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n","        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n","        assert not (input_ids is None and inputs_embeds is None)\n","\n","        if input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        else:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","\n","        seq_length = input_shape[1]\n","        if position_ids is None:\n","            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings, training=training)\n","        return embeddings\n","\n","    def _linear(self, inputs):\n","        \"\"\"\n"," \t\t\t  linear layer를 통해서 input의 logit을 계산\n","        Args:\n","            inputs: float32 tensor (shape [batch_size, length, embedding_size])\n","        Returns:\n","            float32 tensor (shape [batch_size, length, vocab_size])\n","        \"\"\"\n","        batch_size = shape_list(inputs)[0]\n","        length = shape_list(inputs)[1]\n","        x = tf.reshape(inputs, [-1, self.embedding_size])\n","        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n","        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n","\n","\n","class TFAlbertSelfOutput(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, input_tensor, training=False):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class TFAlbertAttention(tf.keras.layers.Layer):\n","    \"\"\" dropouts and layer norm을 포함한 attention layer \"\"\"\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.hidden_size = config.hidden_size\n","        self.output_attentions = config.output_attentions\n","        self.num_attention_heads = config.num_attention_heads\n","        assert config.hidden_size % config.num_attention_heads == 0\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","        self.query = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n","        )\n","        self.key = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n","        )\n","        self.value = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n","        )\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.pruned_heads = set()\n","        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n","        self.attention_dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n","        self.output_dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def transpose_for_scores(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n","        batch_size = shape_list(input_tensor)[0]\n","        mixed_query_layer = self.query(input_tensor)\n","        mixed_key_layer = self.key(input_tensor)\n","        mixed_value_layer = self.value(input_tensor)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n","        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n","        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n","\n","        # \"query\"와 \"key\"의 dot product : raw attention scores\n","        # (batch size, num_heads, seq_len_q, seq_len_k)\n","        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n","        # scale attention_scores\n","        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)\n","        attention_scores = attention_scores / tf.math.sqrt(dk)\n","\n","        if attention_mask is not None:\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n","\n","        attention_probs = self.attention_dropout(attention_probs, training=training)\n","\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = tf.matmul(attention_probs, value_layer)\n","\n","        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n","        context_layer = tf.reshape(\n","            context_layer, (batch_size, -1, self.all_head_size)\n","        )  # (batch_size, seq_len_q, all_head_size)\n","\n","        self_outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        hidden_states = self_outputs[0]\n","\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.output_dropout(hidden_states, training=training)\n","        attention_output = self.LayerNorm(hidden_states + input_tensor)\n","\n","        outputs = (attention_output,) + self_outputs[1:]\n","\n","        return outputs\n","\n","\n","class TFAlbertLayer(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.attention = TFAlbertAttention(config, name=\"attention\")\n","\n","        self.ffn = tf.keras.layers.Dense(\n","            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n","        )\n","\n","        if isinstance(config.hidden_act, str):\n","            self.activation = get_tf_activation(config.hidden_act)\n","        else:\n","            self.activation = config.hidden_act\n","\n","        self.ffn_output = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n","        )\n","        self.full_layer_layer_norm = tf.keras.layers.LayerNormalization(\n","            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n","        attention_outputs = self.attention(\n","            hidden_states, attention_mask, head_mask, output_attentions, training=training\n","        )\n","        ffn_output = self.ffn(attention_outputs[0])\n","        ffn_output = self.activation(ffn_output)\n","        ffn_output = self.ffn_output(ffn_output)\n","        ffn_output = self.dropout(ffn_output, training=training)\n","\n","        hidden_states = self.full_layer_layer_norm(ffn_output + attention_outputs[0])\n","\n","        outputs = (hidden_states,) + attention_outputs[1:]\n","        return outputs\n","\n","\n","class TFAlbertLayerGroup(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.albert_layers = [\n","            TFAlbertLayer(config, name=\"albert_layers_._{}\".format(i)) for i in range(config.inner_group_num)\n","        ]\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, training=False):\n","        layer_hidden_states = ()\n","        layer_attentions = ()\n","\n","        for layer_index, albert_layer in enumerate(self.albert_layers):\n","            layer_output = albert_layer(\n","                hidden_states, attention_mask, head_mask[layer_index], output_attentions, training=training\n","            )\n","            hidden_states = layer_output[0]\n","\n","            if output_attentions:\n","                layer_attentions = layer_attentions + (layer_output[1],)\n","\n","            if output_hidden_states:\n","                layer_hidden_states = layer_hidden_states + (hidden_states,)\n","\n","        outputs = (hidden_states,)\n","        if output_hidden_states:\n","            outputs = outputs + (layer_hidden_states,)\n","        if output_attentions:\n","            outputs = outputs + (layer_attentions,)\n","        # last-layer hidden state, (layer hidden states), (layer attentions)\n","        return outputs\n","\n","\n","class TFAlbertTransformer(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.num_hidden_layers = config.num_hidden_layers\n","        self.num_hidden_groups = config.num_hidden_groups\n","        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(\n","            config.hidden_size,\n","            kernel_initializer=get_initializer(config.initializer_range),\n","            name=\"embedding_hidden_mapping_in\",\n","        )\n","        self.albert_layer_groups = [\n","            TFAlbertLayerGroup(config, name=\"albert_layer_groups_._{}\".format(i))\n","            for i in range(config.num_hidden_groups)\n","        ]\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask,\n","        head_mask,\n","        output_attentions,\n","        output_hidden_states,\n","        return_dict,\n","        training=False,\n","    ):\n","        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n","        all_attentions = () if output_attentions else None\n","        all_hidden_states = (hidden_states,) if output_hidden_states else None\n","\n","        for i in range(self.num_hidden_layers):\n","            # Number of layers in a hidden group\n","            layers_per_group = int(self.num_hidden_layers / self.num_hidden_groups)\n","\n","            # Index of the hidden group\n","            group_idx = int(i / (self.num_hidden_layers / self.num_hidden_groups))\n","\n","            layer_group_output = self.albert_layer_groups[group_idx](\n","                hidden_states,\n","                attention_mask,\n","                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n","                output_attentions,\n","                output_hidden_states,\n","                training=training,\n","            )\n","            hidden_states = layer_group_output[0]\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + layer_group_output[-1]\n","\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n","        return TFBaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n","        )\n","\n","@keras_serializable\n","class TFAlbertMainLayer(tf.keras.layers.Layer):\n","\"\"\"\n","모델의 전체구조\n","\"\"\"\n","    config_class = AlbertConfig\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.num_hidden_layers = config.num_hidden_layers\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.return_dict = config.use_return_dict\n","\n","        self.embeddings = TFAlbertEmbeddings(config, name=\"embeddings\")\n","        self.encoder = TFAlbertTransformer(config, name=\"encoder\")\n","        self.pooler = tf.keras.layers.Dense(\n","            config.hidden_size,\n","            kernel_initializer=get_initializer(config.initializer_range),\n","            activation=\"tanh\",\n","            name=\"pooler\",\n","        )\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","        self.embeddings.vocab_size = value.shape[0]\n","\n","    def _resize_token_embeddings(self, new_num_tokens):\n","        raise NotImplementedError\n","\n","    def _prune_heads(self, heads_to_prune):\n","        raise NotImplementedError\n","\n","    def call(\n","        self,\n","        inputs,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n","            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n","            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n","            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n","            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n","            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n","            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n","            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n","            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n","            position_ids = inputs.get(\"position_ids\", position_ids)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if attention_mask is None:\n","            attention_mask = tf.fill(input_shape, 1)\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        # 3D attention mask 만들기\n","        # Sizes : [batch_size, 1, 1, to_seq_length]\n","        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n","\n","        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","\t\t\t\t# attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n","        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # head_mask가 1.0이면, head를 유지\n","        # attention_probs : shape bsz x n_heads x N x N\n","        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n","        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.num_hidden_layers\n","            # head_mask = tf.constant([0] * self.num_hidden_layers)\n","\n","        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            extended_attention_mask,\n","            head_mask,\n","            output_attentions,\n","            output_hidden_states,\n","            return_dict,\n","            training=training,\n","        )\n","\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output[:, 0])\n","\n","        if not return_dict:\n","            return (\n","                sequence_output,\n","                pooled_output,\n","            ) + encoder_outputs[1:]\n","\n","        return TFBaseModelOutputWithPooling(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )"],"metadata":{"id":"UdrZbZxBjIEu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# T5(Text-to-Text Transfer Transformer)\n","---\n","\n","2020년 2월 발표된 T5는 Text-to-Text Transfer Transformer의 약자입니다.  \n","T가 5개라서 T5라고 불린다고 하니 구글의 작명 실력을 알만하죠? 😆\n","\n","T5는 Transformer를 기반으로 만들어진 아키텍처로 text-to-text 방법을 사용합니다.  \n","BERT와 차이점을 몇 개 볼 수 있는데요,  \n","우선 causal decoder를 bidirectional architecture에 추가하였고, pre-training tasks를 fill-in-the-blank cloze task로 대체했다는 것입니다.  \n","혹시 fill-in-the-blank cloze task가 무엇인지 궁금하신가요?  \n","여러분도 이 문제를 영어 시간이나 국어 시간에 많이 접했을 것입니다.  \n","바로 빈칸 채우기이죠.  \n","아래와 같이 빈칸이 주어지면 그 빈칸을 채우는 문제입니다.\n","\n","    Lions are the ___________________ living animal in the world.\n","\n","인간인 우리도 풀기 힘든 문제일 수도 있는데, T5는 이 문제를 정말 잘 풀었다고 합니다.  \n","그럼 T5에 대해 본격적으로 알아볼까요?"],"metadata":{"id":"FkOV_Ky6lhDs"}},{"cell_type":"markdown","source":["---\n","### C4\n","---"],"metadata":{"id":"EW1zunhAl114"}},{"cell_type":"markdown","source":["사실 T5를 소개한 논문 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'은 어떤 전이학습이 더 효과적인지 결정하기 위한 대규모의 서베이 논문입니다.  \n","서베이에서 얻은 인사이트를 통해 T5라는 모델을 만들었죠.  \n","이 논문에서는 모델만 소개한 것이 아니라 Colossal Clean Crawled Corpus(C4)라 불리는 open-source pre-training dataset을 소개하기도 하였습니다.  \n","T5와 C4를 가지고 fine-tuning을 하면 다양한 downstream task에 적용할 수 있다고 하네요.\n","  \n","전이학습을 할 때 사용되는 데이터셋은 굉장히 중요합니다.  \n","데이터셋에 따라 사전학습된 모델의 성능이 천차만별일 수도 있습니다.  \n","그러나 기존의 데이터셋은 고품질, 다양성, 많은 양이라는 좋은 데이터셋의 조건을 만족하지 못했어요.  \n","그래서 T5를 만든 저자들은 3가지 조건을 만족하는 데이터셋인 C4를 만들었습니다.\n","\n","C4는 Wikipedia보다 2배 많은 양이고, 중복된 데이터, 불완전한 문장, 혐오 표현이나 의미 없는 내용은 제거하였습니다.  \n","그래서 downstream task에서 좋은 결과를 얻을 수 있고, overfitting 없이 모델 사이즈를 증가시킬 수 있습니다."],"metadata":{"id":"cZqbHscQl_bK"}},{"cell_type":"markdown","source":["---\n","### Shared Text-To-Text Framework\n","---"],"metadata":{"id":"jRsXoUaNmJy9"}},{"cell_type":"markdown","source":["T5에서는 모든 NLP task를 입력과 출력이 모두 text string인 text-to-text format으로 재구성하였습니다.  \n","아래의 이미지는 text-to-text framework를 나타낸 것으로, text를 입력으로 하여 모델에 넣으면 결과가 text로 나오는 것을 보여줍니다.  \n","참고로 녹색은 번역, 빨간색은 CoLA 태스크(문법 적합성), 노란색은 문장 유사성, 파란색은 문서 요약입니다.\n","\n","이런 과정을 통해서 어떤 NLP task에서든 모델, 학습 과정, 디코딩 과정, 손실함수, 하이퍼파라미터를 동일하게 사용할 수 있죠.\n","  \n","<img src='https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif'>\n","<br>\n","\n","모두 동일한 것을 사용한다면 T5는 다른 NLP 문제를 어떻게 구분할 수 있을까요? 이에 대한 해답은 입력 형식에 있습니다.\n","\n","    예1 영-독 번역\n","    입력 - “translate English to German: That is good.”\n","    출력 - “Das ist gut.”\n","\n","    예2 MNLI benchmark\n","    입력 - “mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.”\n","    출력 - “entailment”\n","위의 예에서와 같이 task-specific(text) prefix를 입력 시퀀스 앞에 추가해 모델에 넣어주는 겁니다."],"metadata":{"id":"i2KuhMCsmZS6"}},{"cell_type":"markdown","source":["---\n","### Modified MLM\n","---"],"metadata":{"id":"EYX7_rKFmmWB"}},{"cell_type":"markdown","source":["T5은 입력에 corrupted token을 넣어 더 좋은 성능을 낼 수 있도록 하였습니다.  \n","이를 denoising objectives라고 하는데요,  \n","이는 T5에서만 사용한 것이 아니라 BART 등 다른 모델에서도 흔히 사용하는 방법이었죠?\n","  \n","T5는 타겟을 랜덤으로 샘플링하고, 입력 시퀀스의 15%의 token을 single sentinel token(<X>, <Y>)로 대체하였습니다.  \n","각 sentinel token은 어떤 wordpiece에도 대응되지 않는 특별한 토큰으로, 시퀀스에 유일한 토큰 ID로 지정되어 voca에 추가됩니다.  \n","또한 연속된 단어는 하나의 sentinel token로 여겨집니다.\n","\n","타켓 토큰은 입력에서 사용된 sentinel token과 final sentinel token(<Z>)입니다.  \n","이를 시각화하면 아래와 같습니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-10-18_15-18-27.png'>\n"],"metadata":{"id":"vO0f0zmxmwmB"}},{"cell_type":"markdown","source":["---\n","### 모델 아키텍처\n","---"],"metadata":{"id":"xp9wFjqym6Lv"}},{"cell_type":"markdown","source":["T5와 다른 트랜스포머 계열 모델과 다른 점 중 하나는 어텐션 매커니즘에 의해 사용되는 mask가 다르다는 것입니다.  \n","아래 그림은 attention mask 패턴을 나타낸 것입니다.\n","\n","가장 왼쪽 그림은 모든 출력 time step에서 모든 입력을 고려하는 self-attention 메커니즘으로, encoder-decoder Transformer나 BERT에서 사용하는 마스킹 방법이고,  \n","가운데 그림은 i번째 출력 요소가 미래의 입력에 의존하지 못하도록 하는 마스킹 방법입니다.  \n","T5에서는 앞의 두 방법이 아니라 fully-visible 마스킹을 입력 시퀀스의 일부에 사용하는 causal masking with prefix라는 마스킹 방법을 사용합니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-10-18_15-32-03.png'>\n","<br>\n","\n","다른 모델과 T5의 또다른 차이점은 Transformer 아키텍처에요.  \n","아래의 그림에서 block은 시퀀스이고, 선은 attention입니다. 진한 선은 fully-visible masking이고,  \n","연한 선은 causal masking을 나타내죠. T5는 오른쪽과 같이 언어모델에 prefix를 추가해 입력에 fully-visible masking할 수 있도록 하였습니다.  \n","즉 encoder에는 양방향 attention을, decoder(generation)에서는 단방향 attention을 사용하는 거죠.\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-10-18_15-45-21.png'> \n","\n"],"metadata":{"id":"PAuvJP59m85x"}},{"cell_type":"markdown","source":["---\n","### 새로운 task\n","---"],"metadata":{"id":"Hglucxc2neEI"}},{"cell_type":"markdown","source":["T5 모델은 2가지 새로운 task를 학습하였습니다.  \n","그 중 하나는 Closed-Book Question Answering이라는 것인데요, 이 \n","는 어떤 외부 지식에도 접근하지 않고 사전 학습 중 얻은 파라미터에 저장된 정보만 사용해서 질문에 답하는 것입니다.\n","\n","아래 그림에서 \\<M>이라는 부분을 T5가 내재된 지식에 의존해서 답을 하게 하였습니다. 그리고 그 결과는...??  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-10-18_12-10-25.png'>\n","<br>\n","\n","정말 놀랍게도 T5는 이 문제를 잘 해결했습니다. TriviaQA, WebQuestions, Natural Question에 각각 50.1%, 37.4%, 34.5% 정도의 답을 정확한 텍스트로 출력했어요.\n","\n","T5 연구팀이 직접 이 문제를 가지고 T5와 정면도전했지만 졌다고 하네요.  \n","😂 T5 연구팀과 T5의 흥미진진한 대결을 한번 살펴보시죠!\n","\n","<img src='https://1.bp.blogspot.com/-SllNg6Q4DEE/Xk7ZRCtzXaI/AAAAAAAAFVY/PaaM-FEgyFIdSn7VeT_XhvG9PXQdSC3_wCLcBGAsYHQ/s640/t5-trivia-lrg.gif'>\n","<br>\n","\n","T5가 학습한 새로운 task 중 또다른 하나는 fill-in-the blank task였습니다.  \n","바로 빈칸 채우기인데요, 특이하게도 빈칸에 들어갈 단어의 수가 달라지면 결과도 다르게 나옵니다. 신기하지 않나요?\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-04_17-52-33.max-800x600.png'>\n","<br>\n","\n","T5가 어떻게 위의 두 문제를 풀었는지 궁금하다면 아래의 링크를 참고하세요.\n","\n","[Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n"],"metadata":{"id":"bPYv4SFunpjk"}},{"cell_type":"code","source":["class TFT5LayerNorm(tf.keras.layers.Layer):\n","    def __init__(self, epsilon=1e-6, **kwargs):\n","        \"\"\"\n","        Construct a layernorm module in the T5 style No bias and no subtraction of mean.\n","        \"\"\"\n","        super().__init__(**kwargs)\n","        self.variance_epsilon = epsilon\n","\n","    def build(self, input_shape):\n","        \"\"\"Build shared word embedding layer\"\"\"\n","        self.weight = self.add_weight(\"weight\", shape=(input_shape[-1],), initializer=\"ones\")\n","        super().build(input_shape)\n","\n","    def call(self, hidden_states):\n","        variance = tf.math.reduce_mean(tf.math.square(hidden_states), axis=-1, keepdims=True)\n","        hidden_states = hidden_states * tf.math.rsqrt(variance + self.variance_epsilon)\n","        return self.weight * hidden_states\n","\n","\n","class TFT5DenseReluDense(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.wi = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi\")\n","        self.wo = tf.keras.layers.Dense(config.d_model, use_bias=False, name=\"wo\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","        self.act = tf.keras.activations.relu\n","\n","    def call(self, hidden_states, training=False):\n","        hidden_states = self.wi(hidden_states)\n","        hidden_states = self.act(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.wo(hidden_states)\n","        return hidden_states\n","\n","\n","class TFT5GatedGeluDense(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.wi_0 = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi_0\")\n","        self.wi_1 = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi_1\")\n","        self.wo = tf.keras.layers.Dense(config.d_model, use_bias=False, name=\"wo\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","        self.act = get_tf_activation(\"gelu_new\")\n","\n","    def call(self, hidden_states, training=False):\n","        hidden_gelu = self.act(self.wi_0(hidden_states))\n","        hidden_linear = self.wi_1(hidden_states)\n","        hidden_states = hidden_gelu * hidden_linear\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.wo(hidden_states)\n","        return hidden_states\n","\n","\n","class TFT5LayerFF(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        if config.feed_forward_proj == \"relu\":\n","            self.DenseReluDense = TFT5DenseReluDense(config, name=\"DenseReluDense\")\n","        elif config.feed_forward_proj == \"gated-gelu\":\n","            self.DenseReluDense = TFT5GatedGeluDense(config, name=\"DenseReluDense\")\n","        else:\n","            raise ValueError(\n","                f\"{self.config.feed_forward_proj} is not supported. Choose between `relu` and `gated-gelu`\"\n","            )\n","        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","\n","    def call(self, hidden_states, training=False):\n","        normed_hidden_states = self.layer_norm(hidden_states)\n","        dense_output = self.DenseReluDense(normed_hidden_states, training=training)\n","        hidden_states = hidden_states + self.dropout(dense_output, training=training)\n","        return hidden_states\n","\n","\n","class TFT5Attention(tf.keras.layers.Layer):\n","    NEW_ID = itertools.count()\n","\n","    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.layer_id = next(TFT5Attention.NEW_ID)\n","        self.is_decoder = config.is_decoder\n","        self.use_cache = config.use_cache\n","        self.has_relative_attention_bias = has_relative_attention_bias\n","        self.output_attentions = config.output_attentions\n","\n","        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n","        self.d_model = config.d_model\n","        self.key_value_proj_dim = config.d_kv\n","        self.n_heads = config.num_heads\n","        self.inner_dim = self.n_heads * self.key_value_proj_dim\n","\n","        # Mesh TensorFlow initialization to avoid scaling before softmax\n","        self.q = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"q\")\n","        self.k = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"k\")\n","        self.v = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"v\")\n","        self.o = tf.keras.layers.Dense(self.d_model, use_bias=False, name=\"o\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","\n","        self.pruned_heads = set()\n","\n","    def build(self, input_shape):\n","        if self.has_relative_attention_bias:\n","            with tf.name_scope(\"relative_attention_bias\"):\n","                self.relative_attention_bias = self.add_weight(\n","                    name=\"embeddings\",\n","                    shape=[self.relative_attention_num_buckets, self.n_heads],\n","                )\n","\n","        return super().build(input_shape)\n","\n","    def prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    @staticmethod\n","    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n","        \"\"\"\n","        Adapted from Mesh Tensorflow:\n","        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n","\n","        Translate relative position to a bucket number for relative attention. The relative position is defined as\n","        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n","        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n","        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n","        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n","        This should allow for more graceful generalization to longer sequences than the model has been trained on\n","\n","        Args:\n","            relative_position: an int32 Tensor\n","            bidirectional: a boolean - whether the attention is bidirectional\n","            num_buckets: an integer\n","            max_distance: an integer\n","\n","        Returns:\n","            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n","        \"\"\"\n","        relative_buckets = 0\n","        #        n = -relative_position\n","        if bidirectional:\n","            num_buckets //= 2\n","            relative_buckets += (\n","                tf.cast(tf.math.greater(relative_position, 0), dtype=relative_position.dtype) * num_buckets\n","            )\n","            relative_position = tf.math.abs(relative_position)\n","        else:\n","            relative_position = -tf.math.minimum(relative_position, 0)\n","        # now n is in the range [0, inf)\n","        max_exact = num_buckets // 2\n","        is_small = tf.math.less(relative_position, max_exact)\n","        relative_position_if_large = max_exact + tf.cast(\n","            tf.math.log(relative_position / max_exact)\n","            / math.log(max_distance / max_exact)\n","            * (num_buckets - max_exact),\n","            dtype=relative_position.dtype,\n","        )\n","        relative_position_if_large = tf.math.minimum(relative_position_if_large, num_buckets - 1)\n","        relative_buckets += tf.where(is_small, relative_position, relative_position_if_large)\n","        return relative_buckets\n","\n","    def compute_bias(self, query_length, key_length):\n","        \"\"\"Compute binned relative position bias\"\"\"\n","        context_position = tf.range(query_length)[:, None]\n","        memory_position = tf.range(key_length)[None, :]\n","        relative_position = memory_position - context_position  # shape (query_length, key_length)\n","        relative_position_bucket = self._relative_position_bucket(\n","            relative_position,\n","            bidirectional=(not self.is_decoder),\n","            num_buckets=self.relative_attention_num_buckets,\n","        )\n","        values = tf.gather(\n","            self.relative_attention_bias, relative_position_bucket\n","        )  # shape (query_length, key_length, num_heads)\n","        values = tf.expand_dims(\n","            tf.transpose(values, [2, 0, 1]), axis=0\n","        )  # shape (1, num_heads, query_length, key_length)\n","        return values\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        mask=None,\n","        key_value_states=None,\n","        position_bias=None,\n","        past_key_value=None,\n","        layer_head_mask=None,\n","        query_length=None,\n","        use_cache=False,\n","        training=False,\n","        output_attentions=False,\n","    ):\n","        \"\"\"\n","        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n","        \"\"\"\n","        # Input is (batch_size, query_length, dim)\n","        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n","        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n","        batch_size, seq_length = shape_list(hidden_states)[:2]\n","\n","        real_seq_length = seq_length\n","\n","        if past_key_value is not None:\n","            assert (\n","                len(past_key_value) == 2\n","            ), f\"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states\"\n","            real_seq_length += shape_list(past_key_value[0])[2] if query_length is None else query_length\n","\n","        key_length = real_seq_length if key_value_states is None else shape_list(key_value_states)[1]\n","\n","        def shape(hidden_states):\n","            \"\"\"projection\"\"\"\n","            return tf.transpose(\n","                tf.reshape(hidden_states, (batch_size, -1, self.n_heads, self.key_value_proj_dim)), perm=(0, 2, 1, 3)\n","            )\n","\n","        def unshape(hidden_states):\n","            \"\"\"compute context\"\"\"\n","            return tf.reshape(tf.transpose(hidden_states, perm=(0, 2, 1, 3)), (batch_size, -1, self.inner_dim))\n","\n","        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n","            \"\"\"projects hidden states correctly to key/query states\"\"\"\n","            if key_value_states is None:\n","                # self-attn\n","                # (batch_size, n_heads, seq_length, dim_per_head)\n","                hidden_states = shape(proj_layer(hidden_states))\n","            elif past_key_value is None:\n","                # cross-attn\n","                # (batch_size, n_heads, seq_length, dim_per_head)\n","                hidden_states = shape(proj_layer(key_value_states))\n","\n","            if past_key_value is not None:\n","                if key_value_states is None:\n","                    # self-attn\n","                    # (batch_size, n_heads, key_length, dim_per_head)\n","                    hidden_states = tf.concat([past_key_value, hidden_states], axis=2)\n","                else:\n","                    # cross-attn\n","                    hidden_states = past_key_value\n","            return hidden_states\n","\n","        # get query\n","        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, query_length, dim_per_head)\n","\n","        # get key/value\n","        key_states = project(\n","            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n","        )\n","        value_states = project(\n","            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n","        )\n","\n","        # to cope with keras serialization\n","        if self.is_decoder and use_cache:\n","            present_key_value_state = (key_states, value_states)\n","        else:\n","            present_key_value_state = None\n","\n","        scores = tf.einsum(\n","            \"bnqd,bnkd->bnqk\", query_states, key_states\n","        )  # (batch_size, n_heads, query_length, key_length)\n","\n","        if position_bias is None:\n","            if not self.has_relative_attention_bias:\n","                position_bias = tf.zeros((1, self.n_heads, real_seq_length, key_length))\n","            else:\n","                position_bias = self.compute_bias(real_seq_length, key_length)\n","\n","            # if key and values are already calculated\n","            # we want only the last query position bias\n","            if past_key_value is not None:\n","                position_bias = position_bias[:, :, -seq_length:, :]\n","\n","            if mask is not None:\n","                position_bias = tf.cast(position_bias, dtype=mask.dtype)\n","                position_bias = position_bias + mask  # (batch_size, n_heads, query_length, key_length)\n","\n","        scores += position_bias\n","        weights = tf.nn.softmax(scores, axis=-1)  # (batch_size, n_heads, query_length, key_length)\n","        weights = self.dropout(weights, training=training)  # (batch_size, n_heads, query_length, key_length)\n","\n","        # Mask heads if we want to\n","        if layer_head_mask is not None:\n","            tf.debugging.assert_equal(\n","                shape_list(layer_head_mask),\n","                [self.n_heads],\n","                message=f\"Head mask for a single layer should be of size {(self.n_heads)}, but is {shape_list(layer_head_mask)}\",\n","            )\n","            weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * weights\n","\n","        attn_output = tf.matmul(weights, value_states)  # (batch_size, n_heads, query_length, dim_per_head)\n","\n","        attn_output = self.o(unshape(attn_output))\n","\n","        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n","\n","        if output_attentions:\n","            outputs = outputs + (weights,)\n","\n","        return outputs\n","\n","\n","class TFT5LayerSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.SelfAttention = TFT5Attention(\n","            config,\n","            has_relative_attention_bias=has_relative_attention_bias,\n","            name=\"SelfAttention\",\n","        )\n","        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        position_bias=None,\n","        layer_head_mask=None,\n","        past_key_value=None,\n","        use_cache=False,\n","        output_attentions=False,\n","        training=False,\n","    ):\n","        normed_hidden_states = self.layer_norm(hidden_states)\n","        attention_output = self.SelfAttention(\n","            normed_hidden_states,\n","            mask=attention_mask,\n","            position_bias=position_bias,\n","            layer_head_mask=layer_head_mask,\n","            past_key_value=past_key_value,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            training=training,\n","        )\n","        hidden_states = hidden_states + self.dropout(attention_output[0], training=training)\n","        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","class TFT5LayerCrossAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.EncDecAttention = TFT5Attention(\n","            config,\n","            has_relative_attention_bias=False,\n","            name=\"EncDecAttention\",\n","        )\n","        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        key_value_states,\n","        attention_mask=None,\n","        position_bias=None,\n","        layer_head_mask=None,\n","        past_key_value=None,\n","        query_length=None,\n","        use_cache=False,\n","        output_attentions=False,\n","        training=False,\n","    ):\n","        normed_hidden_states = self.layer_norm(hidden_states)\n","        attention_output = self.EncDecAttention(\n","            normed_hidden_states,\n","            mask=attention_mask,\n","            key_value_states=key_value_states,\n","            position_bias=position_bias,\n","            layer_head_mask=layer_head_mask,\n","            past_key_value=past_key_value,\n","            query_length=query_length,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            training=training,\n","        )\n","        hidden_states = hidden_states + self.dropout(attention_output[0], training=training)\n","        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","class TFT5Block(tf.keras.layers.Layer):\n","    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.is_decoder = config.is_decoder\n","        self.layer = []\n","        self.layer.append(\n","            TFT5LayerSelfAttention(\n","                config,\n","                has_relative_attention_bias=has_relative_attention_bias,\n","                name=\"layer_._0\",\n","            )\n","        )\n","        if self.is_decoder:\n","            self.layer.append(\n","                TFT5LayerCrossAttention(\n","                    config,\n","                    name=\"layer_._1\",\n","                )\n","            )\n","\n","        self.layer.append(TFT5LayerFF(config, name=f\"layer_._{len(self.layer)}\"))\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        position_bias=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        encoder_decoder_position_bias=None,\n","        layer_head_mask=None,\n","        encoder_layer_head_mask=None,\n","        past_key_value=None,\n","        use_cache=False,\n","        output_attentions=False,\n","        training=False,\n","    ):\n","\n","        if past_key_value is not None:\n","            assert self.is_decoder, \"Only decoder can use `past_key_values`\"\n","            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n","\n","            if len(past_key_value) != expected_num_past_key_values:\n","                raise ValueError(\n","                    f\"There should be {expected_num_past_key_values} past states. \"\n","                    f\"{'2 (past / key) for cross attention' if expected_num_past_key_values == 4 else ''}.\"\n","                    f\"Got {len(past_key_value)} past key / value states\"\n","                )\n","\n","            self_attn_past_key_value = past_key_value[:2]\n","            cross_attn_past_key_value = past_key_value[2:]\n","        else:\n","            self_attn_past_key_value, cross_attn_past_key_value = None, None\n","\n","        self_attention_outputs = self.layer[0](\n","            hidden_states,\n","            attention_mask=attention_mask,\n","            position_bias=position_bias,\n","            layer_head_mask=layer_head_mask,\n","            past_key_value=self_attn_past_key_value,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            training=training,\n","        )\n","        hidden_states, present_key_value_state = self_attention_outputs[:2]\n","        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n","\n","        if self.is_decoder and encoder_hidden_states is not None:\n","            # the actual query length is unknown for cross attention\n","            # if using past key value states. Need to inject it here\n","            if present_key_value_state is not None:\n","                query_length = shape_list(present_key_value_state[0])[2]\n","            else:\n","                query_length = None\n","\n","            cross_attention_outputs = self.layer[1](\n","                hidden_states,\n","                key_value_states=encoder_hidden_states,\n","                attention_mask=encoder_attention_mask,\n","                position_bias=encoder_decoder_position_bias,\n","                layer_head_mask=encoder_layer_head_mask,\n","                past_key_value=cross_attn_past_key_value,\n","                query_length=query_length,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","                training=training,\n","            )\n","            hidden_states = cross_attention_outputs[0]\n","            # Combine self attn and cross attn key value states\n","            if present_key_value_state is not None:\n","                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n","\n","            # Keep cross-attention outputs and relative position weights\n","            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n","\n","        # Apply Feed Forward layer\n","        hidden_states = self.layer[-1](hidden_states, training=training)\n","        outputs = (hidden_states,)\n","\n","        # Add attentions if we output them\n","        outputs = outputs + (present_key_value_state,) + attention_outputs\n","        return outputs  # hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","\n","@keras_serializable\n","class TFT5MainLayer(tf.keras.layers.Layer):\n","    config_class = T5Config\n","\n","    def __init__(self, config, embed_tokens=None, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.config = config\n","        self.output_hidden_states = config.output_hidden_states\n","        self.output_attentions = config.output_attentions\n","        self.use_cache = config.use_cache\n","\n","        self.embed_tokens = embed_tokens\n","        self.is_decoder = config.is_decoder\n","\n","        self.config = config\n","        self.num_hidden_layers = config.num_layers\n","\n","        self.block = [\n","            TFT5Block(config, has_relative_attention_bias=bool(i == 0), name=f\"block_._{i}\")\n","            for i in range(config.num_layers)\n","        ]\n","        self.final_layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"final_layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n","\n","    def _prune_heads(self, heads_to_prune):\n","        raise NotImplementedError  # Not implemented yet in the library fr TF 2.0 models\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        inputs_embeds=None,\n","        head_mask=None,\n","        encoder_head_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ) -> Tuple:\n","        inputs = input_processing(\n","            func=self.call,\n","            config=self.config,\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            head_mask=head_mask,\n","            encoder_head_mask=encoder_head_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","            kwargs_call=kwargs,\n","        )\n","\n","        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n","            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n","            raise ValueError(\n","                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n","            )\n","        elif inputs[\"input_ids\"] is not None:\n","            input_shape = shape_list(inputs[\"input_ids\"])\n","            inputs[\"input_ids\"] = tf.reshape(inputs[\"input_ids\"], (-1, input_shape[-1]))\n","        elif inputs[\"inputs_embeds\"] is not None:\n","            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n","        else:\n","            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n","            raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n","\n","        if inputs[\"inputs_embeds\"] is None:\n","            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n","            inputs[\"inputs_embeds\"] = self.embed_tokens(inputs[\"input_ids\"])\n","\n","        batch_size, seq_length = input_shape\n","\n","        # required mask seq length can be calculated via length of past\n","        mask_seq_length = (\n","            shape_list(inputs[\"past_key_values\"][0][0])[2] + seq_length\n","            if inputs[\"past_key_values\"] is not None\n","            else seq_length\n","        )\n","\n","        if inputs[\"attention_mask\"] is None:\n","            inputs[\"attention_mask\"] = tf.fill((batch_size, mask_seq_length), 1)\n","        if (\n","            self.is_decoder\n","            and inputs[\"encoder_attention_mask\"] is None\n","            and inputs[\"encoder_hidden_states\"] is not None\n","        ):\n","            encoder_seq_length = shape_list(inputs[\"encoder_hidden_states\"])[1]\n","            inputs[\"encoder_attention_mask\"] = tf.fill((batch_size, encoder_seq_length), 1)\n","\n","        # initialize past_key_values with `None` if past does not exist\n","        if inputs[\"past_key_values\"] is None:\n","            inputs[\"past_key_values\"] = [None] * len(self.block)\n","\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        inputs[\"attention_mask\"] = tf.cast(inputs[\"attention_mask\"], dtype=inputs[\"inputs_embeds\"].dtype)\n","        num_dims_attention_mask = len(shape_list(inputs[\"attention_mask\"]))\n","        if num_dims_attention_mask == 3:\n","            extended_attention_mask = inputs[\"attention_mask\"][:, None, :, :]\n","        elif num_dims_attention_mask == 2:\n","            # Provided a padding mask of dimensions [batch_size, mask_seq_length]\n","            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n","            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]\n","            if self.is_decoder:\n","                seq_ids = tf.range(mask_seq_length)\n","                causal_mask = tf.less_equal(\n","                    tf.tile(seq_ids[None, None, :], (batch_size, mask_seq_length, 1)),\n","                    seq_ids[None, :, None],\n","                )\n","                causal_mask = tf.cast(causal_mask, dtype=inputs[\"attention_mask\"].dtype)\n","                extended_attention_mask = causal_mask[:, None, :, :] * inputs[\"attention_mask\"][:, None, None, :]\n","                if inputs[\"past_key_values\"][0] is not None:\n","                    extended_attention_mask = extended_attention_mask[:, :, -seq_length:, :]\n","            else:\n","                extended_attention_mask = inputs[\"attention_mask\"][:, None, None, :]\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and  -1e9 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","\n","        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n","        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n","        # extended_attention_mask = tf.math.equal(extended_attention_mask,\n","        #                                         tf.transpose(extended_attention_mask, perm=(-1, -2)))\n","\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n","\n","        if self.is_decoder and inputs[\"encoder_attention_mask\"] is not None:\n","            # If a 2D ou 3D attention mask is provided for the cross-attention\n","            # we need to make broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]\n","            # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","            inputs[\"encoder_attention_mask\"] = tf.cast(\n","                inputs[\"encoder_attention_mask\"], dtype=extended_attention_mask.dtype\n","            )\n","            num_dims_encoder_attention_mask = len(shape_list(inputs[\"encoder_attention_mask\"]))\n","            if num_dims_encoder_attention_mask == 3:\n","                encoder_extended_attention_mask = inputs[\"encoder_attention_mask\"][:, None, :, :]\n","            if num_dims_encoder_attention_mask == 2:\n","                encoder_extended_attention_mask = inputs[\"encoder_attention_mask\"][:, None, None, :]\n","\n","            # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n","            # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n","            # encoder_extended_attention_mask = tf.math.equal(encoder_extended_attention_mask,\n","            #                                         tf.transpose(encoder_extended_attention_mask, perm=(-1, -2)))\n","\n","            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n","        else:\n","            encoder_extended_attention_mask = None\n","\n","        present_key_value_states = () if inputs[\"use_cache\"] and self.is_decoder else None\n","        all_hidden_states = () if inputs[\"output_hidden_states\"] else None\n","        all_attentions = () if inputs[\"output_attentions\"] else None\n","        position_bias = None\n","        encoder_decoder_position_bias = None\n","\n","        hidden_states = self.dropout(inputs[\"inputs_embeds\"], training=inputs[\"training\"])\n","\n","        for idx, (layer_module, past_key_value) in enumerate(zip(self.block, inputs[\"past_key_values\"])):\n","            if inputs[\"output_hidden_states\"]:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","            layer_outputs = layer_module(\n","                hidden_states,\n","                attention_mask=extended_attention_mask,\n","                position_bias=position_bias,\n","                encoder_hidden_states=inputs[\"encoder_hidden_states\"],\n","                encoder_attention_mask=encoder_extended_attention_mask,\n","                encoder_decoder_position_bias=encoder_decoder_position_bias,\n","                layer_head_mask=inputs[\"head_mask\"][idx] if inputs[\"head_mask\"] is not None else None,\n","                encoder_layer_head_mask=inputs[\"encoder_head_mask\"][idx]\n","                if inputs[\"encoder_head_mask\"] is not None\n","                else None,\n","                past_key_value=past_key_value,\n","                use_cache=inputs[\"use_cache\"],\n","                output_attentions=inputs[\"output_attentions\"],\n","                training=inputs[\"training\"],\n","            )\n","\n","            # layer_outputs is a tuple with:\n","            # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","            hidden_states, present_key_value_state = layer_outputs[:2]\n","\n","            # We share the position biases between the layers - the first layer store them\n","            # layer_outputs = hidden-states, past_key_values, (self-attention weights),\n","            # (self-attention position bias), (cross-attention position bias), (cross-attention weights),\n","            position_bias = layer_outputs[2]\n","\n","            if self.is_decoder and inputs[\"encoder_hidden_states\"] is not None:\n","                encoder_decoder_position_bias = layer_outputs[4 if inputs[\"output_attentions\"] else 3]\n","\n","            # append next layer key value states\n","            if present_key_value_state is not None and inputs[\"use_cache\"] and self.is_decoder:\n","                present_key_value_states = present_key_value_states + (present_key_value_state,)\n","\n","            if inputs[\"output_attentions\"]:\n","                all_attentions = all_attentions + (layer_outputs[3],)\n","\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n","\n","        # Add last layer\n","        if inputs[\"output_hidden_states\"]:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not inputs[\"return_dict\"]:\n","            outputs = (hidden_states,)\n","            # need to check if is decoder here as well for special cases when using keras compile\n","            if inputs[\"use_cache\"] and self.is_decoder:\n","                outputs = outputs + (present_key_value_states,)\n","            if inputs[\"output_hidden_states\"]:\n","                outputs = outputs + (all_hidden_states,)\n","            if inputs[\"output_attentions\"]:\n","                outputs = outputs + (all_attentions,)\n","            return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n","\n","        if self.is_decoder:\n","            return TFBaseModelOutputWithPast(\n","                last_hidden_state=hidden_states,\n","                past_key_values=present_key_value_states,\n","                hidden_states=all_hidden_states,\n","                attentions=all_attentions,\n","            )\n","        else:\n","            return TFBaseModelOutput(\n","                last_hidden_state=hidden_states,\n","                hidden_states=all_hidden_states,\n","                attentions=all_attentions,\n","            )"],"metadata":{"id":"cGruojXDlj9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Switch Transformer\n","---"],"metadata":{"id":"bvwlzKx3oMXJ"}},{"cell_type":"markdown","source":["2021년 1월 Google Brain AI 연구팀이 Switch Transformer라는 모델을 공개했습니다.  \n","Switch Transformer는 backbone 모델로 T5을 사용하여 T5와 거의 비슷한 성능을 보이지만,  \n","T5보다 4배에서 7배까지 학습 속도가 빠른 모델이었어요.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-04_16-08-51.png'>\n","<br>\n","\n","Switch Transformer의 특징 중 하나는 1조 6천억 개의 파라미터를 가졌다는 것입니다.  \n","이전까지 이렇게 많은 파라미터를 가진 모델은 없었어요.  \n","아래의 그림은 Transformer 계열 모델의 파라미터를 비교한 것인데요, 파라미터가 많다고 여겨졌던 GPT3와는 비교할 수 없을 정도로 파라미터의 수가 큰 것을 알 수 있습니다.\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-10-28_10-24-46.max-800x600.png'>\n","<br>\n","\n","일반적으로 파라미터의 수가 증가하면 성능도 증가하지만, 계산 비용이 증가하고 학습 시간도 오래 걸리는 단점이 있습니다.  \n","그러나 Switch Transformer는 이런 단점을 해결하기 위해 다양한 방법을 시도하였고, 결국 성공하였죠.  \n","이제부터 Switch Transformer가 많은 수의 파라미터로 인한 단점을 보완하기 위해 사용한 여러 방법을 하나씩 알아봅시다."],"metadata":{"id":"Bn6i-X0OoU_e"}},{"cell_type":"markdown","source":["---\n","### MoE\n","---"],"metadata":{"id":"uQyFuht6oliU"}},{"cell_type":"markdown","source":["Switch Transformer의 가장 큰 특징은 Transformer의 FFN(Feed Forward Network) 레이어를 MoE(Mixture of Expert) 레이어로 바꾸었다는 것입니다.\n","\n","우선 MOE가 무엇인지 설명해야겠죠? 예를 들어 '고양이'라는 클래스가 있을 때 고양이 클래스를 나타내는 이미지는 다양할 것입니다.  \n","고양이의 일부만 보일 수 있고, 다른 객체들이 고양이와 함께 있는 이미지도 있을 수 있겠죠.  \n","그런 경우 하나의 모델이 다양한 입력 이미지를 학습하기 어려울 수 있기 때문에 'expert'라는 개념을 도입했습니다.  \n","고양이의 일부만 보는 expert, 객체 탐지 expert, 배경 분리 expert 등의 다양한 expert를 두고, gate 또는 router라고 불리는 곳에서 각각의 입력에 대해 적합한 expert를 할당하는 거죠.\n","\n","gate에서는 하나의 입력과 가중치를 곱해 softmax를 통과시켜, 높은 확률 순으로 적당한 expert 후보를 top K개 뽑고, top K개의 expert와 입력을 연산한 결과를 가중합하여 출력하게 됩니다.\n","\n","그런데 MoE에는 몇 가지 단점이 있었습니다. 구조가 복잡하고 expert간 커뮤니케이션 비용이 높으며 학습이 불안정하다는 것이었죠.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-18_10-52-07.png'>"],"metadata":{"id":"03lQLqxqo-Lq"}},{"cell_type":"markdown","source":["---\n","### Switch Routing\n","----\n","<br>\n","Switch Transformer는 MoE의 단점을 해결하기 위해 expert를 1개만 선택하였습니다.  \n","아래의 그림에서 보듯 여러 개의 토큰이 입력으로 들어오게 되면 Router라는 곳에서 각 토큰에 적합한 expert를 하나만 지정해 주는 것죠.\n","\n","이로 인해 성능은 기존의 MoE와 비슷하지만 router의 연산과 batch 사이즈, 그리고 routing 연산 후의 expert간 커뮤티케이션 비용이 줄어든다는 효과를 얻을 수 있었습니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-04_15-35-17.png'>"],"metadata":{"id":"CaB49GLzpExA"}},{"cell_type":"markdown","source":["---\n","### Distributed Switch Implementation\n","---\n","<br>\n","\n","하나의 token에 하나의 expert를 지정한다고 해도 expert의 수가 제한되어 있기 때문에 token이 많아지면 한 expert에 여러 개의 token이 몰리게 되는 현상이 일어날 수 있습니다.  \n","각 expert에 token이 고르게 배정된다면 이상적이겠지만 하나의 expert에 많은 token이 배정되고,  \n","어떤 expert에는 token이 배정되지 않는 경우가 있을 수도 있겠죠.\n","\n","이런 경우를 방지하기 위해서 Switch Transformer는 Distributed Switch Implementation라는 방법을 고안해 하나의 expert에 배정될 token의 수를 제한해 두었습니다.  \n","아래의 그림에서 보듯 capacity factor라는 것을 두어 expert가 배정받을 token 수를 제한한 거죠.  \n","예를 들어 capacity factor가 1이면 배정될 token수는 2입니다.  \n","당연하게도 capacity factor가 증가하면 계산 속도나 연산 비용은 늘어나겠죠?\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-11-04_15-41-36.png'>"],"metadata":{"id":"E3uTVGrxpTJC"}},{"cell_type":"markdown","source":["---\n","### Differentiable Load Balancing Loss\n","---\n","\n","한 expert에 배정될 token수가 제한되었다 하더라도 일부의 expert에 token이 더 몰릴 가능성이 있습니다.  \n","따라서 각각의 expert에 token이 골고루 배분되도록 Switch Transformer는 load balancing loss라는 것을 만들어 두었습니다.  \n","간단히 설명하자면 router에서 각 expert에 대한 확률값의 최대값이 최소화되도록 기존의 loss에 load balancing loss라는 항을 더해 주는 겁니다. 이에 대한 식은 아래와 같고,  \n","\n","α는 0.01로 해 주었습니다.\n","\n","$$ loss=αN⋅ ∑^N_{i=1}f_i⋅P_i $$ "],"metadata":{"id":"q5P1VTjvpjtc"}},{"cell_type":"markdown","source":["---\n","### Selective precision\n","---\n","\n","Google Brain 팀에서는 bfloat16(Brain Floating Point Format)라는 새로운 데이터타입을 만들었습니다.  \n","bfloat16는 floating point를 16 비트로 줄인 후, 지수부가 32만큼 넓은 range를 표현할 수 있도록 하되, precision을 희생하는 부동소수점 포맷입니다.  \n","보통 거대한 모델에서는 bfloat16을 사용하면 모델을 효율적으로 학습시킬 수 있었지만, MoE 모델에서는 bfloat16을 사용하면 발산하는 문제가 생겼다고 합니다.\n","\n","그래서 Switch Transformer에서는 router 레이어에서만 bfloat32를 사용하고,  \n","그 외의 부분에서는 bfloat16을 사용해 발산하는 문제도 줄이고 효율적인 커뮤니케이션도 가능하도록 했습니다.  \n","  \n","이 외에도 Switch Transformer에서 사용된 다양한 방법에는 parameter initialization를 작게 만들거나  \n","expert dropout와 기존의 dropout을 적절히 조정해 사용하는 selective dropout을 사용하는 것 등이 있습니다."],"metadata":{"id":"lDy06R-op911"}},{"cell_type":"markdown","source":["---\n","# ERNIE\n","---"],"metadata":{"id":"3YHVE4omqKOA"}},{"cell_type":"markdown","source":["이번에는 조금 특별한 모델을 살펴볼 예정입니다.  \n","지금까지 살펴본 모델들은 우리가 흔히 접하는 Tensorflow나 PyTorch같은 프레임워크로 구현이 되었습니다.  \n","그런데 이제부터 살펴볼 모델은 PaddlePaddle이라는 중국의 오픈소스 프레임워크를 사용하였습니다.\n","\n","PaddlePaddle은 'PArallel Distributed Deep LEarning’의 약자로,  \n","중국의 검색엔진 Baidu에서 만든 머신러닝 프레임워크입니다.  \n","코어 라이브러리는 C++이고, Python으로 인터페이스를 만들었다고 하네요.\n","\n","그 후 PaddlePaddle을 사용한 모델이 여러 개 만들어졌고, 그 중 하나가 ERNIE입니다.  \n","사실 ERNIE는 두 가지 버전이 있어요.  \n","하나는 칭화대에서 만든 [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)이고,  \n","다른 하나는 바이두에서 만든 [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf)입니다.\n","\n","ERNIE라고 불리기에는 조금 억지가 아닌가 싶지만 BERT에게 친구를 만들어주긴 위한 중국인들의 노력이 가상하지 않나요? \n","\n","2가지 버전의 ERNIE 중 바이두에서 만든 ERNIE를 살펴볼 예정인데요,  \n","그 이유는 바이두의 ERNIE v3가 SuperGLUE Leaderboard에서 1등을 차지하기도 하였고,  \n","바이두에서 계속해서 발전된 모델을 선보이고 있기 때문이기도 해요.\n","\n","아래의 그림에서처럼 바이두의 ENRIE는 다양한 버전이 존재하고, 라이브러리도 지원되며 서버로 서비스할 수 있는 모듈까지 제공하고 있습니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/ERNIE_milestone_en.max-800x600.png'>\n","<br>\n","\n","위에서도 살짝 언급했듯이 ERNIE는 버전 3까지 있는데요, 이번 스텝에서는 v1만 살펴볼 예정입니다.  \n","[논문 ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf)에서 소개된 ERNIE v1를 살펴볼까요?"],"metadata":{"id":"u1TmCGODqORY"}},{"cell_type":"markdown","source":["---\n","### Masking Strategies\n","---\n","\n","ERNIE는 아래의 한 문장으로 요약할 수 있습니다. 다양한 마스킹 전략을 통해 language reperesentation을 향상시켰다는 거죠.\n","\n",">ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking.\n","\n","ERNIE는 일반적인 마스킹 방법뿐 아니라 Phrase-Level Masking과 Entity-Level Masking을 추가로 사용하였습니다.   \n","일반적인 마스킹 방법은 각 글자를 단위로 하여 입력의 15%를 무작위로 마스킹하였지만,  \n","  ENRIE의 경우 구문 전체를 마스킹(Phrase-Level Masking)하거나 고유 명사 전체를 마스킹(Entity-Level Masking)하였죠.  \n","  \n","이는 모델에 사전 지식을 주입하기 위한 것이었어요.\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-16_12-36-13.max-800x600.png'>\n"],"metadata":{"id":"G6uSk6kXqy-V"}},{"cell_type":"markdown","source":["---\n","### Transformer Encoder\n","---\n","\n","ERNIE는 GPT, BERT, XLM과 같이 여러 층을 쌓은 Transformer를 Encoder로 사용했어요.  \n","하지만 중국어 corpus를 사용하기 때문에 CJK Unicode에 있는 모든 글자에 공간을 추가해주었고, WordPiece를 사용했다고 해요.\n","\n","ERNIE는 BERT와 같이 12개의 Encoder layer, 768개의 Hidden Units, 12개의 Attention Head로 구성되어 있고,  \n","중국어 위키피디아, 바이두 백과, 바이두 뉴스, 레딧과 비슷한 포럼 공간인 바이두 티에바에서 가져온 다양한 데이터를 입력 데이터로 사용합니다.  \n","특히 바이두 티에바에서는 다이얼로그 형식의 데이터를 가지고 와 DLM task에 사용했다고 해요.\n","\n","DLM(Dialogue Language Model)는 대화형 데이터를 학습하기 위해 도입된 것으로,  \n","아래 그림에서와 같이 Dialogue embedding을 사용해 다이얼로그의 역할을 구분하였습니다.  \n","Dialogue embedding은 BERT의 token embedding과 같은 역할을 하지만 여러 번의 턴(turn)을 가지는 대화를 표현할 수 있도록 하였어요.  \n","예를 들어 QRQ, QRR, QQR과 같은 식으로 말이죠.  \n","여기서 Q는 Query, R은 Response에요.  \n","ERNIE는 Q와 R에 마스킹을 씌워 마스킹된 단어를 예측하게 하기도 하고,  \n","가짜 대화를 만들어서 DLM이 여러 번의 턴을 가지는 대화가 진짜인지, 가짜인지 판단하도록 하기도 했어요.\n","\n","DLM task는 ERNIE가 대화에서 암묵적인 관계를 학습하도록 해 semantic representation을 잘 학습할 수 있도록 도와주었습니다.  \n","또한 MLM과 같은 구조와 역할을 하기 때문에 BERT의 MLM 대신 DLM을 사용했다고 하네요.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/seukeurinsyas_2021-11-16_12-46-40.max-800x600.png'>\n","<br>\n","\n","ERNIE v1이 나온지 3개월만에 v2가 나왔습니다.  \n","ERNIE v2는 pretraining할 때 여러 task를 추가해 학습하는 방식을 사용했어요.  \n","2021년에 나온 ERNIE v3은 v1과 v2를 합친 모델이에요."],"metadata":{"id":"RC9r8fkzrVcC"}},{"cell_type":"code","source":[""],"metadata":{"id":"NlJxl_x5oSNB"},"execution_count":null,"outputs":[]}]}