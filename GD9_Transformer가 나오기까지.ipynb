{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GD9_Transformer가 나오기까지.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM6cuWILAeyIpfSsZrr2XZ9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["----\n","# Attention의 역사\n","---"],"metadata":{"id":"ik_q8KRLeI5M"}},{"cell_type":"markdown","source":["트랜스포머를 본격적으로 다루기 전에 Seq2seq와 Attention 기법을 간단하게 복습해 봅시다!  \n","  \n","Sequence-to-sequence(Seq2seq)는 신경망 언어 모델, 특히 기계번역에서 혁신적인 발전을 이룩해냈습니다.  \n","기존의 단일 RNN은 번역에 적합한 구조가 아니었으나 두 개의 RNN을 결합한 Encoder-Decoder 구조를 만들어 문맥 전체를 반영한 번역이 가능하게 했죠. \n","\n","    Before)\n","    Step 1: [나는] -> [I]\n","    Step 2: [나는] [점심을] -> [I] [lunch]\n","    Step 3: [나는] [점심을] [먹는다] -> [I] [lunch] [eat(?)]\n","\n","    After)\n","    Encode(나는 점심을 먹는다)\n","    -> 고정된 크기의 컨텍스트 벡터\n","    -> Decode(컨텍스트 벡터) = I eat lunch!\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L-2.max-800x600.jpg'>\n","<br><br>\n","단순 RNN은 긴 입력에 대한 정보를 학습시키기 어렵기 때문에 Seq2Seq에서는 LSTM을 사용한 Encoder-Decoder 구조를 채택했습니다.  \n","논문에서 사용된 표현을 빌리면, Encoder에 Input Sequence x를 넣으면 고정된 크기의 Representation Vector v에 모든 정보를 담아 Decoder에게 전달해 주는 구조입니다.  \n","Decoder는 전달받은 v를 기반으로 Output Sequence y를 생성합니다.  \n","추가로 v는 컨텍스트 벡터(context vector) 라고 불리기도 하죠.\n","\n","하지만 고정된 크기의 컨텍스트 벡터를 사용하는 것은 필연적으로 정보의 손실을 야기합니다.  \n","특히 문장이 길어지면 더더욱 손실이 커집니다.  \n","단순하게 생각해서 컨텍스트 벡터가 모든 Embedding의 평균이라 하면 3단어를 포함하는 문장과 100단어를 포함하는 문장 중 손실이 일어날 쪽은 당연히 후자겠죠?  \n","  \n","그래서 모든 단어를 같은 비중으로 압축하지 말고, 번역하는 데에 중요한 단어만 큰 비중을 줘서 성능을 높여보자며 Dzmitry Bahdanau가 Attention을 제안합니다.\n","\n","매 스텝의 Hidden State 값을 사용하는 것은 제법 효과가 좋았습니다.  \n","실제로 그렇게 만들어진 Attention을 히트맵으로 출력하면 의미적으로 유사한 단어들끼리 연결되는 것을 볼 수 있었거든요.  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-3.jpg'>\n","<br><br>\n","대각선이 역행하는 European Economic Area 부분을 유심히 보시면  \n","같은 의미의 불어 zone economique europeenne과 의미적으로 유사한 것들끼리 연결된 것을 볼 수 있습니다.  \n","  \n","Bahdanau Attention의 문제라면 T 스텝에서 Decoder의 Hidden State를 구하기 위해 T-1 스텝의 Hidden State를 사용해야 한다는 것이었죠.  \n","이는 재귀적으로 동작하는 RNN에 역행하는 연산이므로 효율적이지 못했습니다.  \n","이를 개선하고자 한 것이 Luong이 제안한 Attention 기법입니다!\n","  \n","이러나저러나 NLP 세상에 Attention 붐이 온 것은 확실했습니다.  \n","실제로 우리가 문장을 이해하는 방식과도 굉장히 유사하기에 \"저건 무조건 발전해야 해!\"라는 느낌을 주죠.  \n","그리고 2017년, 이 흐름을 종결하기라도 하려는 듯 Attention Is All You Need 라는 충격적인 제목의 논문이 등장합니다."],"metadata":{"id":"8OWbWnu5ebVH"}},{"cell_type":"markdown","source":["---\n","# Attention Is All You Need!\n","---"],"metadata":{"id":"y_8gNdX7jaj-"}},{"cell_type":"markdown","source":["집중! 이번 코스에서 여러분께 필요한 것도 오직 Attention입니다!\n","\n","충격적인 제목의 논문은 그 내용마저도 충격적이었습니다. 서론의 한 구절을 살펴보죠.\n","\n",">We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.   \n","(우리는 RNN과 CNN을 완전히 배제하고 Attention 메커니즘에만 기반을 둔 새롭고 단순한 구조, Transformer를 제안합니다.)*\n","\n","RNN과 CNN을 완전히 배제한다니! CNN은 자연어 처리에 있어서 흔히 사용되지는 않기 때문에 그럴 수 있다고 하지만 RNN은 고안된 배경이 애초에 언어를 모델링 하기 위함이었는데...  \n","\n","생각해 보면 그간 RNN은 많은 발전을 거듭했지만 그럼에도 고질병인 기울기 소실(Vanishing Gradient) 은 완벽히 해결되지 않았습니다.  \n","그럼에도 울며 겨자 먹기로 문장 데이터의 순차적인 특성을 유지하기 위해 사용이 필연적이라 생각했지만... 고정관념이었던 거죠!\n","\n","심지어 순차적으로 계산한다는 특성은 양날의 검이었습니다.  \n","그 특성 때문에 병렬 처리가 불가능하다 는 것은 큰 문제점이었거든요.  \n","이 또한 다들 지적은 하지만 마땅한 해결책이 없던 와중에 이를 배제할 수 있다니!  트랜스포머 는 그야말로 신문물이었습니다.\n","\n","논문이 포함하는 (거의) 모든 내용은 본 코스에서 다룰 예정입니다.  \n","이 코스를 마치고 나서 복습하며 살펴보시는 것도 좋고, 지금 가볍게 읽어보시는 것도 좋습니다. 😃\n","\n","저자들은 문장에서 연속성이라는 개념을 과감히 배제 하였습니다.  \n","대신 Attention으로 각 단어들의 상관관계를 구하는 데 집중 하였죠.  \n","일전에 배우셨듯이 문장을 모델링 한다는 것은 주어진 단어를 보고 모르는 단어에 확률을 할당하는 것입니다.   \n","연속성이 배제된 채로 문장을 모델링 한다는 것은, 단순히 생각하면 입력으로 빨간 사과 노란 바나나가 들어가는 것과 노란 사과 빨간 바나나가 들어가는 것이 동일하게 취급되는 셈입니다.  \n","\n","RNN처럼 단어 입력을 순차적으로 처리하지도 않으면서 어떻게 문장을 정확히 처리해냈는지는 차근차근 이해해 보도록 하죠!\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-4.jpg'>\n","<br><br>\n","트랜스포머도 엄밀하게는 Seq2seq에서 제안된 Encoder-Decoder 구조를 사용합니다.  \n","대신 그 안에는 LSTM 대신 다양한 모듈들이 가득 차 있죠.  \n","그리고 이 멋진 Encoder와 Decoder를 여러 개(논문에서는 6개) 쌓아올려 모델을 완성합니다.  \n","\n",">가끔 헷갈려 하시는 분들이 있어 첨언하면,  \n","Enc-Dec-Enc-Dec-... 구조가 아닌 Enc-Enc-...-Dec-Dec-... 구조입니다!\n","\n","먼저 문장의 연속성을 어떻게 배제할 수 있었는지, 모델의 최하단에 위치한 Positional Encoding 부터 차근차근 알아보도록 하겠습니다."],"metadata":{"id":"qVQnJqifklGu"}},{"cell_type":"markdown","source":["----\n","#  Positional Encoding\n","---"],"metadata":{"id":"kpj_cikflLHS"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-5.jpg'>\n","<br><br>\n","이전 스텝에서 문장의 연속성을 배제할 경우 빨간 사과 노란 바나나와 노란 사과 빨간 바나나를 같은 문장으로 간주하게 된다고 했죠?  \n","Positional Encoding은 그런 불상사를 막기 위한 방법입니다.  \n","즉, 문장에 연속성을 부여하는 새로운 방법을 제시한 거죠.\n","\n","쉽게 말하면 입력이 들어온 순서대로 단어에 표기를 하는 것과 같습니다.  \n","예를 들면 [빨간 + 1] [사과 + 2] [노란 + 3] [바나나 + 4]과 같은 모양새로 말이죠.  \n","하지만 그렇다고 단어 Embedding에 선형적으로 증가하는 값을 더해줬다간 후에 데이터의 분포가 엉망이 될 겁니다.  \n","문제없이 Position을 나타낼 수 있는 방법엔 어떤 것들이 있을까요?  \n","\n","저자들이 사용한 Positional Encoding 수식은 아래와 같습니다.\n","\n","$$ PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) $$\n","\n","$$ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}}) $$\n","\n","pos는 단어가 위치한 Time-step을 의미하며  \n","i는 Encoding 차원의 Index,  \n","$d_{model} $ 은 모델의 Embedding 차원 수입니다.  \n","\n","이를 Sinusoid(사인파) Embedding이라고 칭합니다.   \n","  \n","이해를 돕기 위해 실제 구현을 확인해보죠."],"metadata":{"id":"ONz1bqj2lNvw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qF2LJlaqeFko"},"outputs":[],"source":["import numpy as np\n","\n","def positional_encoding(pos, d_model):\n","    def cal_angle(position, i):\n","        return position / np.power(10000, int(i) / d_model)\n","\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, i) for i in range(d_model)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n","\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n","\n","    return sinusoid_table\n","\n","pos = 7\n","d_model = 4\n","i = 0\n","\n","print(\"Positional Encoding 값:\\n\", positional_encoding(pos, d_model))\n","\n","print(\"\")\n","print(\"if pos == 0, i == 0: \", np.sin(0 / np.power(10000, 2 * i / d_model)))\n","print(\"if pos == 1, i == 0: \", np.sin(1 / np.power(10000, 2 * i / d_model)))\n","print(\"if pos == 2, i == 0: \", np.sin(2 / np.power(10000, 2 * i / d_model)))\n","print(\"if pos == 3, i == 0: \", np.sin(3 / np.power(10000, 2 * i / d_model)))\n","\n","print(\"\")\n","print(\"if pos == 0, i == 1: \", np.cos(0 / np.power(10000, 2 * i + 1 / d_model)))\n","print(\"if pos == 1, i == 1: \", np.cos(1 / np.power(10000, 2 * i + 1 / d_model)))\n","print(\"if pos == 2, i == 1: \", np.cos(2 / np.power(10000, 2 * i + 1 / d_model)))\n","print(\"if pos == 3, i == 1: \", np.cos(3 / np.power(10000, 2 * i + 1 / d_model)))"]},{"cell_type":"markdown","source":["위 예제의 경우, 직접 Positional Encoding의 1번째 열과 2번째 열을 4번째 행까지 구해보는 하드 코딩이 포함되어 있습니다.  \n","혹시라도 수식의 각 요소들이 무엇을 의미하는지 와닿지 않는다면 소스를 분석하며 이해해 보세요!   \n","  \n","또한 Position 값이 각 Time-step 별로 고유하다는 것을 시각화를 통해 확인해 볼 수 있습니다. "],"metadata":{"id":"dvn53t6ytEYC"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(7, 7))\n","plt.imshow(positional_encoding(100, 300), cmap='Blues')\n","plt.show()"],"metadata":{"id":"tsReT9MwtF3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["세로축이 Time-step에 해당하고 가로축이 Word Embedding에 더해질 Position 값입니다.  \n","한눈에 봐도 각 스텝마다 고유한 값을 가지는 것을 알 수 있죠?  \n","게다가 굉장히 아름답습니다!\n","  \n","저자들은 이뿐만 아니라 Positional Embedding 기법도 제안했습니다.  \n","수식적으로 계산한 Position 값이 아니라 Position에 대한 정보를 담은 Embedding 레이어를 선언하여 위치에 대한 정보를 학습할 수 있게 한 거죠.  \n","  \n","그림으로 표현하면 아래와 같습니다.  \n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L-7.max-800x600.jpg'>\n","\n","위와 같은 구조에서 Positional Embedding이 훈련 중 값이 변한다고 하더라도 그것은 모든 문장에 대해 동일하게 적용되기에 문제가 되지 않습니다.  \n","오히려 Sinusoid Embedding보다 적합한 값이 학습될 것을 기대할 수도 있죠!\n","\n","하지만 실제로는 두 방법 모두 거의 동일한(±0.1 BLEU) 결과를 보였으며,  \n","저자들은 길이가 길어져도 부담이 없는 Sinusoid Embedding을 채택하였습니다.  \n","Positional Embedding은 문장의 길이만큼 Embedding Table의 크기가 커지니까요.  \n","Positional Embedding은 추후에 BERT라는 모델에 적용되어 멋진 성능을 보여줄 예정입니다!"],"metadata":{"id":"ZJLGEHZktSih"}},{"cell_type":"markdown","source":["---\n","# Multi-Head Attention\n","---"],"metadata":{"id":"dIFYqjUUt90y"}},{"cell_type":"markdown","source":["이번 스텝에선 트랜스포머의 핵심으로 꼽히는 Multi-Head Attention 에 대해 알아보겠습니다.   Positional Embedding이 된 문장으로부터 Attention을 추출하는 부분이죠!  \n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-8.jpg'>\n","<br><br>\n","위 그림에서 보라색으로 표시된 Masked Multi-Head Attention은 Multi-Head Attention과 동일하지만  \n","인과 관계 마스킹(Causality Masking) 이라는 과정이 하나 더 추가됩니다.  \n","흐름에 따라 자연스럽게 후술하겠습니다!\n","\n","Multi-Head Attention 모듈은 Linear 레이어와 Scaled Dot-Product Attention 레이어로 이루어집니다.\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L-9.max-800x600.jpg'>\n","<br><br>\n","지금은 너무 헷갈리는 그림과 용어들일 수 있습니다, 차근차근 배워볼게요!\n"],"metadata":{"id":"ru2coDfBuASD"}},{"cell_type":"markdown","source":["---\n","### Scaled Dot-Product Attention\n","----"],"metadata":{"id":"KRA0U2yPugPX"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-10.jpg'>\n","<br><br>\n","먼저 Scaled Dot-Product Attention에 대해 배워보겠습니다.   \n","트랜스포머 논문 원문을 처음 읽게 될 때 맞닥뜨리게 되는 어려움 중 하나는,  \n","수식도 이해되고 네트워크 구조도 이해할 수 있음에도 불구하고 그 의미가 직관적으로 잘 와닿지 않는다는 점인데요,  \n","특히 그중에서도 위 그림에 나오는 Q(query), K(key), V(value)가 어떤 의미를 가지고 있는지 특별한 설명이 없다는 점입니다.   \n","  \n","이 3가지의 의미는 오히려 후속 연구들을 통해 더욱 구체적으로 밝혀지고 있는데, 어쩌면 지금 이 시점까지도 계속 새롭게 그 의미가 드러나고 있다고도 할 수 있겠습니다.  \n","  \n","하지만 2017년 시점으로 돌아가서, 이 Scaled Dot-Product Attention도 결국 그때까지 나왔던 attention 개념의 연장선이었음을 떠올려 봅시다.  \n","그래서 이해를 돕기 위해 이전 시간에 다루어 보았던 Bahdanau attention 개념과 나란히 놓고 비교해 보겠습니다.  \n","  \n","아래 그림의 오른쪽은 지난 시간에 다루었던 전통적인 attention 개념입니다.  \n","\n","seq2seq 인코더-디코더 구조에서 attention이란 바로 디코더의 포지션 i에서 바라본 인코더의 context vector $C_i$를 해석하기 위해, 인코더의 각 포지션 j에 부여한 가중치였습니다. \n","\n","이 가중치는 디코더의 state $S_i$ 와 인코더의 state $h_j$ 사이의 유사도를 통해 계산되었습니다.  \n","\n","원리는 동일합니다. 단지 디코더의 state를 Q(query)라고 부르고, 인코더의 state를 K(key)라고 추상화한 것입니다. 그래서 \n","\n"," ### Q와 K의 유사도를 dot product로 계산하여 $softmax(\\frac{QK^T}{\\sqrt{dk}})$를 attention 가중치로 삼고, 이것으로 V(value)를 재해석해 준 것입니다.  \n","\n","하나 다른 점은, 인코더 쪽에서 $h_j$ 하나만 존재하던 것이 K와 V 2가지로 분화되었다는 점입니다.\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L_attentions.max-800x600.png'>\n","<br><br>\n","이제 여러분들은 위 그림에서 왼쪽 그림의 Attention(Q, K, V)와 오른쪽 그림의 $C_i$ 가 실은 같은 형태이며,  \n","$softmax(\\frac{QK^T}{\\sqrt{dk}})$와 $α_{ij}$ 가 같은 역할을 하고 있다는 것이 눈에 들어오실 거라 생각합니다.\n","\n","    Q4. 트랜스포머의 Attention은 위의 식으로 Attention 값을 나눠준다는 것에서 \"Scaled\" Dot-Product Attention이라고 불립니다.  \n","    이 Scale 과정은 어떤 의미를 가지나요? (또는, 왜 필요한가요?)\n","\n","    Embedding 차원 수가 깊어지면 깊어질수록 Dot-Product의 값은 커지게 되어  \n","    Softmax를 거치고 나면 미분 값이 작아지는 현상이 나타난다. 그 경우를 대비해 Scale 작업이 필요하다.\n","\n","정리하자면 Scaled Dot-Product Attention은 Additive(합 연산 기반) Attention 과 \n","Dot-Product(=Multiplicative, 곱 연산 기반) Attention 중 후자를 사용한 Attention이고,  \n","차원 수가 깊어짐에 따라 Softmax 값이 작아지는 것을 방지하기 위해 Scale 과정을 포함하였습니다.  \n","이 과정을 본떠 Scaled Dot-Product Attention 이라고 이름이 붙여진 것이죠.  \n","   \n","다만 Attention 속 Softmax의 성질을 이해하지 못한 상태라면 약간의 찜찜함이 남아있을 수 있습니다.  \n","\n","이를 해결하기 위해 간단한 코딩으로 직접 값을 확인해 보도록 하죠!"],"metadata":{"id":"5_LFo5Pduigo"}},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","def make_dot_product_tensor(shape):\n","    A = tf.random.uniform(shape, minval=-3, maxval=3)\n","    B = tf.transpose(tf.random.uniform(shape, minval=-3, maxval=3), [1, 0])\n","\n","    return tf.tensordot(A, B, axes=1)\n","\n","length = 30\n","big_dim = 1024.\n","small_dim = 10.\n","\n","big_tensor = make_dot_product_tensor((length, int(big_dim)))\n","scaled_big_tensor = big_tensor / tf.sqrt(big_dim)\n","small_tensor = make_dot_product_tensor((length, int(small_dim)))\n","scaled_small_tensor = small_tensor / tf.sqrt(small_dim)\n","\n","fig = plt.figure(figsize=(13, 6))\n","\n","ax1 = fig.add_subplot(141)\n","ax2 = fig.add_subplot(142)\n","ax3 = fig.add_subplot(143)\n","ax4 = fig.add_subplot(144)\n","\n","ax1.set_title('1) Big Tensor')\n","ax2.set_title('2) Big Tensor(Scaled)')\n","ax3.set_title('3) Small Tensor')\n","ax4.set_title('4) Small Tensor(Scaled)')\n","\n","ax1.imshow(tf.nn.softmax(big_tensor, axis=-1).numpy(), cmap='cividis')\n","ax2.imshow(tf.nn.softmax(scaled_big_tensor, axis=-1).numpy(), cmap='cividis')\n","ax3.imshow(tf.nn.softmax(small_tensor, axis=-1).numpy(), cmap='cividis')\n","ax4.imshow(tf.nn.softmax(scaled_small_tensor, axis=-1).numpy(), cmap='cividis')\n","\n","plt.show()"],"metadata":{"id":"mDpw7nUtta_0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모두 같은 범위인 [-3, 3]에서 랜덤 Tensor를 만들어 실제 Attention을 하듯 Dot-Product를 하고, Softmax를 취했습니다.  \n","위 히트맵에서 어두운 부분으로는 미분 값(Gradient) 이 흐르기 어렵기 때문에 모델이 넓은 특성을 반영할 수 없게 됩니다.  \n","즉, 히트맵이 선명할수록 모델의 시야가 편협해진다 고 이해하시면 좋아요!  \n","  \n","위 시각화를 통해 Embedding의 깊이가 깊을수록 모델의 시야가 편협해지는 문제가 생기고 [1, 3],  \n","Scale은 그 문제의 훌륭한 해결책임을 확인할 수 있습니다[2, 4].  \n","심지어 깊이에 무관하게 일정한 결과를 만들어내므로 어떤 경우에도 적용할 수 있는 훌륭한 Attention 기법이 탄생했음을 알 수 있습니다!"],"metadata":{"id":"3E_OtoIA1tUO"}},{"cell_type":"markdown","source":["---\n","### 인과 관계 마스킹(Causality Masking)\n","----"],"metadata":{"id":"TieYqpS218qw"}},{"cell_type":"markdown","source":["이 놀라운 Attention 기법에 대해 배울 것이 아직 하나 더 남아있습니다.  \n","앞서 언급한 인과 관계 마스킹(Causality Masking) 이 바로 그것입니다!  \n","\n","Sequence-to-sequence 모델을 훈련할 때에 Decoder의 첫 입력으로 \\<start> 토큰이 들어가던 것을 기억하실 겁니다.  \n","그럼 Decoder는 컨텍스트 벡터로 압축된 입력 문장과 \\<start> 토큰만을 단서로 첫 번째 단어를 생성해야 하죠.  \n","그 다음 스텝도 같은 단서에 추가로 방금 Decoder 본인이 생성한 첫 번째 단어를 포함하여 두 번째 단어를 생성합니다.  \n","이 같은 특성을 자기 회귀(Autoregressive) 라 칭합니다. 자기 자신을 입력으로 하여 자기 자신을 예측하는 것이죠.  \n","  \n","하지만 트랜스포머는 모든 단어를 병렬적으로 처리하기에 자기 회귀적인 특성을 잃어버립니다.  \n","이는 곧 문장을 생성할 수 없다는 의미이고, 쓸모가 없다는 얘기죠!\n","  \n","이에 저자들이 자기 회귀적인 특성을 살리기 위해 추가한 것이 바로 인과 관계 마스킹(Causality Masking) 입니다!  \n","인과 관계 마스킹은 목표하는 문장의 일부를 가려 인위적으로 연속성을 학습 하게 하는 방법입니다.  \n","  \n","말보단 그림으로 살펴보시죠!  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/causal_masking.max-800x600.png'>\n","<br><br>\n","위와 같은 과정을 거치면 모든 Time-Step에 대한 입력을 한 번에 처리하면서 자기 회귀적인 특성도 유지하게 됩니다.  \n","테스트 시 소스 문장을 Encoder에 전달하고 타겟 문장은 \\<start>만 넣더라도  \n","모델이 \\<start> 토큰만 보고 문장을 생성한 적(위 그림의 마지막 Step)이 있기 때문에 첫 번째 단어를 생성해낼 수 있고,  \n","생성된 단어는 다시금 입력으로 전달되어 그야말로 자기 회귀적으로 문장을 생성하게 됩니다.  \n","  \n","이러한 연유로 인과 관계 마스크는 대각항을 포함하지 않는 삼각 행렬의 모양새를 갖습니다.  \n","입력 문장만을 보고 첫 번째 단어를 생성하는 것은 타겟 문장을 모두 가리는 것이 타당하니 대각항을 포함하는 게 맞지만,  \n","그럴 경우 Attention 값을 구하는 과정에서 마지막(혹은 첫 번째) 행이 0개 요소에 대해 Softmax를 취하게 되므로 오류를 야기합니다.  \n","따라서 \\<start> 토큰을 활용해 마스크가 대각항을 포함하지 않는 형태가 되게끔 만든 것이죠.\n","  \n","(참고) 대각항(diagonal entry)이란 행렬에서 행과 열의 지표수가 같은 성분(대각성분)을 말합니다 "],"metadata":{"id":"aauv2-T53E3j"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","def make_dot_product_tensor(shape):\n","    A = tf.random.uniform(shape, minval=-3, maxval=3)\n","    B = tf.transpose(tf.random.uniform(shape, minval=-3, maxval=3), [1, 0])\n","\n","    return tf.tensordot(A, B, axes=1)\n","\n","def generate_causality_mask(seq_len):\n","    mask = 1 - np.cumsum(np.eye(seq_len, seq_len), 0)\n","    return mask\n","\n","sample_tensor = make_dot_product_tensor((20, 512))\n","sample_tensor = sample_tensor / tf.sqrt(512.)\n","\n","mask = generate_causality_mask(sample_tensor.shape[0])\n","\n","fig = plt.figure(figsize=(7, 7))\n","\n","ax1 = fig.add_subplot(121)\n","ax2 = fig.add_subplot(122)\n","\n","ax1.set_title('1) Causality Mask')\n","ax2.set_title('2) Masked Attention')\n","\n","ax1.imshow((tf.ones(sample_tensor.shape) + mask).numpy(), cmap='cividis')\n","\n","mask *= -1e9\n","ax2.imshow(tf.nn.softmax(sample_tensor + mask, axis=-1).numpy(), cmap='cividis')\n","\n","plt.show()"],"metadata":{"id":"NbqwwOMw17Xr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["좌측은 실제 마스크의 형태, 우측은 마스킹이 적용된 Attention입니다.  \n","마스킹은 마스킹 할 영역을 -∞로 채우고 그 외 영역을 0으로 채운 배열을 Dot-Product된 값에 더해주는 방식으로 진행됩니다.  \n","후에 진행될 Softmax는 큰 값에 높은 확률을 할당하는 함수이므로 -∞로 가득 찬 마스킹 영역에는 무조건 0의 확률을 할당하게 됩니다."],"metadata":{"id":"xuSd9cb3E0IX"}},{"cell_type":"markdown","source":["---\n","### Multi-Head Attention\n","---"],"metadata":{"id":"LbtgofI0E5e1"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-14.jpg'>\n","<br><br>\n","Scaled Dot-Product Attention에서 정말 많은 이야기를 했는데, 그것을 이해하고 나면 정작 Multi-Head Attention 모듈은 간단합니다.  \n","일전에 Embedding이 어떤 추상적인 속성을 담고 있다고 배웠던 것, 기억하시나요?  \n","Multi-Head Attention은 그 개념을 활용해서 이해를 하면 아주 쉽습니다.  \n","  \n","바나나라는 단어가 512차원의 Embedding을 가진다고 가정합시다.  \n","그 중 64차원은 노란색에 대한 정보를 표현하고, 다른 64차원은 달콤한 맛에 대한 정보를 표현할 겁니다.   \n","같은 맥락으로 바나나의 형태, 가격, 유통기한까지 모두 표현될 수 있겠죠.  \n","저자들은 '이 모든 정보들을 섞어서 처리하지 말고, 여러 개의 Head로 나누어 처리하면 Embedding의 다양한 정보를 캐치할 수 있지 않을까?' 라는 아이디어를 제시합니다.\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L-15.max-800x600.jpg'>\n","<br><br>\n","Multi-Head Attention에서 Head는 주로 8개를 사용합니다.  \n","Embedding된 10개 단어의 문장이 [10, 512]의 형태를 가진다면, Multi-Head Attention은 이를 [10, 8, 64]로 분할하여 연산합니다.  \n","각 64차원의 Embedding을 독립적으로 Attention한 후, 이를 이어붙여 다시금 [10, 512]의 형태로 되돌리며 연산은 끝이 납니다.  \n","\n",">\"에이, 쪼개진 64차원이 연관 있는 것들끼리 묶여있을 거란 보장이 없잖아요~\"\n","\n","위와 같은 의문을 가졌다면 잘하셨습니다! Head로 쪼갠 Embedding들끼리 유사한 특성을 가진다는 보장이 없기 때문에 앞단에 Linear 레이어를 추가해 주는 겁니다.  \n","Linear 레이어는 데이터를 특정 분포로 매핑 시키는 역할을 해주기 때문에,  \n","설령 단어들의 분포가 제각각이더라도 Linear 레이어는 Multi-Head Attention이 잘 동작할 수 있는 적합한 공간으로 Embedding을 매핑합니다.\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-16.jpg'>\n","<br><br>\n","비슷한 이유로 각각의 Head가 Attention 한 값이 균일한 분포를 가질 거란 보장이 없습니다.  \n","따라서 모든 Attention 값을 합쳐준 후, 최종적으로 Linear 레이어를 거치며 비로소 Multi-Head Attention이 마무리가 됩니다."],"metadata":{"id":"3rawY-pjFZ2a"}},{"cell_type":"markdown","source":["---\n","# Position-wise Feed-Forward Networks\n","---"],"metadata":{"id":"4JyLWngQHr3o"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-17.jpg'>\n","<br><br>\n","Position-wise Feed-Forward Network는 위의 Multi-Head Attention만큼 센세이셔널하지는 않습니다.  \n","그래서 논문에서도 큰 분량을 차지하지 않는 개념이죠. 양이 많지 않으니 논문의 구절을 그대로 읽어보시는 것을 추천드립니다.\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L-18.max-800x600.jpg'>\n","<br><br>\n","W는 Linear 레이어를 의미하고, max(0, x) 함수는 활성 함수 ReLU입니다.  \n","예를 들면 10단어로 이루어진 Attention된 문장 [10, 512]를 [10, 2048] 공간으로 매핑,  \n","활성함수를 적용한 후 다시 [10, 512] 공간으로 되돌리는 것입니다. \n","\n","설명 중 와닿지 않는 부분을 하나 꼽으라면, 많은 분들이 이 모듈이 커널 사이즈가 1인 Convolution을 두 번 연산한 것과 동일하다고 하는 부분일 거라 예상합니다.  \n","생각보다 이 Linear 연산을 CNN으로 구현한 것도 많고 논문에서도 Convolution으로 언급하는 경우가 종종 있어 지금 이해해두시면 좋습니다.  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-5-L_1dconv.max-800x600.png'>\n","<br><br>\n","한 단어를 Embedding 차원만큼의 채널을 갖는 이미지라고 취급한다면 이해가 되실 겁니다.  \n","Convolution 레이어의 Weight는 [입력 차원 수 X 출력 차원 수 X 커널의 크기] 이므로 커널의 크기가 1이라면 Linear 레이어와 동일한 크기의 Weight를 갖게 되죠.  \n","\n","최근에는 구글에서 AI를 만드는 AI, AutoML을 활용해 최적의 트랜스포머 구조를 찾아내기도 했습니다.  \n","진화된 트랜스포머(Evolved Transformer) 는 두 층의 레이어 중 아래층 레이어에서 차이를 보이는데,  \n","이때 Linear 레이어를 Convolution 레이어로 표기하는 방식을 택합니다.  \n","궁금하신 분들은 아래 웹페이지에서 진화된 트랜스포머의 구조를 살펴보세요! \n","\n","[Applying AutoML to Transformer Architectures](https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html)"],"metadata":{"id":"SQywespvHuZY"}},{"cell_type":"markdown","source":["---\n","# Additional Techniques\n","---"],"metadata":{"id":"b89F12pJNRZw"}},{"cell_type":"markdown","source":["여기까지 트랜스포머의 큰 줄기는 다 배웠습니다. 이제 세부 줄기에 대해서 살펴볼 건데요,  \n","이 세부 줄기가 별거 아닌 것 같아 보여도 성능에 제법 영향을 끼치니 놓치는 부분 없이 잘 숙지하시길 바랍니다!  \n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-5-L-20.jpg'>  \n","\n","먼저 그림 상으로 우리가 배우지 않은 유일한 모듈, Add & Norm 에 대해 파헤쳐 보도록 하죠!"],"metadata":{"id":"iG2m95PhNVTf"}},{"cell_type":"markdown","source":["---\n","### Layer Normalization\n","---"],"metadata":{"id":"Hdf2-7tENfsE"}},{"cell_type":"markdown","source":["Layer Normalization은 데이터를 Feature 차원에서 정규화를 하는 방법입니다.  \n","또다시 10단어의 Embedding된 문장을 예로 [10, 512]에서 512차원 Feature를 정규화하여 분포를 일정하게 맞춰주는 것이죠.  \n","Layer Normalization 외에도 다양한 Normalization 방법이 있고,  \n","약간 논외지만 Regularization과 Generalization 등 우리를 헷갈리게 하는 많은 방법들이 있습니다.  \n","다행히도 아랫글에서 모든 개념을 쉽게 정리해 주고 있으니, 살펴보시길 적극 권장합니다!\n","\n"],"metadata":{"id":"RDHddSlwNkel"}},{"cell_type":"markdown","source":["---\n","### Residual Connection\n","----"],"metadata":{"id":"vCFz8BU-Sbw0"}},{"cell_type":"markdown","source":["Skip Connection이라고도 부르는 Residual Connection이 처음 제안된 것은 2015년 ResNet이라는 모델과 함께였습니다.  \n","ResNet은 자연어 처리를 공부하는 우리에겐 다소 낯선 이름인데요, 컴퓨터 비전에서는 트랜스포머만큼이나 유명한 모델이랍니다!  \n","모델 자체를 완벽히 이해할 필요는 없지만, 등장 배경이 된 멋진 아이디어는 알아두면 좋습니다."],"metadata":{"id":"VIhkkqdAVf64"}},{"cell_type":"markdown","source":["---\n","### Learning Rate Schedular\n","---"],"metadata":{"id":"aMTkqzyUVrLN"}},{"cell_type":"markdown","source":["트랜스포머를 훈련하는 데에는 Adam Optimizer를 사용했는데,  \n","특이한 점은 Learning Rate를 수식에 따라 변화시키며 사용했다는 것입니다.  \n","\n","수식은 아래와 같습니다. \n","$$lrate=d_{model}^{−0.5} ⋅ min(step_num^{−0.5}, step_num ⋅ warmup_steps^{−1.5}) $$\n","\n","위 수식을 따르게 되면 warmup_steps까지는 lrate가 선형적으로 증가 하고, 이후에는 step_num에 비례해 점차 감소하는 모양새 를 보이게 됩니다."],"metadata":{"id":"8mms4KdCVuhx"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","d_model = 512\n","warmup_steps = 4000\n","\n","lrates = []\n","for step_num in range(1, 50000):\n","    lrate = (np.power(d_model, -0.5)) * np.min(\n","        [np.power(step_num, -0.5), step_num * np.power(warmup_steps, -1.5)])\n","    lrates.append(lrate)\n","\n","plt.figure(figsize=(6, 3))\n","plt.plot(lrates)\n","plt.show()"],"metadata":{"id":"m6ktcbG7E4Ug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이와 같은 Learning Rate를 사용하면 초반 학습이 잘되지 않은 상태에서의 학습 효율이 늘어나고,  \n","어느 정도 학습이 된 후반에는 디테일한 튜닝을 통해 Global Minimum을 찾아가는 효과가 나게 됩니다.  \n","학습의 초반과 후반은 warmup_steps 값에 따라 결정되고요.  \n","직관적으로도 멋진 개념입니다!"],"metadata":{"id":"iG-meke0WnbK"}},{"cell_type":"markdown","source":["---\n","### Weight Sharing\n","---"],"metadata":{"id":"o4Xe3JuyWrxA"}},{"cell_type":"markdown","source":["Weight Sharing은 모델의 일부 레이어가 동일한 사이즈의 Weight를 가질 때 종종 등장하는 테크닉입니다.  \n","하나의 Weight를 두 개 이상의 레이어가 동시에 사용하도록 하는 것인데,  \n","대표적으로 언어 모델의 Embedding 레이어와 최종 Linear 레이어가 동일한 사이즈의 Weight를 가집니다.  \n","프로그래밍 개념 중 Call By Reference와 비슷한 느낌이죠!\n","\n","이는 가볍게 생각했을 때에는 비효율적일 것으로 보입니다.  \n","출력층의 Linear 레이어는 그만의 역할이 있을 것이고 Embedding 레이어 또한 그럴 것인데, \n","둘을 서로 엮어버리면 이도 저도 아닐 것 같은 느낌이 들죠.  \n","하지만 앞서 ResNet이 증명한 것처럼 많은 Weight가 곧 성능으로 이어지지 않고,  \n","외려 Optimization에서 불리한 경향을 보인다는 것을 생각하면 이해가 갈 거예요.  \n","  \n","실제로 Weight Sharing은 튜닝해야 할 파라미터 수가 감소하기 때문에 학습에 더 유리하며 자체적으로 Regularization 되는 효과도 있습니다.  \n","유연성이 제한되어 과적합을 피하기 용이해지거든요!  \n","\n","트랜스포머에서는 Decoder의 Embedding 레이어와 출력층 Linear 레이어의 Weight를 공유하는 방식을 사용했습니다.  \n","소스 Embedding과 타겟 Embedding도 논문상에서는 공유했지만 이는 언어의 유사성에 따라서 선택적으로 사용합니다.  \n","만일 소스와 타겟 Embedding 층까지 공유한다면 3개의 레이어가 동일한 Weight를 사용하는 셈이죠.  \n"," \n","또한 출력층 Linear 레이어와 Embedding 레이어의 Feature 분포가 다르므로 Embedding 된 값에 d_model의 제곱근 값을 곱해줍니다.  \n","이는 분포를 맞춰줌과 동시에 Positional Encoding이 Embedding 값에 큰 영향을 미치는 것을 방지해 줍니다. "],"metadata":{"id":"zZjclcfaXPiE"}},{"cell_type":"markdown","source":["---\n","# 트랜스포머, 그 후엔\n","----"],"metadata":{"id":"MjfGw6cjXhDK"}},{"cell_type":"markdown","source":["트랜스포머가 NLP의 꽃이라는 것은 학계의 수많은 관련 논문들이 증명합니다!  \n","이번 스텝에서는 트랜스포머를 기반으로 발전한 큰 줄기들을 간단하게 소개해 드리겠습니다.\n","\n","가장 먼저 등장한 것은 BERT와 GPT였습니다.  \n","두 모델 다 엄청난 양의 데이터를 기반으로 훈련되어 경이로운 성능을 보여줬죠.  \n","\n","오늘 소개한 트랜스포머는 max_sequence_length를 512에서 1024 정도로만 사용합니다.  \n","그래서 초기에는 주로 몇 개 문장 단위의 번역 작업에 활용되었죠.  \n","문단의 문맥을 파악하는 수준까지의 보다 긴 입력 길이를 처리할 수 있도록,  \n","기존의 트랜스포머에 Recurrence라는 개념을 추가한 Transformer-XL이 제안되기도 했습니다.  \n","\n"],"metadata":{"id":"x4lxhyGNXj9l"}},{"cell_type":"code","source":[""],"metadata":{"id":"3fnJZsZFWp-F"},"execution_count":null,"outputs":[]}]}