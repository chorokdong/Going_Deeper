{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GD1_텍스트 데이터 다루기_예제.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNuqr5+wxwR7QJlTlyZnSAX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["---\n","# 1. 들어가며\n","----"],"metadata":{"id":"OSeF4njMFSmp"}},{"cell_type":"markdown","source":["우리는 수많은 문장 속에서 살아갑니다. 아침에 읽은 신문 기사도 문장, 친구에게 온 메시지도 문장, 지금 읽고 계신 이 글까지도 필자가 작성한 문장이죠.  \n","이 모든 것들을 일상에서 자연히 발생하여 쓰이는 언어, 자연어(Natural language) 라 부릅니다.  \n","\n","그런데, 자연어라는 말의 의미에 대해 깊이 생각해 보셨나요? 그럼 자연어가 아닌 언어에는 무엇이 있을까요?   \n","자연어의 반대말이라면... 인공어(Artificial language)라고나 할까요? 그런 언어가 생각나시나요?   그렇습니다. 프로그래밍 언어(Programming language)야말로 대표적인 인공어라고 할 수 있겠네요.  \n","여러분이 인공지능을 위해 공부하는 파이썬 같은 것들 말입니다.  \n","그렇다면 우리가 일상적으로 사용하는 자연어와 프로그래밍 언어 사이의 본질적인 차이가 무엇인지 혹시 아시나요?  \n","\n","대학교에서 전산학을 전공하셨다면 컴파일러 수업을 들으시면서 언어의 문법에 대한 이론을 접해보셨을 것입니다.  \n","여기서 문맥 자유 문법(Context-Free Grammar), 문맥 의존 문법(Context-Sensitive Grammar)이라는 용어가 나옵니다.  \n","들어보신 적 없으셔도 상관없습니다.  \n","이 개념을 깊숙이 다루는 것은 오늘의 주제 범위에서 벗어나는 일이니까요.  \n","하지만 자연어처리의 본질적 어려움을 이해해 보는 측면에서 아래 링크의 글들을 읽으면서 한번 개념을 정리해 봅시다.  "],"metadata":{"id":"vB-yQbxC0Quf"}},{"cell_type":"markdown","source":["위 두 질문에 대한 답변을 통해 우리는 왜 기계를 통한 자연어처리가 어려운지의 본질적 이유를 파악할 수 있을 것입니다.  \n","프로그래밍 언어에는 파서가 존재합니다. 문법에 맞게 파싱을 했다면 컴퓨터는 우리가 프로그래밍한 내용을 전혀 오해하지 않고 정확하게 우리의 의도대로 동작할 것입니다.  \n","서로 다른 상황(문맥)에서 동작시켜도 같은 동작을 하게 되죠.  \n","모든 프로그래밍 언어는 문맥과 상황에 따라 그 뜻이 달라지지 않는 언어,  \n","다시 말해 문맥을 완전히 배제하고 해석 가능한 언어, 즉 Context-free Grammar를 따르는 언어이기 때문입니다.  \n","\n","그러나 자연어는 이와 다릅니다.  \n","컴퓨터를 통한 자연어처리의 어려움을 보여주는 아래 글을 한번 읽어 봅시다.  \n","이 글은 2016년에 구글에서 선보인 자연어 파서 모델을 소개하면서, 자연어 파싱의 어려움을 함께 설명한 글입니다."],"metadata":{"id":"KCHVTheY0gbK"}},{"cell_type":"markdown","source":["하지만 만약 위 예시 문장의 Alice가 이상한 나라의 앨리스의 Alice라면 어떨까요?  \n","어쩌면 자동차 안에 사람이 다니는 거리가 있을 수도 있지 않을까요?😜  \n","이렇듯 문맥에 따라 얼마든지 해석이 달라질 수 있는 것이 자연어입니다.  \n","그러므로 멋진 자연어처리 모델을 만들려면, 단순히 언어의 문법만으로 충분하지 않고 그 언어의 의미까지 이해하는 과정이 필요합니다.  \n","우리가 앞으로 도전하게 될 자연어처리는 이렇게 만만치 않은 작업입니다.\n","\n","어쩌면 여러분들은 워드 벡터 기법들을 다루어 본 경험이 이미 있을 수도 있을 텐데요.  \n","워드 벡터는 단어 속에 담긴 의미를 어떤 의미 벡터 공간에 매핑하는 기법이었습니다.  \n","사람이 프로그래밍을 통해 파서를 만드는 방식으로는 도저히 해낼 수 없었던 자연어처리 태스크가,  \n","이러한 머신러닝 기법을 통해 단어 속의 의미를 파악하는 과정을 통해 훨씬 정확하게 처리될 수 있는 가능성이 생겼습니다.  \n","\n","하지만 오늘 다루게 될 내용은 자연어를 그렇게 단어의 의미 단위로 쪼개 내는 작업 자체도 만만치 않다는 것을 보여줄 것입니다.  \n","자연어를 의미 단위로 쪼개는 토큰화(Tokenization) 기법은 자연어처리 모델의 성능에 결정적 영향을 미칩니다.  \n","토크나이저가 문장을 단어로 쪼개는 방식에 따라 같은 문장도 완전히 다른 워드 벡터 리스트가 되기 때문입니다.  \n","특히 Wordpiece Model같은 Subword level의 처리 기법은 최신 자연어처리 모델에 필수적으로 자리 잡고 있으므로,  \n","이러한 아이디어가 나오기까지의 자연어 전처리, 토큰화 과정을 이해하는 것은 매우 중요합니다.  \n","\n","앞으로의 코스들을 진행하는 데에 필수적인 테크닉들이니 오늘의 내용을 반드시 숙지하시길 바랍니다!"],"metadata":{"id":"CWqI8V_m2zc_"}},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-1-L-2.max-800x600_Hcen3qw.png'>\n","\n","이상적인 데이터의 형태는 이른바 교과서적인 문장들입니다. 앞으로 배워나가겠지만, 우리가 다루게 될 언어 모델(Language Model)이란 바로 단어가 출현하게 될 확률 모델로 이루어져 있습니다.  \n","그러므로 언어 출현 확률의 일정한 패턴을 통계적으로든 딥러닝을 이용해서든 학습시키려면 문법이나 언어 사용 관습의 일정한 패턴을 따른 텍스트 데이터가 이상적입니다.  \n","\n","하지만 위 예시와 같은 채팅 데이터는 띄어쓰기와 맞춤법, 약어 사용 등에서 문제가 많죠.  \n","우리가 사용하는 일상어들은 교과서적인 문장에서 예외적으로 변형될 여지가 무궁무진합니다.  \n","이러한 변형들이 자연어처리 모델 입장에서는 노이즈가 됩니다.  \n","따라서 아직은 소설책이나 신문 기사같이 맞춤법이 비교적 정확하고 노이즈가 적은 데이터를 사용하는 것이 일반적입니다.  \n","혹은 여러 가지 테크닉으로 노이즈를 제거한 후 사용하는 방법이 있죠!\n","\n","아직은 이라고 표현한 이유는, 위와 같은 데이터들도 양이 무지막지하게 받쳐준다면 모든 변형을 커버할 수 있기 때문입니다.  \n","심지어 교과서만 읽은 인공지능보다 따분하지 않은 언어력을 가질 수 있게 되겠죠!  \n","다만 지금의 인공지능 수준은 서비스 목적에 맞게 데이터를 골라 사용하는 정도기에 아직은 이라는 표현이 적합하다고 봅니다.  \n","\n","그럼 채팅 데이터와 같이 이상적이지 않은 데이터에서 발견할 수 있는 노이즈에는 어떤 유형들이 있을까요?"],"metadata":{"id":"rIX4kGzw3Vbv"}},{"cell_type":"markdown","source":["---\n","# 2. 전처리: 자연어의 노이즈 제거\n","---"],"metadata":{"id":"dsgkCSIiFXPv"}},{"cell_type":"markdown","source":["---\n","### 불완전한 문장으로 구성된 대화의 경우\n","\n","한 문장씩 주고 받는 대화와 달리 메신저는 한 문장을 여러 번에 나눠 전송하거나 여러 문장을 한 번에 전송하는 경우가 있습니다.\n","\n","예1) A: \"아니아니\" \"오늘이 아니고\" \"내일이지\" / B: \"그럼 오늘 사야겠네. 내일 필요하니까?\"\n","\n","-------\n","### 문장의 길이가 너무 길거나 짧은 경우\n","\n","아주 짧은 문장은 의미가 없을 수 있고, 대체로 사용빈도가 높은 리액션에 해당하는 경우가 많아서 언어 모델을 왜곡시킬 우려가 있기 때문에 제외해 주는 게 좋습니다.\n","\n","예1) A: \"ㅋㅋㅋ\", \"ㅠㅠㅠ\"\n","\n","아주 긴 문장은 대화와는 관계가 없는 문장일 수 있습니다.\n","\n","예2) A: \"이 편지는 영국에서부터 시작되어…\"\n","\n","---\n","### 채팅 데이터에서 문장 시간 간격이 너무 긴 경우\n","\n","메신저는 누가 자판을 치는지 모르기 때문에 서로의 말이 얽히게 됩니다. 따라서 서로의 말의 텀이 짧으면 그것은 대화가 아니라 서로 할말만 하는 상태일 수 있습니다.\n","\n","예1) A: \"겨울왕국2\" / B: \"보러가자\" / A: \"엊그제 봤는데 잼씀\" / B: \"오늘 저ㄴ 아니 ㅡㅡ\"\n","\n","혹은 말의 텀이 너무 길다면 그것은 연속된 대화로 보기 어렵습니다.\n","\n","예2) A: \"나 10만원만 빌려줄 수 있어?\" / …… / B: \"아 미안 지금 봤다 아직 필요해?\"\n","\n","---\n","### 바람직하지 않은 문장의 사용\n","\n","욕설의 비율이나, 오타의 비율이 높은 문장은 자연어 모델 학습에 사용하지 않는 것이 좋습니다.\n","\n","---\n","\n"],"metadata":{"id":"-toDt6pM3hcv"}},{"cell_type":"markdown","source":["하지만 지금은 좀 더 단순하고 근본적인 노이즈에 집중하겠습니다. 예를 들면 아래와 같은 문장들이죠.  \n","\n","- Hi, my name is John. (\"Hi,\" \"my\", ..., \"John.\" 으로 분리됨) - 문장부호  \n","- First, open the first chapter. (First와 first를 다른 단어로 인식) - 대소문자  \n","- He is a ten-year-old boy. (ten-year-old를 한 단어로 인식) - 특수문자  \n","\n","대표적인 세 가지 노이즈 유형입니다. 하나하나 해결하며 완벽한 말뭉치를 만들어봅시다!  "],"metadata":{"id":"TQNHJbGZ4Xv-"}},{"cell_type":"markdown","source":["---\n","## 노이즈 유형 (1) 문장부호 : Hi, my name is john.\n","---"],"metadata":{"id":"4c43O5fP4dRn"}},{"cell_type":"markdown","source":["먼저 노이즈 유형 (1) 문장부호입니다. 우리는 문장부호를 배웠으니 Hi, 가 Hi 와 , 의 결합인 것을 알지만 컴퓨터는 명시해 주지 않는다면 알파벳에 , 가 포함되어 있다고 생각할 수도 있겠죠?  \n","문장부호를 단어와 분리하면 해결이 되는 상황이기 때문에 문장부호 양쪽에 공백을 추가하는 방법을 취합시다!"],"metadata":{"id":"YkOIwcsQ4iiF"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RyOtOaAxhir","executionInfo":{"status":"ok","timestamp":1656901931187,"user_tz":-540,"elapsed":314,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"4f42a639-61e1-4131-af4f-48397d579900"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hi ,  my name is john . \n"]}],"source":["def pad_punctuation(sentence, punc):\n","    for p in punc:\n","        sentence = sentence.replace(p, \" \" + p + \" \")\n","\n","    return sentence\n","\n","sentence = \"Hi, my name is john.\"\n","\n","print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"]},{"cell_type":"markdown","source":["Python은 replace() 라는 직관적인 함수를 지원해 주기에, 이를 활용하면 풀고자 하는 문제를 쉽게 풀어낼 수 있습니다!  \n","\n","단순히 문장부호 양옆에 공백을 추가해 주는 방식은 문제를 야기할 수도 있습니다.  \n","이를테면 \"www.semi01sample.com\" 라는 구절이 문장에 포함되어 있다면 \"www . semi01sample . com\" 이라는 요상한 모양새로 변형을 시키겠죠.  \n","\"Mr.Johnson\" 이나 \"45,756\" 같은 경우도 마찬가집니다.\n","\n","이런 경우는 불가피한 손실로 취급하고 넘어갑니다. 모든 규칙에 대해 변환을 정의해 줄 수는 없기 때문이죠.  \n","게다가 이러한 예외들은 보통 우리가 수행하고자 하는 목적에 큰 영향을 주지 못합니다. \"Mr / . / Johnson\" 과 \"Barack / Hussein / Obama\" 는 컴퓨터 입장에서 똑같이 세 단어로 이루어진 이름일 뿐이니까요."],"metadata":{"id":"vH-ZHBG_46KS"}},{"cell_type":"markdown","source":["---\n","## 노이즈 유형 (2) 대소문자 : First, open the first chapter.\n","---"],"metadata":{"id":"PkG57yZR5ATA"}},{"cell_type":"markdown","source":["그다음은 노이즈 유형 (2) 대소문자입니다.  \n","영어에서 발생하는 문제인데, First와 first 는 같은 의미를 갖고 있음에도 컴퓨터는 f와 F를 다르다고 구분 지어 버릴 수 있겠죠.  \n","이를 방지하기 위해 모든 단어를 소문자로 바꾸는 방법을 취하겠습니다.\n","\n","엄연히 말하면 위 예시에서 First와 first는 부사와 한정사로 구분되어 각각의 용도를 배우면 더 좋을 것 같지만, 앞서 언급했듯이 아직은 이른 접근이랍니다!"],"metadata":{"id":"WjYbzKAn5CYH"}},{"cell_type":"code","source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.lower())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4pVv2Un4trz","executionInfo":{"status":"ok","timestamp":1656901931544,"user_tz":-540,"elapsed":15,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"ba40a4df-9cf9-4140-cc47-f8be3a34f54e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["first, open the first chapter.\n"]}]},{"cell_type":"code","source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.upper())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-XFsnhH5JzD","executionInfo":{"status":"ok","timestamp":1656901931544,"user_tz":-540,"elapsed":14,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"72610640-0abf-4a6a-9082-e0d26fa73be0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["FIRST, OPEN THE FIRST CHAPTER.\n"]}]},{"cell_type":"markdown","source":["---\n","## 노이즈 유형 (3) 특수문자 : He is a ten-year-old boy.\n","---"],"metadata":{"id":"T_uBaQR95M9s"}},{"cell_type":"markdown","source":["마지막인 노이즈 유형 (3) 특수문자입니다. ten-year-old와 seven-year-old,  \n","그 외의 수많은 나이 표현들을 각각의 단어 취급을 해버리는 일이 벌어지겠죠.  \n","그런 불상사를 방지하기 위해 특수문자를 제거하려 합니다.  \n","하지만 무한한 특수문자를 모두 정의하여 제거할 수는 없겠죠?  \n","따라서 우리는 사용할 알파벳과 기호들을 정의해 이를 제외하곤 모두 제거하도록 하겠습니다."],"metadata":{"id":"q7P_OLMN5Sun"}},{"cell_type":"code","source":["import re\n","\n","sentence = \"He is a ten-year-old boy.\"\n","sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n","\n","print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjYGs5715LgO","executionInfo":{"status":"ok","timestamp":1656901931545,"user_tz":-540,"elapsed":13,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"adeaee04-5e43-41cf-c317-b37cff9fe8a8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["He is a ten year old boy.\n"]}]},{"cell_type":"markdown","source":["re 패키지는 정규표현식 사용을 도와주는 패키지입니다. 정규표현식을 사용하면 어떤 문장을 규칙에 따라 일반화하는 것이 굉장히 편해집니다.  \n","대신 처음 접하는 분들께는 기호들이 다소 난해하게 다가오죠... 이에 대한 자세한 설명이 궁금하시다면 아래 페이지를 방문해보세요!"],"metadata":{"id":"EBY_o2x05ZXJ"}},{"cell_type":"code","source":["# From The Project Gutenberg\n","# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n","\n","corpus = \\\n","\"\"\"\n","In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n","But my teacher had been with me several weeks before I understood that everything has a name.\n","One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n","Some one was drawing water and my teacher placed my hand under the spout. \n","As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n","I stood still, my whole attention fixed upon the motions of her fingers. \n","Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n","I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n","That living word awakened my soul, gave it light, hope, joy, set it free! \n","There were barriers still, it is true, but barriers that could in time be swept away.\n","\"\"\" \n","\n","def cleaning_text(text, punc, regex):\n","    # 노이즈 유형 (1) 문장부호 공백추가\n","    for p in punc:\n","        text = text.replace(p, \" \" + p + \" \")\n","\n","    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n","    text = re.sub(regex, \" \", text).lower()\n","\n","    return text\n","\n","print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vIMr7Is5XF4","executionInfo":{"status":"ok","timestamp":1656901931545,"user_tz":-540,"elapsed":12,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"cad881b1-d4d2-4aa0-be5d-5241f1251492"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n","but my teacher had been with me several weeks before i understood that everything has a name . \n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n","some one was drawing water and my teacher placed my hand under the spout .  \n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n","i stood still ,  my whole attention fixed upon the motions of her fingers .  \n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n","there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n","\n"]}]},{"cell_type":"markdown","source":["---\n","# 3. 분산표현 : 바나나와 사과의 관계를 어떻게 표현할까?\n","---"],"metadata":{"id":"cxMTeTzo5qmS"}},{"cell_type":"markdown","source":["---\n","### 단어의 희소 표현과 분산 표현\n","---\n","\n","임베딩 레이어(Embedding Layer)를 통해 단어의 분산 표현(distributed representation) 을 구현할 수 있는데요.  \n","임베딩 레이어는 자연어 처리 분야에서 거의 기본에 해당하는 요소입니다. 그럼 **분산 표현**에 대해 알아 보겠습니다.  \n","\n","분산 표현과 반대되는 표현으로 **희소 표현(Sparse representation)** 이라는 것이 있습니다.  \n","어느 표현이든 단어를 벡터로 표현하겠다는 점에서는 동일합니다.  \n","하지만, 단어의 의미를 표현하는 접근 방식에서 큰 차이가 있습니다.  \n","희소 표현방식은 벡터의 각 차원마다 단어의 특정 의미 속성을 대응시키는 방식입니다. 예를 들자면 아래와 같습니다.\n","\n","사람의 성별을 표현하는 남자와 여자라는 두 단어를 수로 표현하려면 어떻게 하면 될까요?  \n","예컨대 남자: [-1], 여자: [1] 혹은 남자: [1], 여자: [-1] 의 형태로 표현해 볼 수 있겠네요. 전자로 생각을 하도록 하죠.\n","\n","다음으로 등장한 단어는 소년과 소녀입니다.  \n","두 단어는 각각 어린 남자와 여자를 의미하니, 앞서 생성한 \"성별\" 이라는 속성에 \"나이가 어리다\" 라는 속성을 추가해야겠습니다.  \n","즉, 소년: [-1, -1], 소녀: [1, -1] 이 되겠군요! \"나이가 많다\" 라는 속성을 가진 할아버지,  \n","할머니는 자동적으로 할아버지: [-1, 1], 할머니: [1, 1] 이 되는 것도 알 수 있습니다.  \n","\n","잠깐, 그러면 아저씨와 아줌마는 어떻게 표현하죠..?  \n","아하, 각 속성값들이 정수가 아니라 실수라면 표현이 가능하겠네요! 1차적으로 완성된 우리의 희소 표현 방식의 단어 사전은 아래와 같습니다."],"metadata":{"id":"i97vCgOU5u7t"}},{"cell_type":"markdown","source":["- 분산 표현(distributed representation)\n","\n","  - 희소표현과 비교했을때 데이터 공간을 절약할수 있으며, 유사도를 파악하기에 효율적이다.  \n","​분산 표현의 대표적인 예시 (RGB)\n","\n","  - Red, Green, Blue의 색을 256가지의 값을 넣어 무한에 가까운 새로운 색을 만들어 내는데 이러한 방식으로 원핫 인코딩보다 데이터간의 의미를 파악하기 쉬우며 엄청난 압축률을 보여준다.\n","  - 고양이와 개를 예를 든 분산 표현  \n","    Cat [0.583, 0.157, 0.947]   \n","    Dog [0.583, 0.347, 0.934] \n"],"metadata":{"id":"RzWFwZ-o8sdP"}},{"cell_type":"markdown","source":["    {\n","        //     [성별, 연령]\n","        남자: [-1.0, 0.0], // 이를테면 0.0 이 \"관계없음 또는 중립적\" 을 의미할 수 있겠죠!\n","        여자: [1.0, 0.0],\n","        소년: [-1.0, -0.7],\n","        소녀: [1.0, -0.7],\n","        할머니: [1.0, 0.7],\n","        할아버지: [-1.0, 0.7],\n","        아저씨: [-1.0, 0.2],\n","        아줌마: [1.0, 0.2]\n","    }\n","\n","지금 우리가 한 것이 바로 희소 표현(Sparse representation) 입니다.  \n","단어를 고차원 벡터로 변환하는 것이죠. 사람을 나이와 성별로 구분하기 위해선 적어도 2차원이 필요함을 배웠네요!  \n","2차원이라 뭐가 고차원인지, 뭐가 희소하다는 건지 느낌이 안 오시나요? 그럼 좀 더 진행해 볼까요?\n","\n","위 예시는 모든 단어들이 \"사람\"이라는 속성을 가짐 을 전제하죠?  \n","하지만 세상엔 바나나도 있고 사과도 있답니다.  \n","\"과일인지 아닌지 판단\" 하는 속성을 추가하고, \"색깔\" 속성을 추가해서 이번엔 바나나와 사과를 구분해보죠!\n","\n","    {\n","        //      [성별, 연령, 과일, 색깔]\n","        남자: [-1.0, 0.0, 0.0, 0.0],\n","        여자: [1.0, 0.0, 0.0, 0.0],\n","        사과: [0.0, 0.0, 1.0, 0.5],   // 빨갛게 잘 익은 사과\n","        바나나: [0.0, 0.0, 1.0, -0.5] // 노랗게 잘 익은 바나나\n","    }\n","\n","어떤가요? 속성의 종류가 늘어나고 워드 벡터의 차원이 따라서 늘어나기 시작하니까 벡터에서 0.0이 자주 나오기 시작하는 것이 보이시나요?  \n","4차원까지는 우리의 직관으로 늘려보았지만, 우리가 이런 방식으로 세상의 모든 단어의 속성을 규명해서 표현하려면 과연 몇 차원의 벡터가 필요할까요?  \n","이번 생에는 어려울지도 모르겠네요.  \n","엄청난 고차원의 벡터를 이런 방식으로 만들어낸다 한들, 워드 벡터의 대부분의 차원은 0.0으로 채워진 희소 표현이 만들어질 것입니다.\n","\n","이런 방식의 문제가 무엇일까요?  \n","너무 고차원이 필요하고 불필요한 메모리와 연산량이 낭비된다는 점 말고도  \n","중요한 문제점 하나는, 희소 표현의 워드 벡터끼리는 단어들 간의 의미적 유사도를 계산할 수 없다는 점입니다.\n","\n","두 고차원 벡터의 유사도는 코사인 유사도(Cosine Similarity) 를 통해 구할 수 있습니다.  \n","자세한 내용이 궁금하다면 아래 웹페이지를 확인하세요!\n","\n"],"metadata":{"id":"MTxSba8N9-vm"}},{"cell_type":"code","source":["#################### 코사인 유사도 구하는 법 ####################\n","import numpy as np\n","from numpy import dot\n","from numpy.linalg import norm\n","\n","word_1 = np.array([-1.0, 0.0, 0.0, 0.0])\n","word_2 = np.array([0.0, 0.0, 1.0, 0.5])\n","\n","def cos_sim(A, B):\n","  return dot(A, B)/(norm(A)*norm(B))\n","\n","print(cos_sim(word_1,word_2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oHVbqm75hmS","executionInfo":{"status":"ok","timestamp":1656901931545,"user_tz":-540,"elapsed":10,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"faa77162-f6cb-4031-8e58-fea90064b55d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}]},{"cell_type":"markdown","source":["그래서 우린 Embedding 레이어를 사용해 각 단어가 몇 차원의 속성을 가질지 정의하는 방식으로 단어의 분산 표현(distributed representation) 를 구현하는 방식을 주로 사용하게 됩니다.  \n","만약 100개의 단어를 256차원의 속성으로 표현하고 싶다면 Embedding 레이어는 아래와 같이 정의되겠죠.  \n","\n","    embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)\n","\n","위 단어의 분산 표현에는 우리가 일일이 정의할 수 없는 어떤 추상적인 속성들이 256차원 안에 골고루 분산되어 표현됩니다.  \n","희소 표현처럼 속성값을 임의로 지정해 주는 것이 아니라, 수많은 텍스트 데이터를 읽어가며 적합한 값을 찾아갈 것입니다.  \n","적절히 훈련된 분산 표현 모델을 통해 우리는 단어 간의 의미 유사도를 계산하거나,  이를 feature로 삼아 복잡한 자연어처리 모델을 훈련시킬 수 있게 됩니다."],"metadata":{"id":"LBcOe5kY_Wxb"}},{"cell_type":"markdown","source":["---\n","### 단어 사전 구성과 활용의 문제\n","---\n","\n","하지만 여기서 짚고 넘어가야 할 점이 있습니다.  \n","위에서 Embedding 레이어를 사용해 구현한 분산 표현은 컴퓨터 입장에서는 단어 사전이 될 것입니다.   \n","그런데 만약 사전만 가지고 외국에 나가면 바로 그 나라 말을 알아듣고 해석할 수 있을까요?  \n","아마 단어 단위로 명확하게 끊어서 들리지조차 않을 것이기 때문에 단어 사전이 있다 한들 아무런 소용이 없게 되지 않을까요?  \n","\n","컴퓨터한테도 비슷한 일이 벌어지곤 합니다.  \n","우리는 마치 문장이 단어 단위로 명확하게 쪼개져 있어서, 컴퓨터가 각 단어에 해당하는 분산 표현을 언제든 정확히 찾을 수 있을 것처럼 전제하고 위와 같은 컴퓨터용 단어 사전을 구축해 두었지만,  \n","정작 컴퓨터는 전혀 엉뚱한 단어로 해석하거나 혹은 사전에서 그 단어를 찾지 못해 당황하는 일이 생기게 됩니다.  \n","그것은 바로 컴퓨터가 문장을 단어 단위로 정확하게 끊어 읽지 못하기 때문에 벌어지는 일입니다.  \n","다음 스텝에서 이 문제의 어려움을 좀 더 자세하게 다루어 봅시다."],"metadata":{"id":"RDH9FtTK_oZS"}},{"cell_type":"markdown","source":["---\n","# 4. 토큰화 : 그녀는? 그녀+는?\n","---\n","\n","문장을 어떤 기준으로 쪼개었을 때, 쪼개진 각 단어들을 토큰(Token) 이라고 부릅니다.  \n","그리고 그 쪼개진 기준이 토큰화(Tokenization) 기법에 의해 정해지죠.  \n","이번 스텝에서는 토큰화의 여러 가지 기법에 대해 배워보도록 하겠습니다."],"metadata":{"id":"oLkQslKM_9On"}},{"cell_type":"markdown","source":["---\n","### 공백 기반 토큰화\n","---\n","\n","자연어의 노이즈를 제거하는 방법 중 하나로 우리는 Hi, 를 Hi와 ,로 나누기 위해 문장부호 양옆에 공백을 추가해 주었습니다.  \n","그것은 이 공백 기반 토큰화를 사용하기 위해서였죠! 당시의 예제 코드를 다시 가져와 공백을 기반으로 토큰화를 진행해 보겠습니다."],"metadata":{"id":"GuuWoHc4BgFL"}},{"cell_type":"code","source":["corpus = \\\n","\"\"\"\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n","but my teacher had been with me several weeks before i understood that everything has a name . \n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n","some one was drawing water and my teacher placed my hand under the spout .  \n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n","i stood still ,  my whole attention fixed upon the motions of her fingers .  \n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n","there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n","\"\"\"\n","\n","tokens = corpus.split()\n","\n","print(\"문장이 포함하는 Tokens:\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKyuXhVp-2c4","executionInfo":{"status":"ok","timestamp":1656901931546,"user_tz":-540,"elapsed":10,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"eb63fe22-796e-43cb-ec33-d98250cf2243"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"]}]},{"cell_type":"markdown","source":["첫 번째로 배우는 가장 간단한 토큰화 기법이니만큼 사용법도 아주 간단합니다.  \n","split() 함수를 호출하기만 하면 자동으로 토큰들의 List로 만들어주죠.\n","\n","단어 사전에 위 단어들이 전부 등재되어 있다면야 썩 나쁘지 않겠죠?  \n","사실상 같은 단어인 days 와 day 가 구분되어 따로 저장이 되는 정도의 문제는 있겠지만,  \n","s 를 모두 한 칸 띌 수도 없는 노릇이기에 불가피한 손실로 취급합니다(후에 등장할 방법에서 해결될 문제입니다!)."],"metadata":{"id":"FD98-7tdBquo"}},{"cell_type":"markdown","source":["---\n","### 형태소 기반 토큰화\n","---\n","\n","하지만 우리에겐 영어 문장이 아닌 한국어 문장을 처리할 일이 더 많을 것이고,  \n","한국어 문장은 공백 기준으로 토큰화를 했다간 엉망진창의 단어들이 등장하는 것을 알 수 있습니다.   \n","문장부호처럼 \"은 / 는 / 이 / 가\" 양옆에 공백을 붙이자구요?  \n","글쎄요... 가로 시작하는 단어만 해도 가면, 가위, 가족, 가수... 의도치 않은 변형이 너무나도 많이 일어날 것 같네요!\n","\n","이를 어떻게 해결할 수 있을까요? 정답은 형태소에 있습니다.  \n","어릴 적 국어 시간에 배운 기억이 새록새록 나시나요? 상기시켜드리면 형태소의 정의는 아래와 같습니다.\n","\n","(명사) 뜻을 가진 가장 작은 말의 단위.\n","\n","예를 들어, 오늘도 공부만 한다 라는 문장이 있다면,  \n","오늘, 도, 공부, 만, 한다 로 쪼개지는 것이 바로 형태소죠. 한국어는 이를 활용해 토큰화를 할 수 있습니다!\n","\n","한국어 형태소 분석기에서 대표적인 KoNLPy를 사용하여 봅시다.\n","\n","KoNLPy는 내부적으로 5가지의 형태소 분석 Class를 포함하고 있습니다.  \n","형태소 분석기들은 특수한 문장(띄어쓰기 X / 오탈자) 처리 성능, 속도 측면에서 차이를 보입니다.   천하무적인 것은 (아직은) 없으니, 각 분석기를 직접 테스트해보고 적합한 것을 선택해 사용하면 됩니다.\n","\n","아래 웹페이지에서 그 실험을 대신해 주었네요. 정말 좋은 참고 자료입니다!"],"metadata":{"id":"Ybe-nhjqBuXN"}},{"cell_type":"code","source":["# !pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aEEuomHXDLAl","executionInfo":{"status":"ok","timestamp":1656901945803,"user_tz":-540,"elapsed":5245,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"1b90fb9c-dfd9-4c8f-fad7-b3771b569df9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n","\u001b[K     |████████████████████████████████| 453 kB 88.1 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Mecab\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n","%cd Mecab-ko-for-Google-Colab/\n","!bash install_mecab-ko_on_colab190912.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71QreOUrDch7","executionInfo":{"status":"ok","timestamp":1656902314321,"user_tz":-540,"elapsed":187941,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"fcae3148-b736-476d-8404-a80ec39a37a2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Mecab-ko-for-Google-Colab'...\n","remote: Enumerating objects: 115, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 115 (delta 11), reused 10 (delta 3), pack-reused 91\u001b[K\n","Receiving objects: 100% (115/115), 1.27 MiB | 6.99 MiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n","/content/Mecab-ko-for-Google-Colab\n","Installing konlpy.....\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Done\n","Installing mecab-0.996-ko-0.9.2.tar.gz.....\n","Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n","from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","--2022-07-04 02:35:29--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c5:2ef4, 2406:da00:ff00::6b17:d1f5, ...\n","Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNEYW2BKT4&Expires=1656903930&x-amz-security-token=FwoGZXIvYXdzEGwaDAKEjouKYHyACGO6%2FCK%2BAYFOiXL3tsfezBXoJiy8fMHDzfuYkWfkEEPQWtOOX0KWUZpDmMqljurDgyBXy%2BplVWlxWJTtCv9Vpt5Jxx0VKre6J3naoiWVUwmX8UvbvniHOCQnoj7I7Lum6GLQVJWdFp1sm%2F%2BktPz%2Fzv9BCYs6A9X8Ly2OtE7fWifP%2FHm1wiFZHtcfGrV8xU1dXgTNZ9RdSyL9l66ooyHrNxM16sVbGyq5gzaf8qO6%2FyfCvlGfFakGo8xyVCBUx%2BCAaSFjiW4o8qOJlgYyLeVMiN6MxVIKrHOb6qsx88keBssKVLLPOHGWVT1N3JCcUv5kPjBjK33boN7O2Q%3D%3D&Signature=%2BViPh%2FTAatbWP%2FsHoG7%2F4yBPbJU%3D [following]\n","--2022-07-04 02:35:30--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNEYW2BKT4&Expires=1656903930&x-amz-security-token=FwoGZXIvYXdzEGwaDAKEjouKYHyACGO6%2FCK%2BAYFOiXL3tsfezBXoJiy8fMHDzfuYkWfkEEPQWtOOX0KWUZpDmMqljurDgyBXy%2BplVWlxWJTtCv9Vpt5Jxx0VKre6J3naoiWVUwmX8UvbvniHOCQnoj7I7Lum6GLQVJWdFp1sm%2F%2BktPz%2Fzv9BCYs6A9X8Ly2OtE7fWifP%2FHm1wiFZHtcfGrV8xU1dXgTNZ9RdSyL9l66ooyHrNxM16sVbGyq5gzaf8qO6%2FyfCvlGfFakGo8xyVCBUx%2BCAaSFjiW4o8qOJlgYyLeVMiN6MxVIKrHOb6qsx88keBssKVLLPOHGWVT1N3JCcUv5kPjBjK33boN7O2Q%3D%3D&Signature=%2BViPh%2FTAatbWP%2FsHoG7%2F4yBPbJU%3D\n","Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.192.137\n","Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.192.137|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1414979 (1.3M) [application/x-tar]\n","Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n","\n","mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.58MB/s    in 0.4s    \n","\n","2022-07-04 02:35:31 (3.58 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n","\n","Done\n","Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n","Done\n","Change Directory to mecab-0.996-ko-0.9.2.......\n","installing mecab-0.996-ko-0.9.2.tar.gz........\n","configure\n","make\n","make check\n","make install\n","ldconfig\n","Done\n","Change Directory to /content\n","Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n","from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n","--2022-07-04 02:36:43--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n","Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::22e9:9f55, ...\n","Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNMRZWRUGA&Expires=1656904004&x-amz-security-token=FwoGZXIvYXdzEGwaDGr2%2BxjSl1UYG%2Fi3qyK%2BAV9YQFUhYaj4Q%2B48nBNWW0AH6z4HblnypJqmx8zgAuRU3I39VV9iNYItv7iCVnUjPPn5cj%2FyNMlN9eAMt5fCFwlI1wrqqqNMjp31DBuB5MsmYDaWdHc4O%2F3lbvFVXR2Pr%2F49eKFnEaRU2ChIx%2BWAAO2pjw8NgnwjPz%2Fn5jPzyBIodsBFPMlxYdC2zHpKADadqxvSl40Dsgzh784aI8HnoUbthLzRVTMFe96RhZje4x6iq%2Fln2thX61UXFpNQexkovKSJlgYyLbMPndjm8op2cJfabI9jzUQ4meywkpN0sVFiHXx80tsqWIFwvIdJgg%2BC9mLyIg%3D%3D&Signature=mpu%2B2tYg0ttKcaCpT0IZW0%2BdwXc%3D [following]\n","--2022-07-04 02:36:44--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNMRZWRUGA&Expires=1656904004&x-amz-security-token=FwoGZXIvYXdzEGwaDGr2%2BxjSl1UYG%2Fi3qyK%2BAV9YQFUhYaj4Q%2B48nBNWW0AH6z4HblnypJqmx8zgAuRU3I39VV9iNYItv7iCVnUjPPn5cj%2FyNMlN9eAMt5fCFwlI1wrqqqNMjp31DBuB5MsmYDaWdHc4O%2F3lbvFVXR2Pr%2F49eKFnEaRU2ChIx%2BWAAO2pjw8NgnwjPz%2Fn5jPzyBIodsBFPMlxYdC2zHpKADadqxvSl40Dsgzh784aI8HnoUbthLzRVTMFe96RhZje4x6iq%2Fln2thX61UXFpNQexkovKSJlgYyLbMPndjm8op2cJfabI9jzUQ4meywkpN0sVFiHXx80tsqWIFwvIdJgg%2BC9mLyIg%3D%3D&Signature=mpu%2B2tYg0ttKcaCpT0IZW0%2BdwXc%3D\n","Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 3.5.16.182\n","Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|3.5.16.182|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 49775061 (47M) [application/x-tar]\n","Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n","\n","mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  15.5MB/s    in 3.1s    \n","\n","2022-07-04 02:36:48 (15.5 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n","\n","Done\n","Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n","Done\n","Change Directory to mecab-ko-dic-2.1.1-20180720\n","Done\n","installing........\n","configure\n","make\n","make install\n","apt-get update\n","apt-get upgrade\n","apt install curl\n","apt install git\n","bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","Done\n","Successfully Installed\n","Now you can use Mecab\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n","NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n","블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"]}]},{"cell_type":"code","source":["#################### konlpy 및 Mecab의 설치 여부를 확인 ##################\n","from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n","print(\"슝\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNkBmUNFBk7I","executionInfo":{"status":"ok","timestamp":1656901948192,"user_tz":-540,"elapsed":3,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"662b86ee-6068-40b7-8c60-13b286877720"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["슝\n"]}]},{"cell_type":"code","source":["tokenizer_list = [Hannanum(),Kkma(),Komoran(),Okt(),Mecab()]\n","\n","kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n","\n","for tokenizer in tokenizer_list:\n","    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SC9AxUcNDVCv","executionInfo":{"status":"ok","timestamp":1656902412947,"user_tz":-540,"elapsed":19358,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"d5cb95ea-4814-4e99-ad58-0c8ea00ec6ae"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[Hannanum] \n","[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n","[Kkma] \n","[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n","[Komoran] \n","[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n","[Okt] \n","[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"]}]},{"cell_type":"markdown","source":["---\n","### 사전에 없는 단어의 문제\n","---\n","\n","지금까지 배운 공백 기반이나, 형태소 기반의 토큰화 기법들은 모두 의미를 가지는 단위로 토큰을 생성합니다.   \n","이 기법의 경우, 데이터에 포함되는 모든 단어를 처리할 수는 없기 때문에 자주 등장한 상위 N개의 단어만 사용하고 나머지는 <unk>같은 특수한 토큰(Unknown Token)으로 치환해버립니다.  \n","자원이 무한하고 데이터도 무한하다면 그러지 않아도 되겠지만... 모든 것은 한정되어 있기 때문에 이 또한 불가피한 손실이죠. 하지만 이것은 종종 큰 문제를 야기합니다.\n","\n","    코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤\n","    전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.\n","    →\n","    <unk>는 2019년 12월 중국 <unk>에서 처음 발생한 뒤\n","    전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.\n","\n","만약 위 문장을 영문으로 번역해야 한다면 어떨까요?  \n","핵심인 단어 코로나바이러스와 우한을 모른다면 제대로 해낼 수 있을 리가 없습니다.  \n","이를 OOV(Out-Of-Vocabulary) 문제라고 합니다.  \n","이처럼 새로 등장한(본 적 없는) 단어에 대해 약한 모습을 보일 수밖에 없는 기법들이기에,  \n","이를 해결하고자 하는 시도들이 있었습니다.  \n","그리고 그것이 우리가 다음 스텝에서 배울, Wordpiece Model이죠!"],"metadata":{"id":"9inpjazUEnE-"}},{"cell_type":"markdown","source":["---\n","# 5. 토큰화 : 다른 방법들\n","---"],"metadata":{"id":"bCYDjDfcFI8a"}},{"cell_type":"markdown","source":["Wordpiece Model(WPM) 은 우리가 접한 적이 있는 아이디어를 기반으로 만들어졌습니다.  \n","두 단어 preview와 predict를 보면 접두어인 pre가 공통되고 있죠?  \n","pre가 들어간 단어는 주로 \"미리\", \"이전의\" 와 연계되는 의미를 가지고 있습니다.  \n","컴퓨터도 두 단어를 따로 볼 게 아니라 pre+view와 pre+dict로 본다면 학습을 더 잘 할 수 있지 않을까요?  \n","\n","이처럼 한 단어를 여러 개의 Subword의 집합으로 보는 방법이 WPM입니다.  \n","WPM의 원리를 알기 전, 먼저 알아야 할 것이 바로 Byte Pair Encoding(BPE) 입니다."],"metadata":{"id":"5LFxF-WRFhkf"}},{"cell_type":"markdown","source":["---\n","### Byte Pair Encoding(BPE)\n","---\n","\n","BPE 알고리즘이 고안된 것은 1994년입니다. 그때는 자연어 처리에 적용하기 위해서가 아니라 데이터 압축을 위해서 생겨났었죠.  \n","데이터에서 가장 많이 등장하는 바이트 쌍(Byte Pair) 을 새로운 단어로 치환하여 압축하는 작업을 반복하는 방식으로 동작합니다.  \n","예시는 아래와 같습니다.\n","\n","    aaabdaaabac # 가장 많이 등장한 바이트 쌍 \"aa\"를 \"Z\"로 치환합니다.\n","    → \n","    ZabdZabac   # \"aa\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n","    Z=aa        # 그다음 많이 등장한 바이트 쌍 \"ab\"를 \"Y\"로 치환합니다.\n","    → \n","    ZYdZYac     # \"ab\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n","    Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면\n","    Y=ab        # 가장 많이 등장한 바이트 쌍 \"ZY\"를 \"X\"로 치환합니다.\n","    → \n","    XdXac\n","    Z=aa\n","    Y=ab\n","    X=ZY       # 압축이 완료되었습니다!\n","\n","아주 직관적인 알고리즘이죠? 이를 토큰화에 적용하자고 제안한 것은 2015년이었습니다.  \n","모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치면, 접두어나 접미어의 의미를 캐치할 수 있고,  \n","처음 등장하는 단어는 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 완전히 해결할 수 있다는 것이죠!\n","\n","비교적 최근의 기술을 소개해드리는 만큼 논문을 함께 첨부합니다."],"metadata":{"id":"EYv1AAliFmPd"}},{"cell_type":"markdown","source":["위 논문은 Python 소스 코드를 함께 제공해 주어 간편하게 실습을 해 볼 수 있습니다.  \n","논문에서 제공해 주는 예제로 동작 방식을 자세히 들여다보죠!"],"metadata":{"id":"i6rTYqr7GCjJ"}},{"cell_type":"code","source":["import re, collections\n","\n","# 임의의 데이터에 포함된 단어들입니다.\n","# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n","vocab = {\n","    'l o w '      : 5,\n","    'l o w e r '  : 2,\n","    'n e w e s t ': 6,\n","    'w i d e s t ': 3\n","}\n","\n","num_merges = 5\n","\n","def get_stats(vocab):\n","    \"\"\"\n","    단어 사전을 불러와\n","    단어는 공백 단위로 쪼개어 문자 list를 만들고\n","    빈도수와 쌍을 이루게 합니다. (symbols)\n","    \"\"\"\n","    pairs = collections.defaultdict(int) # defaultdict = 딕셔너리와 동일하며 키값을 지정해주지 않으면 기본값이 0으로 설정됨 \n","    \n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","\n","        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n","            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n","        \n","    return pairs\n","\n","def merge_vocab(pair, v_in):\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair)) \n","    # escape = escape() 함수는 알파벳과 숫자 및 * , @, - , _ , + , . , / 를 제외한 문자를 모두 16진수 문자로 변경 \n","    # 이 함수는 쉼표와 세미콜론 같은 문자가 쿠키문자열과의 충돌을 피하기 위해 사용\n","    \n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    \n","    for word in v_in:\n","        w_out = p.sub(''.join(pair), word)\n","        v_out[w_out] = v_in[word]\n","        \n","    return v_out, pair[0] + pair[1]\n","\n","token_vocab = []\n","\n","for i in range(num_merges):\n","    print(\">> Step {0}\".format(i + 1))\n","    \n","    pairs = get_stats(vocab)\n","    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n","    vocab, merge_tok = merge_vocab(best, vocab)\n","    print(\"다음 문자 쌍을 치환:\", merge_tok)\n","    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n","    \n","    token_vocab.append(merge_tok)\n","    \n","print(\"Merge Vocab:\", token_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtIYcw0uDXW3","executionInfo":{"status":"ok","timestamp":1656903594274,"user_tz":-540,"elapsed":3,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"ef8a330c-37e2-4ba5-ea3e-cd446552ed58"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Step 1\n","다음 문자 쌍을 치환: es\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n","\n",">> Step 2\n","다음 문자 쌍을 치환: est\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 3\n","다음 문자 쌍을 치환: lo\n","변환된 Vocab:\n"," {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 4\n","다음 문자 쌍을 치환: low\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 5\n","다음 문자 쌍을 치환: ne\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n","\n","Merge Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"]}]},{"cell_type":"markdown","source":["만일 lowest라는 처음 보는 단어가 등장하더라도, 위 알고리즘을 따르면 어느 정도 의미가 파악된 low와 est의 결합으로 표현할 수 있습니다.  \n","또 BPE의 놀라운 점은 아무리 큰 데이터도 원하는 크기로 OOV 문제없이 사전을 정의할 수 있다는 것입니다.  \n","극단적으로 생각했을 때 알파벳 26개와 특수문자, 문장부호를 아무리 추가해도 100개 이내로 사전을 정의할 수 있죠.(물론 그러면 안 됩니다!!)\n","\n","Embedding 레이어는 단어의 개수 x Embedding 차원 수 의 Weight를 생성하기 때문에 단어의 개수가 줄어드는 것은 곧 메모리의 절약으로 이어집니다.  \n","많은 데이터가 곧 정확도로 이어지기 때문에 이런 기여는 굉장히 의미가 있습니다!\n","\n","하지만 아직도! 완벽하다고는 할 수 없습니다.  \n","만약 수많은 데이터를 사용해 만든 BPE 사전으로 모델을 학습시키고 문장을 생성하게 했다고 합시다.  \n","그게 [i, am, a, b, o, y, a, n, d, you, are, a, gir, l]이라면, 어떤 기준으로 이들을 결합해서 문장을 복원하죠?  \n","몽땅 한꺼번에 합쳤다간 끔찍한 일이 벌어질 것만 같습니다..."],"metadata":{"id":"fcbTtrIwKsUQ"}},{"cell_type":"markdown","source":["---\n","### Wordpiece Model(WPM)\n","---"],"metadata":{"id":"77EwzEb9K2l0"}},{"cell_type":"markdown","source":["이에 구글에서 BPE를 변형해 제안한 알고리즘이 바로 WPM입니다. WPM은 BPE에 대해 두 가지 차별성을 가집니다.\n","\n","- 공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가합니다.  \n","- 빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합칩니다.  \n","\n","첫 번째 문항은 아주 쉬운 내용으로,  \n","앞서 사용한 예문을 빌리면 [_i, _am, _a, _b, o, y, _a, n, d, _you, _are, _a, _gir, l]로 토큰화를 한다는 것입니다.  \n","이렇게 하면 문장을 복원하는 과정이 1) 모든 토큰을 합친 후, 2) 언더바 _를 공백으로 치환으로 마무리되어 간편하죠.\n","\n","두 번째 문항은 다소 난해하게 다가올 수도 있습니다.  \n","직관적인 이해를 얻고 넘어가는 것을 목표로 하죠. 본 내용은 아래 논문 3절과 4절에 자세하게 나와 있습니다.\n","\n","(여기서 잠깐)\n"," 구글에서 이 기법을 한국어, 일본어 텍스트 처리를 위해 고려했다는 사실이 흥미롭지 않나요? 이 점은 2가지를 시사합니다.\n","\n","- 조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안이 될 수 있다.\n","- WPM은 어떤 언어든 무관하게 적용 가능한 language-neutral하고 general한 기법이다.  \n","  한국어 형태소 분석기처럼 한국어에만 적용 가능한 기법보다 훨씬 활용도가 크다.  \n","\n","토큰화의 끝판왕으로 보이는 이 WPM은 아쉽게도 공개되어 있지는 않습니다.  \n","대신에 구글의 SentencePiece 라이브러리를 통해 고성능의 BPE를 사용할 수 있습니다!  \n","SentencePiece에는 전처리 과정도 포함되어 있어서, 데이터를 따로 정제할 필요가 없어 간편하기까지 합니다."],"metadata":{"id":"g6VHG_kxLCHC"}},{"cell_type":"markdown","source":["---\n","### soynlp\n","---"],"metadata":{"id":"Oqpj4lo7MWo-"}},{"cell_type":"markdown","source":["이외에도 한국어를 위한 토크나이저로 soynlp를 활용할 수 있습니다.  \n","soynlp는 한국어 자연어 처리를 위한 라이브러리인데요. 토크나이저 외에도 단어 추출, 품사 판별, 전처리 기능도 제공합니다.\n","\n","형태소 기반의 토크나이저가 미등록 단어에 취약하기 때문에 WordPiece Model을 사용하는 것처럼,  \n","형태소 기반인 koNLPy의 단점을 해결하기 위해 soynlp를 사용할 수 있습니다.\n","\n","soynlp의 토크나이저는 '학습데이터를 이용하지 않으면서 데이터에 존재하는 단어를 찾거나, 문장을 단어열로 분해, 혹은 품사 판별을 할 수 있는 비지도학습 접근법을 지향합니다' 라고 밝히고 있는데요.\n","\n","문장에서 처음 단어를 받아들일 때 단어의 경계를 알아야 올바르게 토큰화를 할 수 있습니다.  \n","이때 단어의 경계를 비지도학습을 통해 결정하겠다는 말이에요.  \n","비지도학습을 통한 방법이기 때문에 미등록 단어도 토큰화가 가능합니다.  \n","여기서 비지도학습을 가능케 하는 것이 통계적인 방법이라서 soynlp를 통계 기반 토크나이저로 분류하기도 합니다.\n","\n","트와이스가 한 단어임을 인지하기 위해서 트, 트와, 트와이, 트와이스 각각 다음 글자의 확률을 계산해서 비교한다고 생각하면 좋습니다.  \n","수학적으로 자세한 내용은 여기서는 다루지 않습니다. 다만 koNLPy외에도 soynlp가 있다는 점을 기억해 주세요."],"metadata":{"id":"wtfZDZAlMnrQ"}},{"cell_type":"markdown","source":["이렇게 토큰화 방법을 알아 보았지만 이걸 가지고 단어의 분산 표현을 얻는 법을 제대로 다루지는 않았습니다.  \n","이쯤에서 이런 고민이 생기게 됩니다. 한국어라면 자동차를 _자동 / 차 로 분리되는데... 속성이 아무리 추상적이래도 보기에 차가 마시는 차인지, 달리는 차인지 도통 알 수가 없죠?  \n","게다가 설령 토큰화가 완벽하다고 해도, 남자가 [-1, 0]인지 [1, 0]인지는 컴퓨터 입장에서는 알 도리가 없습니다.\n","\n","Embedding 레이어는 선언 즉시 랜덤한 실수로 Weight 값을 채우고,  \n","학습을 진행하며 적당히 튜닝해가는 방식으로 속성을 맞춰가지만 이는 뭔가 찜찜합니다.  \n","토큰들이 멋지게 의미를 갖게 하는 방법은 없을까요?"],"metadata":{"id":"weE5-eoRNN1M"}},{"cell_type":"markdown","source":["---\n","# 6. 토큰에게 의미를 부여하기\n","---"],"metadata":{"id":"TwvT5wmjNTdS"}},{"cell_type":"markdown","source":["---\n","### Word2Vec\n","---\n","\n","Word2Vec은 \"단어를 벡터로 만든다\"는 멋진 이름을 가지고 있습니다.  \n","난 오늘 술을 한 잔 마셨어 라는 문장의 각 단어 즉, 동시에 등장하는 단어끼리는 연관성이 있다는 아이디어로 시작된 알고리즘입니다.  \n","예문의 경우 다른 단어는 몰라도 술과 마셨어는 괜찮은 연관성을 캐치해낼 수 있겠네요"],"metadata":{"id":"DiDYgZDlNb91"}},{"cell_type":"markdown","source":["- Ex) 강아지 = [ 0 0 0 0 4 0 0 0 0 0 0 0 ... 중략 ... 0]\n","1이란 값 뒤에 9,995개의 0의 값을 가지는 벡터가 됩니다.  \n","하지만 Word2Vec으로 임베딩 된 벡터는 굳이 벡터 차원이 단어 집합의 크기가 될 필요가 없습니다. 강아지란 단어를 표현하기 위해 사용자가 설정한 차원의 수를 가지는 벡터가 되며 각 차원의 값은 실수값을 가집니다.\n","\n","- Ex) 강아지 = [0.2 0.3 0.5 0.7 0.2 ... 중략 ... 0.2]\n","요약하면 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면,  \n","분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산 하여 표현합니다.  \n","이런 표현 방법을 사용하면 단어 벡터 간 유의미한 유사도를 계산할 수 있습니다.  \n","\n","이를 위한 대표적인 학습 방법이 Word2Vec입니다.\n","\n","- Word2Vec의 학습 방식에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있습니다.  \n","CBOW는 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법입니다.  \n","대로, Skip-Gram은 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법입니다.  \n","메커니즘 자체는 거의 동일합니다."],"metadata":{"id":"i3brsQRXPkHW"}},{"cell_type":"markdown","source":["# CBOW\n","\n","<img src='https://wikidocs.net/images/page/22660/단어.PNG'>\n","\n","윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 슬라이딩 윈도우(sliding window)라고 합니다.\n","\n","위 그림에서 좌측의 중심 단어와 주변 단어의 변화는 윈도우 크기가 2일때, 슬라이딩 윈도우가 어떤 식으로 이루어지면서 데이터 셋을 만드는지 보여줍니다.  \n","Word2Vec에서 입력은 모두 원-핫 벡터가 되어야 하는데, 우측 그림은 중심 단어와 주변 단어를 어떻게 선택했을 때에 따라서 각각 어떤 원-핫 벡터가 되는지를 보여줍니다.  \n","위 그림은 결국 CBOW를 위한 전체 데이터 셋을 보여주는 것입니다.\n","\n","<img src='https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG'>\n","\n","CBOW의 인공 신경망을 간단히 도식화하면 위와 같습니다.  \n","입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고,  \n","출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요합니다.\n","\n","위 그림에서 알 수 있는 사실은 Word2Vec은 은닉층이 다수인 딥 러닝(deep learning) 모델이 아니라 은닉층이 1개인 얕은 신경망(shallow neural network)이라는 점입니다.  \n","또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고 부르기도 합니다."],"metadata":{"id":"uJEMEWo2Q2Au"}},{"cell_type":"markdown","source":["# Skip-gram\n","\n","<img src='https://wikidocs.net/images/page/22660/skipgram_dataset.PNG'>\n","\n","<img src = 'https://wikidocs.net/images/page/22660/skipgram_dataset.PNG' >\n","\n","중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없습니다.  \n","여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있습니다.\n"],"metadata":{"id":"27uk924VRb8C"}},{"cell_type":"markdown","source":["---\n","### FastText\n","---\n","\n","Word2Vec은 정말 좋은 방법이지만, 연산의 빈부격차가 존재했습니다. 자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있습니다. FastText는 이를 해결하기 위해 BPE와 비슷한 아이디어를 적용했습니다.\n","\n","어휘를 글자수준으로 쪼개서 학습하면 감성분석 / 문서분류 / 언어 모델링 등의 과제에서 더 좋은 성능을 보인다고 알려졌는데, 이러한 접근 방법을 어휘 임베딩에 적용한 것이 FastText이다. \n","\n","기존에는 임베딩 벡터를 어휘마다 하나씩 할당하고 학습했다면,  \n","이제는 어휘를 구성하고 있는 n-gram마다 임베딩 벡터를 하나씪 할당하고,  \n","어휘를 구성하는 모든 n-gram벡터의 평균 벡터를 어휘 임베딩으로 본 것이다.\n","\n","'Where'이라는 어휘를 쪼갠다면 (n=3으로 가정)\n","\n","<wh, whe, her, ere, re>\n","\n","시작과 끝을 표혀하는 <>를 사용, where라는 어휘의 임베딩 벡터는 5개의 trigram에 대한 임베딩 벡터의 평균으로 계산된다.\n","\n","\n","\n"],"metadata":{"id":"bRrRLb0PSx3N"}},{"cell_type":"markdown","source":["---\n","### ELMo - the 1st Contextualized Word Embedding\n","---\n","\n","위에 소개했던 Word Embedding 알고리즘들은 (역시나) 정말 훌륭하지만, 여전히 고질적인 문제점이 있습니다.  \n","바로 고정적이라는 겁니다! 무슨 말이냐면, 동음이의어를 처리할 수 없다는 얘기입니다.\n","\n","이렇게나 탐스럽고 먹음직스러운 사과를 보셨나요?  \n","저의 간절한 사과를 받아주시기 바랍니다.  \n","우리는 이 두 문장에 나오는 '사과'의 의미가 다르다는 것을 알고 있습니다.  \n","그러나 Word2Vec이든 FastText이든 간에 이 두 문장에 나오는 사과의 워드 벡터값은 동일할 수밖에 없습니다.\n","\n","분명 의미가 다른 두 '사과'의 의미를 명확하게 해석하려면 우리에게 무엇이 필요할까요?  \n","네, 서론에서 우리는 그 답을 미리 살펴보았습니다.  \n","Context-sensitive Grammar를 따르는 자연어를 이해하려면 문맥(context)의 활용이 필수적입니다.\n","여기서 '사과'의 context가 되는 것은 무엇일까요?  \n","첫 문장이라면 탐스럽고 먹음직스러운 이 될 것이고 다음 문장이라면 간절한 이 될 것입니다.  \n","즉, 단어의 의미 벡터를 구하기 위해서는 그 단어만 필요한 것이 아니라 그 단어가 놓인 주변 단어 배치의 맥락이 함께 고려되는 Word Embedding이 필요한 것입니다.  \n","이런 개념을 Contextualized Word Embedding이라고 합니다.\n","\n","2018년 NLP계에 큰 폭풍을 몰고 왔던 ELMo라는 모델은 데이터에 단어가 등장한 순간,  \n","그 주변 단어 정보를 사용해 Embedding을 구축하는 개념을 처음 소개하면서 자연어처리의 획기적인 발전의 계기를 마련해 준 첫 번째 Contextualized Word Embedding 모델입니다.  \n","정말 멋진 아이디어에요!\n","\n","------\n","\n","지금까찌 발표 되었던 수많은 어휘 임베딩 방법들은, 주변 맥락 단어를 학습할 때만 고려할 뿐,  \n","맥락 단어를 어휘 임베딩을 다른 모델의 입력으로써 사용할 때 고려하지는 않았다.  \n","학습이 긑난 뒤에는 어휘 임베딩이 변하지 않으며, 어휘와 임베딩 벡터는 1:1 매칭이 되는 관계로,  \n","이러한 임베딩에서 어떤 정보를 활용할 것인지는 또 다른 모델의 몫이 었다.   \n","\n","하지만 현실에서의 어휘의 의미는 고정된것이 아니라,  \n","맥락에 따라 가변적이며, 어휘 중에는 다양한 동음이의어오 다의어가 존재한다.  \n","\n","- '나는 **머리**를 끄덕였다.'\n","- '나는 **머리**를 짧게 깎았다.'\n","- '나는 **머리**가 좋아서 공부를 잘한다'\n","\n","FastText 등의 어휘 임베딩 방법은 '머리'라는 어휘에 대하여 하나의 임베딩 벡터를 할당한다.    \n","임베딩 벡터가 잘 학습되었다면 하나의 벡터가 '머리'에 대한 다양한 의미를 모두 포괄할 것이지만,  \n","실제로 '머리'의 의미는 주변 단어의 맥락에 대해서 가변적이고,  \n","실제 어휘가 사용되는 맥라과는 관련 없는 정보는 불필요하디.\n","\n","따라서 ELMO는 임베딩을 사용할 때도 해당 벡터가 주변 단어에서 추론된 맥락 정보에 따라 가변적일 수 있도록 만들었다는 의미에서  \n","맥락화된 어휘 임베딩(Contextualized Word Embedding)이라는 표현을 쓰게 된다.\n","\n","<img src='https://t1.daumcdn.net/thumb/R1280x0/?fname=http://t1.daumcdn.net/brunch/service/user/4Ncx/image/NYfG-cguC9HouphXMLoux2fTEkc.png'>\n","\n","\n","ELMO는 양뱡향 LSTM을 이용한다.  \n","순방향 LSTM은 주어진 문장에서 n개의 단어를 보고 n+1 번째 단어를 맞추는 방식  \n","역방향 LSTM은 주어진 문장의 끝부터 n개의 단어를 역순으로 보고, n-1번째 단어를 맞추는 방식  \n","\n","언어 모델을 학습할 때는 양방향 LSTM이 생성한 잠재 상태(hidden state)를 이어 붙인 벡터를 토대로 하는 softmax Layer를 통해 예측하고자 하는 단어를 맞추게 되는데,  \n","임베딩을 생성할 때는 이 레이어는 제거 된다. 또한 양방향 LSTM은 입력으로 기존 어휘 임베딩을 사용한다.    \n","\n","ELMO는 다른 모델에 사용될 어휘 임베딩 벡터를 구성하기 위해 모든 정보를 종합하여 사용된다.   \n","기존 어휘 임베딩(노란색)(입력 토큰의 wordvector), 순방향 LSTM(왼쪽), 역방향 LSTM(오른쪽)을 모두 이어붙인 벡터가 ELMO 임베딩이 된다. "],"metadata":{"id":"Y_VGbjD-dOwD"}},{"cell_type":"markdown","source":["이번에 배운 내용은 토큰화와 분산 표현이 중심입니다. 문장이 입력되면 적절히 토큰화를 하고 토큰을 임베딩(Embedding)을 통해 분산 표현으로 만드는 것이지요.  \n","분산 표현은 벡터이므로 이제 인공지능에 활용할 수 있습니다.\n","\n","토큰화에 사용되는 방법은 언어마다 다른데요. 문장 구성 성분이 다르기 때문입니다.  \n","조사가 있는 한국어는 형태소 기반인 koNLPy를 주로 쓰고, WordPiece Model인 SentencePiece를 쓸 수도 있어요. 물론 그 외에 다른 방법도 있습니다.\n","\n","토큰화를 마친 후 임베딩을 할 때는 토큰마다 독립적으로 만들지 않고 토큰 간의 관계성을 주입합니다.  \n","그래야 문장을 구성할 때 적절히 사용될 수 있기 때문이에요.  \n","이렇게 토큰 간의 관계성을 고려해서 만든 것으로는 Word2Vec, FastText 등이 있어요.  \n","거기다가 문장의 문맥까지 고려하는 ELMo까지 등장했습니다.\n","\n","이제 지금까지 배운 내용이 그려지나요?  \n","오늘 배운 것들은 앞으로 진행될 모든 것들의 기초가 되는 부분이니 꼭 숙지하시길 권장합니다.\n","\n","자연어 처리 분야는 인공지능 모델도 중요하지만 토큰화와 임베딩에 관련된 내용이 매우 중요합니다.  \n","그러니 오늘 배용을 토대로 이 분야에 재미를 느껴 멋진 NLP 연구자로 성장하시기를 바랍니다.   \n","고생하셨습니다!!"],"metadata":{"id":"pSh3-mywhL8X"}},{"cell_type":"code","source":[""],"metadata":{"id":"IYufEJ62Jmx2"},"execution_count":null,"outputs":[]}]}