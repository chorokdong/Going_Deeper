{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GD7_Seq2seq와 Attention.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOZgGdSW4yiOqE7in0Y0oQT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["---\n","# 우리가 만드는 언어 모델\n","---\n","\n","언어 모델(Language Model)이란, 주어진 단어들을 보고 다음 단어를 맞추는 모델입니다.  \n","더 자세하게는, 단어의 시퀀스를 보고 다음 단어에 확률을 할당 하는 모델이죠!m  \n","  \n","좀 더 수학적으로 표현하자면, 언어 모델은 n-1개의 단어 시퀀스 $ w_1,⋯,w_{n−1} $ 가 주어졌을 때,   \n","n번째 단어 $w_n$ 으로 무엇이 올지를 예측하는 확률 모델로 표현됩니다.  \n","파라미터 θ로 모델링하는 언어 모델을 다음과 같이 표현할 수 있습니다. \n","\n","$$ P(w_n∣w_1,...,w_{n−1} ;θ)$$\n","\n","하지만 나중에는 꼭 시퀀스 형태의 Next Token Prediction 언어모델이 아니더라도,  \n","주변 단어를 보고 중심 단어를 예측하는 형태로 언어모델을 구성하는 것도 보시게 될 것입니다."],"metadata":{"id":"-D0ZkFIldNG8"}},{"cell_type":"markdown","source":["---\n","### 통계적 언어 모델 (Statistical Language Model)\n","----"],"metadata":{"id":"Lq9cwsaMdTmi"}},{"cell_type":"markdown","source":["딥러닝이 등장하기 이전엔 통계적 언어 모델(Statistical Language Model) 의 사용이 지배적이었습니다.  \n","대표적으로 2000년대 초반까지 구글이나 네이버의 번역기는 모두 통계적 언어 모델을 기반으로 하고 있었죠.   \n","  \n","등장한 적 없는 단어나 문장에 대해 모델링을 할 수 없다는 통계적 언어 모델의 단점은 치명적이었습니다.  \n","데이터가 아무리 많다고 해도 세상 모든 단어를 포함할 수는 없었으니 말이죠."],"metadata":{"id":"5A5a_5xPeTc9"}},{"cell_type":"markdown","source":["---\n","### 신경망 언어 모델 (Neural Network Language Model)\n","---"],"metadata":{"id":"Y85SPUvueTXw"}},{"cell_type":"markdown","source":["통계적 언어 모델의 단점을 개선한 것이 우리가 배울 신경망 언어 모델(Neural Network Language Model, 이하 NNLM) 입니다.  \n","NNLM의 시초는 Feed-Forward 신경망 언어 모델인데, 지금의 Embedding 레이어의 아이디어인 모델입니다.  \n","이에 대한 설명이 아래 웹페이지에 아주 잘 되어 있습니다.\n","\n","각 단어를 일련의 Embedding 벡터로 표현한 후, 이전의 몇 개 단어를 활용해 다음 단어를 예측하는 것은 분명 많은 문제를 해결했습니다.  \n","특히 단어 간의 유사도를 표현할 수 있게 되어 문장의 유창성이 높아진 것은 혁신적이었죠!\n","  \n","하지만 예측에 정해진 개수의 단어만 참고한다는 분명한 한계가 있었습니다.  \n","예를 들어 번역문을 생성하려면 문장이 짧을 수도, 길 수도 있으니, n개의 단어를 참고하기보다는 \"몇 개 단어가 들어와도 문장 단위로 처리한다!\"는 종류의 모델링이 필요하게 되었죠.  \n","그렇게 고안된 것이 바로 여러분들이 잘 알고 계신 **순환 신경망(Recurrent Neural Network, 이하 RNN)**을 활용한 언어 모델 입니다. "],"metadata":{"id":"vWW0_34negZ3"}},{"cell_type":"markdown","source":["---\n","# Sequence to Sequence 문제\n","---\n","\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-2.jpg'>"],"metadata":{"id":"QkLNtJoXinWZ"}},{"cell_type":"markdown","source":["다시 한번 설명하면, 여러 개의 단어(Embedding)를 합쳐(Concatenate) 고정된 크기의 Weight를 Linear로 처리하는 방식은 유연성에 한계가 있었습니다.  \n","단어의 개수에 무관하게 처리할 수 있는 네트워크가 필요했고, 그것은 곧 RNN의 고안으로 이어졌습니다.   \n","RNN은 고정된 크기의 Weight가 선언되는 것은 동일하지만 입력을 순차적으로 \"적립\"하는 방식을 채택함으로써 유동적인 크기의 입력을 처리할 수 있었습니다.\n","<br><br>\n","\"적립\"이라는 표현을 일반적으로 사용하지는 않습니다만, 필자가 생각하기에 RNN의 동작 방식에 잘 어울린다고 생각해 선택했습니다.  \n","기억, 누적, 압축 등으로 대체할 수 있으며 가장 와닿는 방향으로 이해하세요!\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-3.jpg'>\n","<br><br>\n","단어가 자체적으로 의미를 가질 수 있는 Embedding을 도입하고, 입력의 유연성을 위해 RNN도 적용했는데, 아직도 해결할 문제가 있을까요? 물론입니다!  \n","대표적으로 RNN에는 두 가지 문제점이 꼽히곤 합니다.\n","\n","- 하나의 Weight에 입력을 적립하다 보니 입력이 길어질수록 이전 입력에 대한 정보가 소실되는 기울기 소실(Vanishing Gradient) 문제가 있습니다.  \n","위의 그림을 보면 각 입력마다 정보를 색으로 확인할 수 있습니다.  \n","첫 입력인 What(남색)의 정보가 마지막 입력인 ?에 다다라서는 거의 희석된 모습을 보여주고 있죠.  \n","이 문제는 LSTM을 고안함으로써 개선되었습니다.  \n","LSTM에 대한 자세한 내용은 이미 알고 계신다고 생각하겠습니다.  \n","기억이 잘 나지 않는 분은 아래 링크를 참고해 주세요.\n","\n","[Long Short-Term Memory (LSTM) 이해하기](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)\n","\n","- 단어 단위로 입력과 출력을 순환하는 RNN 구조는 문장 생성엔 적합할지언정 번역에 사용하기는 어렵다는 문제가 있습니다.  \n","나는 점심을 먹는다 라는 문장을 영문으로 번역하자면 목표 문장은 I eat lunch 가 될 것인데,  \n","과정을 순차적으로 생각하면 eat 이라는 단어를 만들 때는 먹는다 에 대한 정보가 없습니다.  \n","각 언어별로 어순이 다르기 때문입니다.\n","\n",">나는 -> I  \n","나는 점심을 -> I lunch  \n","나는 점심을 먹는다 -> I lunch eat(?)\n","\n","심지어 입력의 길이와 번역의 길이가 같다는 보장도 없죠.  \n","번역에 있어서는 문장을 다 읽고 번역하는, 즉 문장 전체를 보고 나서 생성하는 구조가 필요했습니다.  \n","이에 2014년, 구글이 Sequence to Sequence(Seq2Seq) 구조를 제안합니다.\n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-4.max-800x600.jpg'>\n","\n","  \n","\n"],"metadata":{"id":"TBufE-O7i2nt"}},{"cell_type":"markdown","source":["---\n","# Sequence to Sequence 구현\n","---\n","\n","이제 Sequence to Sequence를 TensorFlow로 구현해보죠.  \n","일단은 데이터를 직접 다루기보다는 차원 수를 확인하는 실습을 해보겠습니다.  \n","RNN 계통의 레이어들은 입력값과 반환값이 설정에 따라 각양각색입니다.  \n","이번 구현에서는 입력으로 Embedding된 단어만 전달하고 (Hidden State는 전달하지 않습니다),  \n","출력은 Encoder와 Decoder 별로 상이하므로 각각 설명을 첨부하겠습니다."],"metadata":{"id":"5sIMZQvOs0l3"}},{"cell_type":"markdown","source":["---\n","### LSTM Encoder\n","---"],"metadata":{"id":"Yzr9fVJz5gVy"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"6E2C30lKdJTY","executionInfo":{"status":"ok","timestamp":1657852408425,"user_tz":-540,"elapsed":1461,"user":{"displayName":"김건국","userId":"04251633153705627950"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units):\n","    super(Encoder, self).__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm = tf.keras.layers.LSTM(enc_units) # return_sequences 매개변수를 기본값 False로 전달\n","\n","  def call(self, x):\n","    print(\"입력 Shape:\", x.shape)\n","\n","    x = self.embedding(x)\n","    print(\"Embedding Layer를 거친 Shape:\", x.shape)\n","\n","    output = self.lstm(x)\n","    print(\"LSTM Layer의 Output Shape:\", output.shape)\n","\n","    return output"]},{"cell_type":"markdown","source":["Embedding 레이어를 단어 사이즈와 Embedding 차원에 대해 선언을 한 후,  \n","논문에서 소개한 대로 tf.keras.layers.LSTM(enc_units)으로 LSTM을 정의합니다.  \n","TensorFlow 속 LSTM 모듈의 기본 반환 값은 최종 State 값이므로 return_sequences 나 return_state 값은 따로 조정하지 않습니다 (기본: False).  \n","즉, 우리가 정의해 준 Encoder 클래스의 반환 값이 곧 **컨텍스트 벡터(Context Vector)** 가 되는 겁니다.  \n"],"metadata":{"id":"vZYiJPg3tKPD"}},{"cell_type":"code","source":["vocab_size = 30000\n","emb_size = 256\n","lstm_size = 512\n","batch_size = 1\n","sample_seq_len = 3\n","\n","print(\"Vocab Size: {0}\".format(vocab_size))\n","print(\"Embedidng Size: {0}\".format(emb_size))\n","print(\"LSTM Size: {0}\".format(lstm_size))\n","print(\"Batch Size: {0}\".format(batch_size))\n","print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKMsdT0Ys_Y1","executionInfo":{"status":"ok","timestamp":1657852491205,"user_tz":-540,"elapsed":8,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"7213cb2f-45c4-4e72-d6f3-2739383c1f29"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab Size: 30000\n","Embedidng Size: 256\n","LSTM Size: 512\n","Batch Size: 1\n","Sample Sequence Length: 3\n","\n"]}]},{"cell_type":"code","source":["encoder = Encoder(vocab_size, emb_size, lstm_size)\n","sample_input = tf.zeros((batch_size, sample_seq_len))\n","\n","sample_output = encoder(sample_input)    # 컨텍스트 벡터로 사용할 인코더 LSTM의 최종 State값"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N37Mfe67tXF1","executionInfo":{"status":"ok","timestamp":1657852503695,"user_tz":-540,"elapsed":6939,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"4dcb0170-3b13-4934-b585-7524dd2e19f2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 Shape: (1, 3)\n","Embedding Layer를 거친 Shape: (1, 3, 256)\n","LSTM Layer의 Output Shape: (1, 512)\n"]}]},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-6.max-800x600.jpg'>\n","\n","   \n","아주 간편하게 Encoder 클래스를 정의했습니다.  \n","어떤 Source 문장을 Encoder에 읽히고, 그 반환 값인 LSTM의 최종 State 값을 Decoder에게 전달해 주면 되겠죠?"],"metadata":{"id":"JvRzdESx5SA4"}},{"cell_type":"markdown","source":["---\n","### LSTM Decoder\n","---"],"metadata":{"id":"azpF4cM95rBX"}},{"cell_type":"code","source":["# Encoder 구현에 사용된 변수들을 이어 사용함에 유의!\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units):\n","    super(Decoder, self).__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm = tf.keras.layers.LSTM(dec_units,\n","                                     return_sequences=True) # return_sequences 매개변수를 True로 설정\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","    self.softmax = tf.keras.layers.Softmax(axis=-1)\n","\n","  def call(self, x, context_v):  # 디코더의 입력 x와 인코더의 컨텍스트 벡터를 인자로 받는다. \n","    print(\"입력 Shape:\", x.shape)\n","\n","    x = self.embedding(x)\n","    print(\"Embedding Layer를 거친 Shape:\", x.shape)\n","\n","    context_v = tf.repeat(tf.expand_dims(context_v, axis=1),\n","                          repeats=x.shape[1], axis=1)\n","    x = tf.concat([x, context_v], axis=-1)  # 컨텍스트 벡터를 concat 해준다\n","    print(\"Context Vector가 더해진 Shape:\", x.shape)\n","\n","    x = self.lstm(x)\n","    print(\"LSTM Layer의 Output Shape:\", x.shape)\n","\n","    output = self.fc(x)\n","    print(\"Decoder 최종 Output Shape:\", output.shape)\n","\n","    return self.softmax(output)"],"metadata":{"id":"b0q8Hyeku2Vi","executionInfo":{"status":"ok","timestamp":1657855735014,"user_tz":-540,"elapsed":365,"user":{"displayName":"김건국","userId":"04251633153705627950"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Decoder는 Encoder와 구조적으로 유사하지만 결과물을 생성해야 하므로 Fully Connected 레이어가 추가되었고,  \n","출력값을 확률로 변환해 주는 Softmax 함수도 추가되었습니다 (Softmax는 모델 내부에 포함시키지 않아도 훈련 과정에서 포함시키는 방법도 있습니다).  \n","그리고 Decoder가 매 스텝 생성하는 출력은 우리가 원하는 번역 결과에 해당하므로  \n","LSTM 레이어의 return_sequences 변수를 True로 설정하여 State 값이 아닌 Sequence 값을 출력으로 받습니다.\n"],"metadata":{"id":"RBhZNVgW507v"}},{"cell_type":"code","source":["print(\"Vocab Size: {0}\".format(vocab_size))\n","print(\"Embedidng Size: {0}\".format(emb_size))\n","print(\"LSTM Size: {0}\".format(lstm_size))\n","print(\"Batch Size: {0}\".format(batch_size))\n","print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eP5GzKHn6P8o","executionInfo":{"status":"ok","timestamp":1657855874190,"user_tz":-540,"elapsed":426,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"3d10a8b6-ccd7-4ce8-f462-9a0650b5cdb6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab Size: 30000\n","Embedidng Size: 256\n","LSTM Size: 512\n","Batch Size: 1\n","Sample Sequence Length: 3\n","\n"]}]},{"cell_type":"code","source":["decoder = Decoder(vocab_size, emb_size, lstm_size)\n","sample_input = tf.zeros((batch_size, sample_seq_len))\n","\n","dec_output = decoder(sample_input, sample_output)  # Decoder.call(x, context_v) 을 호출"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Onp0FlF6Q_D","executionInfo":{"status":"ok","timestamp":1657855880794,"user_tz":-540,"elapsed":1902,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"5aa60974-9378-422e-9e8c-274d15e2e801"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 Shape: (1, 3)\n","Embedding Layer를 거친 Shape: (1, 3, 256)\n","Context Vector가 더해진 Shape: (1, 3, 768)\n","LSTM Layer의 Output Shape: (1, 3, 512)\n","Decoder 최종 Output Shape: (1, 3, 30000)\n"]}]},{"cell_type":"markdown","source":["<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-7.max-800x600.jpg'>\n","<br><br>\n","$ p(y_1,...,y_{T′}|x_1,...,x_T)=Π^{T′}_{t=1}p(y_t|v,y_1,...,y_{t−1}) $\n","<br><br>\n","Encoder가 생성한 컨텍스트 벡터 v 를 Embedding 레이어를 거친 y 값에 Concatnate하여 위 수식을 비로소 만족하게 됩니다. 우리가 Seq2seq를 완성한 거죠!\n"," \n"],"metadata":{"id":"PZuSI1vo6WmP"}},{"cell_type":"markdown","source":["---\n","# Attention! (1) Bahdanau Attention\n","---\n","\n","혁신적이었던 Seq2Seq은 Encoder-Decoder 구조라는 딥러닝 모델의 큰 틀을 제시했고, 지금까지도 그 구조는 널리 활용되고 있습니다.  \n","하지만 그것만으로 기계 번역이 완벽했다면 우리는 이미 외국어에 대한 두려움이 사라졌겠죠?\n","  \n","Seq2Seq 역시 여느 기법처럼 한계점이 존재했으며 이를 발전시키려는 시도가 아주 많았습니다.  \n","가장 대표적인 방법이 바로 Attention 메커니즘입니다!  \n","이번 스텝에서는 Attention 메커니즘의 이모저모를 살펴볼 거예요!"],"metadata":{"id":"PCqE3vPe66Ke"}},{"cell_type":"markdown","source":["---\n","### Bahdanau Attention\n","---"],"metadata":{"id":"kFkzVzTw7Y4l"}},{"cell_type":"markdown","source":["장점으로 보이는 어떤 것도 누군가에게는 결점이 보이는 법이죠.  \n","Bahdanau는 Seq2Seq의 컨텍스트 벡터가 고정된 길이로 정보를 압축하는 것이 손실을 야기한다고 주장하였습니다.  \n","즉 짧은 문장에 대해서는 괜찮을지 모르겠으나 문장이 길어질수록 성능이 저하된다는 것이지요. 콜럼버스의 달걀처럼, 듣고 보니 그럴듯합니다!  \n","\n","이에 그는 Encoder의 최종 State 값만을 사용하는 기존의 방식이 아닌, 매 스텝의 Hidden State를 활용해 컨텍스트 벡터를 구축하는 Attention 메커니즘을 제안합니다.  \n","\n","아래는 Bahdanau의 Attention 논문입니다.  \n","내용이 제법 어렵지만 must-read 논문이니만큼, 꼭 한번 읽어보시기를 권합니다.\n","\n","Sequence Labeling은 $ x_i$\t와 $y_i$ 의 관계를 구하는 문제지만  \n","Sequence to Sequence는 $x_{1:n}$ 과 동일한 의미를 가지는 $y_{1:m}$ 을 만드는 문제였지요."],"metadata":{"id":"zGWS4UcC76zZ"}},{"cell_type":"markdown","source":["---\n","### seq2seq과 attn-seq2seq, 뭐가 다른가?\n","---"],"metadata":{"id":"mowJbPw3-UXL"}},{"cell_type":"markdown","source":["Attention의 개념에 대해 어려워하는 분들이 많습니다. \n","이렇게 생각해 봅시다. Attention이 없는 것과 있는 것은 과연 뭐가 달라지는 것일까?\n","<br><br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-attn.max-800x600.png'>\n","<br><br>\n","위의 두 식을 비교해 봅시다.  \n","Bahdanau 논문 원문에 나오는 Encoder-Decoder 구조에 대한 수식을 seq2seq만 있는 경우와 attention이 적용된 경우로 나누어 비교해 보면,  \n","약간의 notation을 수정해서 보면 단 한 군데만 빼고는 사실상 동일합니다.  \n","어디가 다른지 눈에 띄시나요?  \n","<br>\n","네 그렇습니다. 유일한 차이는 attention이 있는 경우엔 바로 context vector c에 첨자 i가 붙어있다는 점입니다.  \n","그렇다면 이 작은 첨자 하나가 어떤 근본적인 차이를 가져오게 되는 것인지 생각해 볼까요?  \n","<br>\n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-attn_exp.max-800x600.png'>  \n","<br>\n","위 그림은 Bahdanau 논문 원문의 3.LEARNING TO ALIGN AND TRANSLATE의 내용을 바탕으로 재구성한 것입니다.  \n","위 그림의 왼쪽 부분은 $X_j$ 를 입력으로, $y_i$를 출력으로 하는 인코더-디코더 부분을 도식화한 것입니다.  \n","여기서 유의해야 할 점은 **i는 디코더의 인덱스, j는 인코더의 인덱스**라는 점입니다.\n","  \n","그렇다면 context vector c에 첨자 i가 붙어 $c_i$ 가 된다는 것의 의미는 무엇일까요?  \n","  \n",">인코더가 X를 해석한 context $c_i$ 는 디코더의 포지션 i에 따라 다르게 표현(represent)되어야 한다.\n","\n","seq2seq의 인코더가 해석한 context는 디코더의 포지션 i에 무관하게 항상 일정했습니다.  \n","그러나 attention이 가미되면 달라집니다.  \n","  \n","'나는 밥을 먹었다'라는 한글 문장을 'I ate lunch'로 번역한다고 생각해 봅시다.   \n","영어 문장의 첫 번째(i=0) 단어 'I'를 만들어야 할 때 인코더가 한글 문장을 해석한 컨텍스트 벡터에서는 '나는'이 강조되어야 하고,  \n","영어 문장의 세 번째(i=2) 단어 'lunch'를 만들어야 할 때 인코더의 컨텍스트 벡터에서는 '밥을'이 강조되어야 한다는 것입니다.  \n","  \n","## 디코더가 현재 시점 i에서 보기에 인코더의 어느 부분 j가 중요한가?   \n","  \n","이 가중치가 바로 attention인 것입니다.\n","\n","얼마나 강조되어야 하는지를 나타내는 가중치는 어떻게 계산하나요? 위의 식에서 $α_{ij} $가  \n","바로 인코더의 j번째 hidden state $h_j$ 가 얼마나 강조되어야 할지를 결정하는 가중치 역할을 합니다.  \n","이 가중치는 다시 디코더의 직전 스텝의 hidden state $s_{i−1}$ 와 $h_j$의 유사도가 높을수록 높아지게 되어 있습니다.\n","\n"," $ \\displaystyle\\sum_j α_{ij}=1 $"],"metadata":{"id":"LZ_MbaNf-Y-e"}},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W_decoder = tf.keras.layers.Dense(units)\n","    self.W_encoder = tf.keras.layers.Dense(units)\n","    self.W_combine = tf.keras.layers.Dense(1)\n","\n","  def call(self, H_encoder, H_decoder):\n","    print(\"[ H_encoder ] Shape:\", H_encoder.shape)\n","\n","    H_encoder = self.W_encoder(H_encoder)\n","    print(\"[ W_encoder X H_encoder ] Shape:\", H_encoder.shape)\n","\n","    print(\"\\n[ H_decoder ] Shape:\", H_decoder.shape)\n","    H_decoder = tf.expand_dims(H_decoder, 1)\n","    H_decoder = self.W_decoder(H_decoder)\n","    \n","    print(\"[ W_decoder X H_decoder ] Shape:\", H_decoder.shape)\n","\n","    score = self.W_combine(tf.nn.tanh(H_decoder + H_encoder))\n","    print(\"[ Score_alignment ] Shape:\", score.shape)\n","    \n","    attention_weights = tf.nn.softmax(score, axis=1)\n","    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n","\n","    context_vector = attention_weights * H_decoder\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights\n","\n","W_size = 100\n","\n","print(\"Hidden State를 {0}차원으로 Mapping\\n\".format(W_size))\n","\n","attention = BahdanauAttention(W_size)\n","\n","enc_state = tf.random.uniform((1, 10, 512))\n","dec_state = tf.random.uniform((1, 512))\n","\n","_ = attention(enc_state, dec_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgrUFgyi6SPE","executionInfo":{"status":"ok","timestamp":1657858752343,"user_tz":-540,"elapsed":399,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"cac33157-f897-43ba-cd76-f0fdcd7662a6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Hidden State를 100차원으로 Mapping\n","\n","[ H_encoder ] Shape: (1, 10, 512)\n","[ W_encoder X H_encoder ] Shape: (1, 10, 100)\n","\n","[ H_decoder ] Shape: (1, 512)\n","[ W_decoder X H_decoder ] Shape: (1, 1, 100)\n","[ Score_alignment ] Shape: (1, 10, 1)\n","\n","최종 Weight:\n"," [[[0.07059775]\n","  [0.0860828 ]\n","  [0.16867474]\n","  [0.06657811]\n","  [0.04323225]\n","  [0.10643011]\n","  [0.13267711]\n","  [0.09399804]\n","  [0.1697933 ]\n","  [0.06193582]]]\n"]}]},{"cell_type":"markdown","source":["Encoder의 모든 스텝에 대한 Hidden State를 100차원의 벡터 공간으로 매핑 (1, 10, 100) 하고,  \n","Decoder의 현재 스텝에 대한 Hidden State 역시 100차원의 벡터 공간으로 매핑 (1, 1, 100)해  \n","두 State의 합으로 정의된 Score (1, 10, 1) 를 구하는 모습입니다.   \n","  \n","Softmax를 거쳐 나온 값은 0-1 사이의 값으로 각 단어가 차지하는 비중을 의미하겠죠?  \n","예시에서는 랜덤한 값을 사용했기 때문에 비중이 비슷비슷하지만 실제 단어로 적용시켜보면 유사한 단어에 높은 비중을 할당하게 된답니다!  \n","  \n","그것을 시각화하면 아래와 같은 그림을 보실 수 있습니다.  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-9.jpg'>"],"metadata":{"id":"LNWEzkS_FptN"}},{"cell_type":"markdown","source":["---\n","# Attention! (2) Luong Attention\n","---"],"metadata":{"id":"gi-NYsUNF6Sa"}},{"cell_type":"markdown","source":["---\n","### Luong Attention\n","---\n","Luong의 Attention은 Bahdanau의 방식을 약간 발전시킨 형태입니다.  \n","Decoder의 현재 Hidden State를 구하기 위해 한 스텝 이전의 Hidden State를 활용하는 것은 연산적으로 비효율적입니다.  \n","이는 RNN의 연산 형태 때문인데, 자세한 내용은 아래 웹페이지에서 확인하시죠! \n","   \n","수식적인 부분은 완벽하게 이해하지 않아도 좋으니, Luong의 아이디어에 중점을 맞추도록 합니다.  \n","\n","[[Attention] Luong Attention 개념 정리](https://hcnoh.github.io/2019-01-01-luong-attention)"],"metadata":{"id":"7w_C3SL6GGx3"}},{"cell_type":"markdown","source":["### Bahdanau Attention으로부터 달라진 점\n","\n","논문에서 제시하는 Bahdanau Attention과의 차이점은 다음과 같이 정리할 수 있다.\n","\n","- 디코더의 Hidden State Vector를 구하는 방식이 간소화되었고 결과적으로 Attention Mechanism의 Computation Path가 간소화되었다.\n","- Local Attention과 그것을 위한 Alignment Model을 제시하였다.\n","- 다양한 Score Function들을 제시하였고 그들 각각을 비교해 보았다.\n","  \n","사실 이 논문에서 주된 내용은 Hidden State Vector를 구하는 방식이 달라졌다는 점과 Local Attention을 사용했다는 점 말고는 특이한 점은 없다.  \n","특히 다양한 Score Function을 제시하였다는 내용은 너무도 마이너해보인다.  \n","아마 비슷한 시기에 비슷한 내용으로 논문을 준비하다보니 벌어진 현상으로 보인다.  \n","어쨌든 Hidden State Vector를 어떻게 구했는지, 그리고 Local Attention이 무엇인지를 중점적으로 보면 될 듯 싶다."],"metadata":{"id":"BnnRBaWCJbvu"}},{"cell_type":"markdown","source":["4가지 Score 함수(Dot, General, Concat, Location) 중 \n","가장 좋은 성능을 보인 Score 함수는 General 이다. \n","  \n","이를 기반으로 구현을 해보겠습니다.  \n","\n","$Score(H_{target},H_{source})=H_{Ttarget} × W_{combine} × H_{source}$"],"metadata":{"id":"MUUSJNZlJ09x"}},{"cell_type":"code","source":["class LuongAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(LuongAttention, self).__init__()\n","    self.W_combine = tf.keras.layers.Dense(units)\n","\n","  def call(self, H_encoder, H_decoder):\n","    print(\"[ H_encoder ] Shape:\", H_encoder.shape)\n","\n","    WH = self.W_combine(H_encoder)\n","    print(\"[ W_encoder X H_encoder ] Shape:\", WH.shape)\n","\n","    H_decoder = tf.expand_dims(H_decoder, 1)\n","    alignment = tf.matmul(WH, tf.transpose(H_decoder, [0, 2, 1]))\n","    print(\"[ Score_alignment ] Shape:\", alignment.shape)\n","\n","    attention_weights = tf.nn.softmax(alignment, axis=1)\n","    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n","\n","    attention_weights = tf.squeeze(attention_weights, axis=-1)\n","    context_vector = tf.matmul(attention_weights, H_encoder)\n","\n","    return context_vector, attention_weights\n","\n","emb_dim = 512\n","\n","attention = LuongAttention(emb_dim)\n","\n","enc_state = tf.random.uniform((1, 10, emb_dim))\n","dec_state = tf.random.uniform((1, emb_dim))\n","\n","_ = attention(enc_state, dec_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlXsJzJ3FPqX","executionInfo":{"status":"ok","timestamp":1657860087243,"user_tz":-540,"elapsed":357,"user":{"displayName":"김건국","userId":"04251633153705627950"}},"outputId":"6740f403-aba4-4454-888f-42c7e94a0114"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[ H_encoder ] Shape: (1, 10, 512)\n","[ W_encoder X H_encoder ] Shape: (1, 10, 512)\n","[ Score_alignment ] Shape: (1, 10, 1)\n","\n","최종 Weight:\n"," [[[7.0431167e-03]\n","  [6.2901634e-03]\n","  [1.5273647e-01]\n","  [6.6094911e-01]\n","  [1.5797305e-01]\n","  [4.8017590e-03]\n","  [8.8308062e-03]\n","  [8.5309875e-04]\n","  [3.8503195e-04]\n","  [1.3732456e-04]]]\n"]}]},{"cell_type":"markdown","source":["Bahdanau의 Score 함수와는 다르게 하나의 Weight만을 사용하는 것이 특징입니다.  \n","어떤 벡터 공간에 매핑해주는 과정이 없기 때문에 Weight의 크기는 단어 Embedding 크기와 동일해야 연산이 가능합니다.  \n","이 또한 번역에 적용해보고 성능을 비교해본다면 좋겠죠!"],"metadata":{"id":"6GJBkrrOKYer"}},{"cell_type":"markdown","source":["---\n","# 트랜스포머로 가기 전 징검다리?\n","---\n","\n","Seq2seq와 Attention이 폭풍처럼 휩쓸고 난 후, 잠잠해진 NLP 계를 다시 깨운 것은 2016년 구글의 신경망 번역 시스템이었습니다.  \n","놀라운 구조를 제안한 것은 아니나 무려 8개 층을 쌓은 Encoder-Decoder 구조와 Residual Connection은 제법 멋졌죠.  \n","\n","이에 대한 정리 글을 첨부하니 가볍게 읽어 보세요!"],"metadata":{"id":"f8Sd1j3LLgcX"}},{"cell_type":"markdown","source":["- 구글이 만든 신경망 번역 시스템으로 GNMT (Google Nueral Machine Translation) 라고 부른다.\n","  - 당연히 end-to-end 구조이다.\n","  - 예전 방식인 phrase-based 번역 모델에 비해 짱 좋다.\n","  - 그리고 참고로 일반적인 신경망 번역 시스템을 NMT 라고 부른다.\n","  \n","- 물론 NMT 계열의 모델도 단점이 있다.\n","  - (1) 느린 학습(training) 속도와 느린 추론(inference) 속도\n","  - (2) 드물게 등장하는 단어에 대한 부정확도\n","  - (3) 가끔씩 전체 입력 문장에 대해 다 번역을 하지 않는 경우가 생긴다.\n","  - (4) 연산량이 무지 높다.\n","  - (5) 큰 모델을 사용하려면 많은 데이터가 필요하다.\n","    \n","- 실제로 서비스하려면 정확도와 속도가 생명.\n","- 구글이 사용한 GNMT 모델을 잠시 소개하자면,\n","  - 8개의 LSTM encoder 와 8 개의 LSTM docoder (게다가 Attention 모델)\n","  -학습시 속도를 올리기 위해 low-precision 연산 처리.\n","  - 드문드문 발생하는 단어들도 잘 좀 처리해보자는 의미에서 wordpiece 를 사용.\n","  - 빠른 검색을 위한 beam-search 는 local-normalization 기법을 사용함."],"metadata":{"id":"21qNQphvLzTH"}},{"cell_type":"markdown","source":["GNMT(Google Neural Machine Translation)는 어쩌면 복선이었을 수도 있는데,  \n","왜냐하면 그 후에 등장한 것이 NLP의 꽃, 트랜스포머(Transformer) 이기 때문이죠!  \n","앞서 언급한 레이어를 쌓는 구조나 Residual Connection이 트랜스포머와 굉장히 유사하기에 그렇게 느껴지기도 합니다.  \n","  \n","<img src='https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-10.max-800x600.jpg'>  \n","   \n","트랜스포머 모델은 Multi-Head Attention이라는 개념을 도입해 폭넓은 문맥을 파악하게 하고,  \n","기존의 RNN 구조를 완전히 탈피하여 연산 속도 측면에서도 혁신적인 발전이 일어났습니다!  \n","지금까지도 트랜스포머를 기반으로 한 모델들이 각 분야에서 최고의 성능을 내고 있으니,  \n","그 파급력을 알 만하죠?  \n","  \n","궁금하시면 미리 알아보셔도 좋지만 오늘은 여기까지! 자세한 것은 차근차근 알아보도록 합시다."],"metadata":{"id":"wCgCh0TsQ-p_"}},{"cell_type":"code","source":[""],"metadata":{"id":"7aVC9h2FKVmI"},"execution_count":null,"outputs":[]}]}